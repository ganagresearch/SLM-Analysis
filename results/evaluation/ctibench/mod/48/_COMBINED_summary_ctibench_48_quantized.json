[
    {
        "model_name": "TinyLlama-1.1B-Chat-v1.0-GPTQ",
        "timestamp": "20250424-083812",
        "input_output_file": "results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-083812_outputs.json",
        "input_metrics_file": "results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-083812_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-mcq_ground_truth.jsonl",
        "run_args": {
            "model_path": "/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ",
            "model_type": "causal",
            "benchmark_name": "ctibench",
            "benchmark_path": "/workspace/datasets",
            "results_dir": "/workspace/results/mod/48",
            "device": "cuda",
            "max_new_tokens": 512,
            "num_samples": 100,
            "cti_subset": "cti-mcq"
        },
        "task_type": "mcq",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 97,
        "items_processed_for_accuracy": 3,
        "correct_matches": 0,
        "accuracy": 0.0
    },
    {
        "model_name": "TinyLlama-1.1B-Chat-v1.0-GPTQ",
        "timestamp": "20250424-084159",
        "input_output_file": "results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-084159_outputs.json",
        "input_metrics_file": "results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-084159_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-vsp_ground_truth.jsonl",
        "run_args": {
            "model_path": "/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ",
            "model_type": "causal",
            "benchmark_name": "ctibench",
            "benchmark_path": "/workspace/datasets",
            "results_dir": "/workspace/results/mod/48",
            "device": "cuda",
            "max_new_tokens": 512,
            "num_samples": 100,
            "cti_subset": "cti-vsp"
        },
        "task_type": "vsp",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 75,
        "items_processed_for_accuracy": 25,
        "correct_matches": 6,
        "accuracy": 0.24
    },
    {
        "model_name": "TinyLlama-1.1B-Chat-v1.0-GPTQ",
        "timestamp": "20250424-091924",
        "input_output_file": "results/mod/48/ctibench_cti-rcm_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-091924_outputs.json",
        "input_metrics_file": "results/mod/48/ctibench_cti-rcm_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-091924_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-rcm_ground_truth.jsonl",
        "run_args": {
            "model_path": "/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ",
            "model_type": "causal",
            "benchmark_name": "ctibench",
            "benchmark_path": "/workspace/datasets",
            "results_dir": "/workspace/results/mod/48",
            "device": "cuda",
            "max_new_tokens": 512,
            "num_samples": 100,
            "cti_subset": "cti-rcm"
        },
        "task_type": "rcm",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 41,
        "items_processed_for_accuracy": 59,
        "correct_matches": 14,
        "accuracy": 0.2373
    },
    {
        "model_name": "Phi-3-mini-4k-instruct-GPTQ",
        "timestamp": "20250424-114301",
        "input_output_file": "results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-114301_outputs.json",
        "input_metrics_file": "results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-114301_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-mcq_ground_truth.jsonl",
        "run_args": {
            "model_path": "/workspace/models/Phi-3-mini-4k-instruct-GPTQ",
            "model_type": "causal",
            "benchmark_name": "ctibench",
            "benchmark_path": "/workspace/datasets",
            "results_dir": "/workspace/results/mod/48",
            "device": "cuda",
            "max_new_tokens": 512,
            "num_samples": 100,
            "cti_subset": "cti-mcq"
        },
        "task_type": "mcq",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 100,
        "items_processed_for_accuracy": 0,
        "correct_matches": 0,
        "accuracy": 0.0
    },
    {
        "model_name": "Phi-3-mini-4k-instruct-GPTQ",
        "timestamp": "20250424-114339",
        "input_output_file": "results/mod/48/ctibench_cti-vsp_Phi-3-mini-4k-instruct-GPTQ_20250424-114339_outputs.json",
        "input_metrics_file": "results/mod/48/ctibench_cti-vsp_Phi-3-mini-4k-instruct-GPTQ_20250424-114339_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-vsp_ground_truth.jsonl",
        "run_args": {
            "model_path": "/workspace/models/Phi-3-mini-4k-instruct-GPTQ",
            "model_type": "causal",
            "benchmark_name": "ctibench",
            "benchmark_path": "/workspace/datasets",
            "results_dir": "/workspace/results/mod/48",
            "device": "cuda",
            "max_new_tokens": 512,
            "num_samples": 100,
            "cti_subset": "cti-vsp"
        },
        "task_type": "vsp",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 100,
        "items_processed_for_accuracy": 0,
        "correct_matches": 0,
        "accuracy": 0.0
    },
    {
        "model_name": "Phi-3-mini-4k-instruct-GPTQ",
        "timestamp": "20250424-114436",
        "input_output_file": "results/mod/48/ctibench_cti-rcm_Phi-3-mini-4k-instruct-GPTQ_20250424-114436_outputs.json",
        "input_metrics_file": "results/mod/48/ctibench_cti-rcm_Phi-3-mini-4k-instruct-GPTQ_20250424-114436_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-rcm_ground_truth.jsonl",
        "run_args": {
            "model_path": "/workspace/models/Phi-3-mini-4k-instruct-GPTQ",
            "model_type": "causal",
            "benchmark_name": "ctibench",
            "benchmark_path": "/workspace/datasets",
            "results_dir": "/workspace/results/mod/48",
            "device": "cuda",
            "max_new_tokens": 512,
            "num_samples": 100,
            "cti_subset": "cti-rcm"
        },
        "task_type": "rcm",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 100,
        "items_processed_for_accuracy": 0,
        "correct_matches": 0,
        "accuracy": 0.0
    },
    {
        "model_name": "Phi-3-mini-4k-instruct-GPTQ",
        "timestamp": "20250424-114943",
        "input_output_file": "results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-114943_outputs.json",
        "input_metrics_file": "results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-114943_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-mcq_ground_truth.jsonl",
        "run_args": {
            "model_path": "/workspace/models/Phi-3-mini-4k-instruct-GPTQ",
            "model_type": "causal",
            "benchmark_name": "ctibench",
            "benchmark_path": "/workspace/datasets",
            "results_dir": "/workspace/results/mod/48",
            "device": "cuda",
            "max_new_tokens": 512,
            "num_samples": 100,
            "cti_subset": "cti-mcq"
        },
        "task_type": "mcq",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 100,
        "items_processed_for_accuracy": 0,
        "correct_matches": 0,
        "accuracy": 0.0
    },
    {
        "model_name": "Phi-3-mini-4k-instruct-GPTQ",
        "timestamp": "20250424-123649",
        "input_output_file": "results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-123649_outputs.json",
        "input_metrics_file": "results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-123649_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-mcq_ground_truth.jsonl",
        "run_args": {
            "model_path": "/workspace/models/Phi-3-mini-4k-instruct-GPTQ",
            "model_type": "causal",
            "benchmark_name": "ctibench",
            "benchmark_path": "/workspace/datasets",
            "results_dir": "/workspace/results/mod/48",
            "device": "cuda",
            "max_new_tokens": 512,
            "num_samples": 100,
            "cti_subset": "cti-mcq",
            "batch_size": 32,
            "trust_remote_code": false
        },
        "task_type": "mcq",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 0,
        "items_processed_for_accuracy": 100,
        "correct_matches": 60,
        "accuracy": 0.6
    },
    {
        "model_name": "Phi-3-mini-4k-instruct-GPTQ",
        "timestamp": "20250424-123729",
        "input_output_file": "results/mod/48/ctibench_cti-vsp_Phi-3-mini-4k-instruct-GPTQ_20250424-123729_outputs.json",
        "input_metrics_file": "results/mod/48/ctibench_cti-vsp_Phi-3-mini-4k-instruct-GPTQ_20250424-123729_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-vsp_ground_truth.jsonl",
        "run_args": {
            "model_path": "/workspace/models/Phi-3-mini-4k-instruct-GPTQ",
            "model_type": "causal",
            "benchmark_name": "ctibench",
            "benchmark_path": "/workspace/datasets",
            "results_dir": "/workspace/results/mod/48",
            "device": "cuda",
            "max_new_tokens": 512,
            "num_samples": 100,
            "cti_subset": "cti-vsp",
            "batch_size": 32,
            "trust_remote_code": false
        },
        "task_type": "vsp",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 39,
        "items_processed_for_accuracy": 61,
        "correct_matches": 0,
        "accuracy": 0.0
    },
    {
        "model_name": "Phi-3-mini-4k-instruct-GPTQ",
        "timestamp": "20250424-124534",
        "input_output_file": "results/mod/48/ctibench_cti-rcm_Phi-3-mini-4k-instruct-GPTQ_20250424-124534_outputs.json",
        "input_metrics_file": "results/mod/48/ctibench_cti-rcm_Phi-3-mini-4k-instruct-GPTQ_20250424-124534_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-rcm_ground_truth.jsonl",
        "run_args": {
            "model_path": "/workspace/models/Phi-3-mini-4k-instruct-GPTQ",
            "model_type": "causal",
            "benchmark_name": "ctibench",
            "benchmark_path": "/workspace/datasets",
            "results_dir": "/workspace/results/mod/48",
            "device": "cuda",
            "max_new_tokens": 512,
            "num_samples": 100,
            "cti_subset": "cti-rcm",
            "batch_size": 32,
            "trust_remote_code": false
        },
        "task_type": "rcm",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 1,
        "items_processed_for_accuracy": 99,
        "correct_matches": 15,
        "accuracy": 0.1515
    },
    {
        "model_name": "Mistral-7B-Instruct-v0.3-GPTQ",
        "timestamp": "20250424-131409",
        "input_output_file": "results/mod/48/ctibench_cti-mcq_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131409_outputs.json",
        "input_metrics_file": "results/mod/48/ctibench_cti-mcq_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131409_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-mcq_ground_truth.jsonl",
        "run_args": {
            "model_path": "/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ",
            "model_type": "causal",
            "benchmark_name": "ctibench",
            "benchmark_path": "/workspace/datasets",
            "results_dir": "/workspace/results/mod/48",
            "device": "cuda",
            "max_new_tokens": 512,
            "num_samples": 100,
            "cti_subset": "cti-mcq",
            "batch_size": 32,
            "trust_remote_code": true
        },
        "task_type": "mcq",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 0,
        "items_processed_for_accuracy": 100,
        "correct_matches": 32,
        "accuracy": 0.32
    },
    {
        "model_name": "Mistral-7B-Instruct-v0.3-GPTQ",
        "timestamp": "20250424-131806",
        "input_output_file": "results/mod/48/ctibench_cti-vsp_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131806_outputs.json",
        "input_metrics_file": "results/mod/48/ctibench_cti-vsp_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131806_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-vsp_ground_truth.jsonl",
        "run_args": {
            "model_path": "/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ",
            "model_type": "causal",
            "benchmark_name": "ctibench",
            "benchmark_path": "/workspace/datasets",
            "results_dir": "/workspace/results/mod/48",
            "device": "cuda",
            "max_new_tokens": 512,
            "num_samples": 100,
            "cti_subset": "cti-vsp",
            "batch_size": 32,
            "trust_remote_code": true
        },
        "task_type": "vsp",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 6,
        "items_processed_for_accuracy": 94,
        "correct_matches": 8,
        "accuracy": 0.0851
    },
    {
        "model_name": "Mistral-7B-Instruct-v0.3-GPTQ",
        "timestamp": "20250424-133011",
        "input_output_file": "results/mod/48/ctibench_cti-rcm_Mistral-7B-Instruct-v0.3-GPTQ_20250424-133011_outputs.json",
        "input_metrics_file": "results/mod/48/ctibench_cti-rcm_Mistral-7B-Instruct-v0.3-GPTQ_20250424-133011_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-rcm_ground_truth.jsonl",
        "run_args": {
            "model_path": "/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ",
            "model_type": "causal",
            "benchmark_name": "ctibench",
            "benchmark_path": "/workspace/datasets",
            "results_dir": "/workspace/results/mod/48",
            "device": "cuda",
            "max_new_tokens": 512,
            "num_samples": 100,
            "cti_subset": "cti-rcm",
            "batch_size": 32,
            "trust_remote_code": true
        },
        "task_type": "rcm",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 1,
        "items_processed_for_accuracy": 99,
        "correct_matches": 42,
        "accuracy": 0.4242
    }
]