{
    "model_name": "Mistral-7B-Instruct-v0.3-GPTQ",
    "timestamp": "20250424-133011",
    "input_output_file": "results/mod/48/ctibench_cti-rcm_Mistral-7B-Instruct-v0.3-GPTQ_20250424-133011_outputs.json",
    "input_metrics_file": "results/mod/48/ctibench_cti-rcm_Mistral-7B-Instruct-v0.3-GPTQ_20250424-133011_metrics.json",
    "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-rcm_ground_truth.jsonl",
    "run_args": {
        "model_path": "/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ",
        "model_type": "causal",
        "benchmark_name": "ctibench",
        "benchmark_path": "/workspace/datasets",
        "results_dir": "/workspace/results/mod/48",
        "device": "cuda",
        "max_new_tokens": 512,
        "num_samples": 100,
        "cti_subset": "cti-rcm",
        "batch_size": 32,
        "trust_remote_code": true
    },
    "task_type": "rcm",
    "total_items_in_output": 100,
    "items_with_ground_truth": 100,
    "items_missing_ground_truth": 0,
    "items_refused": 0,
    "items_parsing_error": 1,
    "items_processed_for_accuracy": 99,
    "correct_matches": 42,
    "accuracy": 0.4242
}