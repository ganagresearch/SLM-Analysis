[
    {
        "model_name": "TinyLlama-1.1B-Chat-v1.0",
        "timestamp": "20250422-174349",
        "input_output_file": "results/bare/48/ctibench_cti-mcq__20250422-174349_outputs.json",
        "input_metrics_file": "results/bare/48/ctibench_cti-mcq__20250422-174349_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-mcq_ground_truth.jsonl",
        "model_path": "/workspace/models/TinyLlama-1.1B-Chat-v1.0/",
        "model_type": "causal",
        "benchmark_name": "ctibench",
        "benchmark_path": "/workspace/datasets",
        "results_dir": "/workspace/results/bare/48",
        "device": "cuda",
        "max_new_tokens": 512,
        "num_samples": 100,
        "cti_subset": "cti-mcq",
        "task_type": "mcq",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 99,
        "items_processed_for_accuracy": 1,
        "correct_matches": 0,
        "accuracy": 0.0
    },
    {
        "model_name": "TinyLlama-1.1B-Chat-v1.0",
        "timestamp": "20250422-174551",
        "input_output_file": "results/bare/48/ctibench_cti-vsp__20250422-174551_outputs.json",
        "input_metrics_file": "results/bare/48/ctibench_cti-vsp__20250422-174551_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-vsp_ground_truth.jsonl",
        "model_path": "/workspace/models/TinyLlama-1.1B-Chat-v1.0/",
        "model_type": "causal",
        "benchmark_name": "ctibench",
        "benchmark_path": "/workspace/datasets",
        "results_dir": "/workspace/results/bare/48",
        "device": "cuda",
        "max_new_tokens": 512,
        "num_samples": 100,
        "cti_subset": "cti-vsp",
        "task_type": "vsp",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 18,
        "items_processed_for_accuracy": 82,
        "correct_matches": 17,
        "accuracy": 0.2073
    },
    {
        "model_name": "TinyLlama-1.1B-Chat-v1.0",
        "timestamp": "20250422-175438",
        "input_output_file": "results/bare/48/ctibench_cti-rcm__20250422-175438_outputs.json",
        "input_metrics_file": "results/bare/48/ctibench_cti-rcm__20250422-175438_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-rcm_ground_truth.jsonl",
        "model_path": "/workspace/models/TinyLlama-1.1B-Chat-v1.0/",
        "model_type": "causal",
        "benchmark_name": "ctibench",
        "benchmark_path": "/workspace/datasets",
        "results_dir": "/workspace/results/bare/48",
        "device": "cuda",
        "max_new_tokens": 512,
        "num_samples": 100,
        "cti_subset": "cti-rcm",
        "task_type": "rcm",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 33,
        "items_processed_for_accuracy": 67,
        "correct_matches": 17,
        "accuracy": 0.2537
    },
    {
        "model_name": "Phi-3-mini-4k-instruct",
        "timestamp": "20250422-184125",
        "input_output_file": "results/bare/48/ctibench_cti-mcq__20250422-184125_outputs.json",
        "input_metrics_file": "results/bare/48/ctibench_cti-mcq__20250422-184125_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-mcq_ground_truth.jsonl",
        "model_path": "/workspace/models/Phi-3-mini-4k-instruct/",
        "model_type": "causal",
        "benchmark_name": "ctibench",
        "benchmark_path": "/workspace/datasets",
        "results_dir": "/workspace/results/bare/48",
        "device": "cuda",
        "max_new_tokens": 512,
        "num_samples": 100,
        "cti_subset": "cti-mcq",
        "task_type": "mcq",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 0,
        "items_processed_for_accuracy": 100,
        "correct_matches": 63,
        "accuracy": 0.63
    },
    {
        "model_name": "Phi-3-mini-4k-instruct",
        "timestamp": "20250422-184259",
        "input_output_file": "results/bare/48/ctibench_cti-vsp__20250422-184259_outputs.json",
        "input_metrics_file": "results/bare/48/ctibench_cti-vsp__20250422-184259_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-vsp_ground_truth.jsonl",
        "model_path": "/workspace/models/Phi-3-mini-4k-instruct/",
        "model_type": "causal",
        "benchmark_name": "ctibench",
        "benchmark_path": "/workspace/datasets",
        "results_dir": "/workspace/results/bare/48",
        "device": "cuda",
        "max_new_tokens": 512,
        "num_samples": 100,
        "cti_subset": "cti-vsp",
        "task_type": "vsp",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 2,
        "items_processed_for_accuracy": 98,
        "correct_matches": 8,
        "accuracy": 0.0816
    },
    {
        "model_name": "Phi-3-mini-4k-instruct",
        "timestamp": "20250422-190448",
        "input_output_file": "results/bare/48/ctibench_cti-rcm__20250422-190448_outputs.json",
        "input_metrics_file": "results/bare/48/ctibench_cti-rcm__20250422-190448_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-rcm_ground_truth.jsonl",
        "model_path": "/workspace/models/Phi-3-mini-4k-instruct/",
        "model_type": "causal",
        "benchmark_name": "ctibench",
        "benchmark_path": "/workspace/datasets",
        "results_dir": "/workspace/results/bare/48",
        "device": "cuda",
        "max_new_tokens": 512,
        "num_samples": 100,
        "cti_subset": "cti-rcm",
        "task_type": "rcm",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 3,
        "items_processed_for_accuracy": 97,
        "correct_matches": 37,
        "accuracy": 0.3814
    },
    {
        "model_name": "Mistral-7B-Instruct-v0.3",
        "timestamp": "20250422-202049",
        "input_output_file": "results/bare/48/ctibench_cti-mcq__20250422-202049_outputs.json",
        "input_metrics_file": "results/bare/48/ctibench_cti-mcq__20250422-202049_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-mcq_ground_truth.jsonl",
        "model_path": "/workspace/models/Mistral-7B-Instruct-v0.3/",
        "model_type": "causal",
        "benchmark_name": "ctibench",
        "benchmark_path": "/workspace/datasets",
        "results_dir": "/workspace/results/bare/48",
        "device": "cuda",
        "max_new_tokens": 512,
        "num_samples": 100,
        "cti_subset": "cti-mcq",
        "task_type": "mcq",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 0,
        "items_processed_for_accuracy": 100,
        "correct_matches": 33,
        "accuracy": 0.33
    },
    {
        "model_name": "Mistral-7B-Instruct-v0.3",
        "timestamp": "20250422-202352",
        "input_output_file": "results/bare/48/ctibench_cti-vsp__20250422-202352_outputs.json",
        "input_metrics_file": "results/bare/48/ctibench_cti-vsp__20250422-202352_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-vsp_ground_truth.jsonl",
        "model_path": "/workspace/models/Mistral-7B-Instruct-v0.3/",
        "model_type": "causal",
        "benchmark_name": "ctibench",
        "benchmark_path": "/workspace/datasets",
        "results_dir": "/workspace/results/bare/48",
        "device": "cuda",
        "max_new_tokens": 512,
        "num_samples": 100,
        "cti_subset": "cti-vsp",
        "task_type": "vsp",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 4,
        "items_processed_for_accuracy": 96,
        "correct_matches": 9,
        "accuracy": 0.0938
    },
    {
        "model_name": "Mistral-7B-Instruct-v0.3",
        "timestamp": "20250422-210328",
        "input_output_file": "results/bare/48/ctibench_cti-rcm__20250422-210328_outputs.json",
        "input_metrics_file": "results/bare/48/ctibench_cti-rcm__20250422-210328_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-rcm_ground_truth.jsonl",
        "model_path": "/workspace/models/Mistral-7B-Instruct-v0.3/",
        "model_type": "causal",
        "benchmark_name": "ctibench",
        "benchmark_path": "/workspace/datasets",
        "results_dir": "/workspace/results/bare/48",
        "device": "cuda",
        "max_new_tokens": 512,
        "num_samples": 100,
        "cti_subset": "cti-rcm",
        "task_type": "rcm",
        "total_items_in_output": 100,
        "items_with_ground_truth": 100,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 0,
        "items_processed_for_accuracy": 100,
        "correct_matches": 46,
        "accuracy": 0.46
    },
    {
        "model_name": "electra-small-discriminator",
        "timestamp": "20250422-212417",
        "input_output_file": "results/bare/48/ctibench_cti-mcq__20250422-212417_outputs.json",
        "input_metrics_file": "results/bare/48/ctibench_cti-mcq__20250422-212417_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-mcq_ground_truth.jsonl",
        "model_path": "/workspace/models/electra-small-discriminator/",
        "model_type": "encoder",
        "benchmark_name": "ctibench",
        "benchmark_path": "/workspace/datasets",
        "results_dir": "/workspace/results/bare/48",
        "device": "cuda",
        "max_new_tokens": 256,
        "num_samples": 100,
        "cti_subset": "cti-mcq",
        "task_type": "mcq",
        "total_items_in_output": 0,
        "items_with_ground_truth": 0,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 0,
        "items_processed_for_accuracy": 0,
        "correct_matches": 0,
        "accuracy": 0.0
    },
    {
        "model_name": "electra-small-discriminator",
        "timestamp": "20250422-212450",
        "input_output_file": "results/bare/48/ctibench_cti-vsp__20250422-212450_outputs.json",
        "input_metrics_file": "results/bare/48/ctibench_cti-vsp__20250422-212450_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-vsp_ground_truth.jsonl",
        "model_path": "/workspace/models/electra-small-discriminator/",
        "model_type": "encoder",
        "benchmark_name": "ctibench",
        "benchmark_path": "/workspace/datasets",
        "results_dir": "/workspace/results/bare/48",
        "device": "cuda",
        "max_new_tokens": 256,
        "num_samples": 100,
        "cti_subset": "cti-vsp",
        "task_type": "vsp",
        "total_items_in_output": 0,
        "items_with_ground_truth": 0,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 0,
        "items_processed_for_accuracy": 0,
        "correct_matches": 0,
        "accuracy": 0.0
    },
    {
        "model_name": "electra-small-discriminator",
        "timestamp": "20250422-212523",
        "input_output_file": "results/bare/48/ctibench_cti-rcm__20250422-212523_outputs.json",
        "input_metrics_file": "results/bare/48/ctibench_cti-rcm__20250422-212523_metrics.json",
        "input_ground_truth_file": "code/analysis/ctibench_ground_truth/cti-rcm_ground_truth.jsonl",
        "model_path": "/workspace/models/electra-small-discriminator/",
        "model_type": "encoder",
        "benchmark_name": "ctibench",
        "benchmark_path": "/workspace/datasets",
        "results_dir": "/workspace/results/bare/48",
        "device": "cuda",
        "max_new_tokens": 256,
        "num_samples": 100,
        "cti_subset": "cti-rcm",
        "task_type": "rcm",
        "total_items_in_output": 0,
        "items_with_ground_truth": 0,
        "items_missing_ground_truth": 0,
        "items_refused": 0,
        "items_parsing_error": 0,
        "items_processed_for_accuracy": 0,
        "correct_matches": 0,
        "accuracy": 0.0
    }
]