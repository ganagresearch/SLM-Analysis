2025-04-23 19:17:37,713 - INFO - --- System Information ---
2025-04-23 19:17:37,713 - INFO - Python Version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]
2025-04-23 19:17:37,713 - INFO - Torch Version: 2.6.0+cu124
2025-04-23 19:17:37,714 - INFO - Datasets Version: 3.5.0
2025-04-23 19:17:37,714 - INFO - CPU Count: 96
2025-04-23 19:17:37,714 - INFO - Total RAM: 503.51 GB
2025-04-23 19:17:37,757 - INFO - GPU: NVIDIA A40
2025-04-23 19:17:37,758 - INFO - Total VRAM: 44.45 GB
2025-04-23 19:17:37,758 - INFO - CUDA Version: 12.4
2025-04-23 19:17:37,759 - INFO - Using Device: cuda:0
2025-04-23 19:17:37,759 - INFO - --------------------------
2025-04-23 19:17:37,759 - INFO - Loading calibration dataset from: /workspace/calibration_data/sevenllm_instruct_subset_manual/
2025-04-23 19:17:37,796 - INFO - Loaded and selected 200 calibration samples.
2025-04-23 19:17:37,796 - INFO - Using column 'instruction' for calibration text.
2025-04-23 19:17:37,796 - INFO - --- Starting Model Quantization ---
2025-04-23 19:17:37,797 - INFO - 
Processing model: TinyLlama-1.1B-Chat-v1.0
2025-04-23 19:17:37,797 - INFO - Input path (HF): /workspace/models/TinyLlama-1.1B-Chat-v1.0/
2025-04-23 19:17:37,797 - INFO - ONNX Export Path (Intermediate): /workspace/models/TinyLlama-1.1B-Chat-v1.0/temp-fp32-onnx-export
2025-04-23 19:17:37,798 - INFO - Output path (Quantized ONNX files): /workspace/models/TinyLlama-1.1B-Chat-v1.0/
2025-04-23 19:17:37,798 - INFO - Trust remote code: False
2025-04-23 19:17:37,798 - INFO - Model Task: text-generation
2025-04-23 19:17:37,800 - WARNING - Found existing intermediate FP32 ONNX model at /workspace/models/TinyLlama-1.1B-Chat-v1.0/temp-fp32-onnx-export/model.onnx. Skipping export.
2025-04-23 19:17:37,800 - INFO - Using FP32 ONNX model: /workspace/models/TinyLlama-1.1B-Chat-v1.0/temp-fp32-onnx-export/model.onnx
2025-04-23 19:17:37,800 - INFO - Loading quantizer for the exported ONNX model...
2025-04-23 19:17:37,829 - INFO - Loading tokenizer from original HF path...
2025-04-23 19:17:37,992 - INFO - Quantizer and tokenizer loaded.
2025-04-23 19:17:37,992 - INFO - Preprocessing (tokenizing) the calibration dataset...
2025-04-23 19:17:38,019 - INFO - Preprocessing complete. New columns: ['input_ids', 'attention_mask', 'position_ids']
2025-04-23 19:17:38,019 - INFO - Defining quantization and calibration configurations...
2025-04-23 19:17:38,020 - INFO - Quantization Config: QDQ (mode: QLinearOps, schema: u8/s8, channel-wise: False)
2025-04-23 19:17:38,020 - INFO - Creating Calibration Config using preprocessed dataset.
2025-04-23 19:17:38,020 - INFO - Calibration Method: CalibrationMethod.MinMax
2025-04-23 19:17:38,020 - INFO - Operators to quantize: ['MatMul', 'Add']
2025-04-23 19:17:38,020 - INFO - Starting calibration step (quantizer.fit)...
2025-04-23 19:19:02,885 - INFO - Calibration step (fit) successful. Duration: 0:01:24.864447
2025-04-23 19:19:02,886 - INFO - Computed calibration ranges (TensorsData object returned).
2025-04-23 19:19:02,886 - INFO - Applying quantization and exporting INT8 model (quantizer.quantize)...
2025-04-23 19:19:38,822 - INFO - Peak PyTorch VRAM allocated during quantization export: 0.00 MB
2025-04-23 19:19:38,823 - INFO - Quantized model export successful for TinyLlama-1.1B-Chat-v1.0!
2025-04-23 19:19:38,823 - INFO - Quantized model saved to: /workspace/models/TinyLlama-1.1B-Chat-v1.0/model_quantized.onnx
2025-04-23 19:19:38,823 - INFO - Quantization export duration: 0:00:35.935122
2025-04-23 19:19:38,823 - INFO - Removing intermediate ONNX export directory: /workspace/models/TinyLlama-1.1B-Chat-v1.0/temp-fp32-onnx-export
2025-04-23 19:19:39,834 - INFO - --- Model Quantization Finished ---
