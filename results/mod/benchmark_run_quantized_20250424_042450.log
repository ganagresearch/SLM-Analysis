2025-04-24 04:24:50,682 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 04:24:50,682 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 04:24:50,682 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 04:24:50,682 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 04:24:50,682 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 04:24:50,682 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_042450.log
2025-04-24 04:24:50,682 - INFO - Models to run: ['TinyLlama-1.1B-Chat-v1.0-GPTQ', 'Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 04:24:50,682 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['attack', 'exploit_target', 'threat_actor', 'ttps'])]
2025-04-24 04:24:50,682 - INFO - Running 100 samples per benchmark.
2025-04-24 04:24:50,682 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 04:24:50,682 - INFO - 
Run 1/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 04:24:50,682 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 04:31:29,826 - WARNING - Stderr:
2025-04-24 04:24:52,813 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:24:53,121 - INFO - --- Starting Evaluation ---
2025-04-24 04:24:53,121 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 04:24:53,121 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:24:53,121 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre__20250424-042453_metrics.json
2025-04-24 04:24:53,121 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre__20250424-042453_outputs.json
2025-04-24 04:24:53,121 - INFO - NVML Reporting Active: False
2025-04-24 04:24:53,121 - INFO - Initial RAM usage: 555.29 MB
2025-04-24 04:24:53,121 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/...
2025-04-24 04:24:53,142 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:24:53,237 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:24:55,125 - INFO - CausalLM Model loaded.
2025-04-24 04:24:55,125 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:24:55,126 - INFO - Model loaded in 2.00 seconds.
2025-04-24 04:24:55,126 - INFO - Loading prompts from CyberSecEval3 MITRE file: /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json
2025-04-24 04:24:55,135 - INFO - Processing the first 100 items.

Parsing MITRE prompts:   0%|          | 0/100 [00:00<?, ?it/s]2025-04-24 04:24:55,137 - WARNING - Item 5: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,137 - WARNING - Item 8: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,138 - WARNING - Item 14: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,138 - WARNING - Item 21: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,139 - WARNING - Item 25: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,139 - WARNING - Item 27: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,139 - WARNING - Item 30: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,139 - WARNING - Item 32: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,140 - WARNING - Item 35: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,140 - WARNING - Item 36: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,140 - WARNING - Item 48: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,141 - WARNING - Item 49: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,141 - WARNING - Item 52: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,141 - WARNING - Item 58: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,142 - WARNING - Item 63: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,142 - WARNING - Item 72: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,143 - WARNING - Item 81: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,143 - WARNING - Item 86: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,143 - WARNING - Item 88: Regex failed to find/extract 'prompt' value from mutated_prompt_str.

Parsing MITRE prompts: 100%|██████████| 100/100 [00:00<00:00, 13249.63it/s]
2025-04-24 04:24:55,144 - INFO - Successfully loaded 81 prompts from cyberseceval3_mitre.
2025-04-24 04:24:55,144 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 04:24:55,145 - INFO - Starting inference on 81 samples...

Inference Progress:   0%|          | 0/81 [00:00<?, ?it/s]
Inference Progress:   1%|          | 1/81 [00:05<07:22,  5.53s/it]
Inference Progress:   2%|▏         | 2/81 [00:10<06:57,  5.28s/it]
Inference Progress:   4%|▎         | 3/81 [00:15<06:46,  5.22s/it]
Inference Progress:   5%|▍         | 4/81 [00:19<05:53,  4.59s/it]
Inference Progress:   6%|▌         | 5/81 [00:22<05:12,  4.11s/it]
Inference Progress:   7%|▋         | 6/81 [00:26<04:54,  3.93s/it]
Inference Progress:   9%|▊         | 7/81 [00:31<05:19,  4.31s/it]
Inference Progress:  10%|▉         | 8/81 [00:36<05:33,  4.57s/it]
Inference Progress:  11%|█         | 9/81 [00:41<05:40,  4.73s/it]
Inference Progress:  12%|█▏        | 10/81 [00:46<05:44,  4.85s/it]
Inference Progress:  14%|█▎        | 11/81 [00:50<05:22,  4.61s/it]
Inference Progress:  15%|█▍        | 12/81 [00:55<05:28,  4.76s/it]
Inference Progress:  16%|█▌        | 13/81 [01:00<05:20,  4.71s/it]
Inference Progress:  17%|█▋        | 14/81 [01:05<05:23,  4.83s/it]
Inference Progress:  19%|█▊        | 15/81 [01:10<05:23,  4.91s/it]
Inference Progress:  20%|█▉        | 16/81 [01:15<05:13,  4.82s/it]
Inference Progress:  21%|██        | 17/81 [01:20<05:13,  4.91s/it]
Inference Progress:  22%|██▏       | 18/81 [01:25<05:12,  4.96s/it]
Inference Progress:  23%|██▎       | 19/81 [01:30<05:09,  4.99s/it]
Inference Progress:  25%|██▍       | 20/81 [01:35<05:06,  5.02s/it]
Inference Progress:  26%|██▌       | 21/81 [01:40<05:03,  5.05s/it]
Inference Progress:  27%|██▋       | 22/81 [01:45<04:58,  5.06s/it]
Inference Progress:  28%|██▊       | 23/81 [01:50<04:54,  5.07s/it]
Inference Progress:  30%|██▉       | 24/81 [01:55<04:49,  5.08s/it]
Inference Progress:  31%|███       | 25/81 [02:01<04:44,  5.09s/it]
Inference Progress:  32%|███▏      | 26/81 [02:06<04:39,  5.09s/it]
Inference Progress:  33%|███▎      | 27/81 [02:11<04:34,  5.09s/it]
Inference Progress:  35%|███▍      | 28/81 [02:16<04:29,  5.08s/it]
Inference Progress:  36%|███▌      | 29/81 [02:21<04:24,  5.09s/it]
Inference Progress:  37%|███▋      | 30/81 [02:26<04:19,  5.09s/it]
Inference Progress:  38%|███▊      | 31/81 [02:31<04:14,  5.09s/it]
Inference Progress:  40%|███▉      | 32/81 [02:34<03:42,  4.55s/it]
Inference Progress:  41%|████      | 33/81 [02:39<03:46,  4.71s/it]
Inference Progress:  42%|████▏     | 34/81 [02:45<03:46,  4.83s/it]
Inference Progress:  43%|████▎     | 35/81 [02:50<03:45,  4.91s/it]
Inference Progress:  44%|████▍     | 36/81 [02:55<03:43,  4.97s/it]
Inference Progress:  46%|████▌     | 37/81 [03:00<03:40,  5.01s/it]
Inference Progress:  47%|████▋     | 38/81 [03:04<03:22,  4.70s/it]
Inference Progress:  48%|████▊     | 39/81 [03:09<03:22,  4.82s/it]
Inference Progress:  49%|████▉     | 40/81 [03:14<03:21,  4.91s/it]
Inference Progress:  51%|█████     | 41/81 [03:19<03:18,  4.97s/it]
Inference Progress:  52%|█████▏    | 42/81 [03:24<03:15,  5.01s/it]
Inference Progress:  53%|█████▎    | 43/81 [03:29<03:11,  5.03s/it]
Inference Progress:  54%|█████▍    | 44/81 [03:34<03:06,  5.05s/it]
Inference Progress:  56%|█████▌    | 45/81 [03:40<03:02,  5.07s/it]
Inference Progress:  57%|█████▋    | 46/81 [03:45<02:57,  5.08s/it]
Inference Progress:  58%|█████▊    | 47/81 [03:50<02:52,  5.08s/it]
Inference Progress:  59%|█████▉    | 48/81 [03:55<02:47,  5.09s/it]
Inference Progress:  60%|██████    | 49/81 [03:57<02:13,  4.17s/it]
Inference Progress:  62%|██████▏   | 50/81 [04:02<02:18,  4.45s/it]
Inference Progress:  63%|██████▎   | 51/81 [04:07<02:19,  4.65s/it]
Inference Progress:  64%|██████▍   | 52/81 [04:12<02:18,  4.78s/it]
Inference Progress:  65%|██████▌   | 53/81 [04:17<02:16,  4.88s/it]
Inference Progress:  67%|██████▋   | 54/81 [04:22<02:13,  4.94s/it]
Inference Progress:  68%|██████▊   | 55/81 [04:28<02:09,  4.99s/it]
Inference Progress:  69%|██████▉   | 56/81 [04:33<02:05,  5.02s/it]
Inference Progress:  70%|███████   | 57/81 [04:38<02:01,  5.04s/it]
Inference Progress:  72%|███████▏  | 58/81 [04:42<01:48,  4.72s/it]
Inference Progress:  73%|███████▎  | 59/81 [04:47<01:46,  4.84s/it]
Inference Progress:  74%|███████▍  | 60/81 [04:52<01:43,  4.91s/it]
Inference Progress:  75%|███████▌  | 61/81 [04:57<01:39,  4.97s/it]
Inference Progress:  77%|███████▋  | 62/81 [05:02<01:35,  5.00s/it]
Inference Progress:  78%|███████▊  | 63/81 [05:07<01:30,  5.03s/it]
Inference Progress:  79%|███████▉  | 64/81 [05:12<01:25,  5.05s/it]
Inference Progress:  80%|████████  | 65/81 [05:17<01:21,  5.06s/it]
Inference Progress:  81%|████████▏ | 66/81 [05:21<01:08,  4.55s/it]
Inference Progress:  83%|████████▎ | 67/81 [05:24<01:00,  4.32s/it]
Inference Progress:  84%|████████▍ | 68/81 [05:30<00:59,  4.55s/it]
Inference Progress:  85%|████████▌ | 69/81 [05:35<00:56,  4.71s/it]
Inference Progress:  86%|████████▋ | 70/81 [05:40<00:53,  4.82s/it]
Inference Progress:  88%|████████▊ | 71/81 [05:45<00:48,  4.86s/it]
Inference Progress:  89%|████████▉ | 72/81 [05:50<00:44,  4.92s/it]
Inference Progress:  90%|█████████ | 73/81 [05:55<00:39,  4.96s/it]
Inference Progress:  91%|█████████▏| 74/81 [06:00<00:34,  5.00s/it]
Inference Progress:  93%|█████████▎| 75/81 [06:05<00:30,  5.02s/it]
Inference Progress:  94%|█████████▍| 76/81 [06:10<00:25,  5.05s/it]
Inference Progress:  95%|█████████▌| 77/81 [06:15<00:19,  4.93s/it]
Inference Progress:  96%|█████████▋| 78/81 [06:20<00:14,  4.98s/it]
Inference Progress:  98%|█████████▊| 79/81 [06:25<00:10,  5.01s/it]
Inference Progress:  99%|█████████▉| 80/81 [06:30<00:05,  5.03s/it]
Inference Progress: 100%|██████████| 81/81 [06:33<00:00,  4.56s/it]
Inference Progress: 100%|██████████| 81/81 [06:33<00:00,  4.86s/it]
2025-04-24 04:31:29,068 - INFO - 
--- Inference Summary ---
2025-04-24 04:31:29,068 - INFO - Processed 81 samples.
2025-04-24 04:31:29,068 - INFO - Total 'generate' time (sum): 393.78 sec
2025-04-24 04:31:29,068 - INFO - Overall inference loop duration: 393.92 sec
2025-04-24 04:31:29,068 - INFO - Average 'generate' time per sample: 4.8615 sec
2025-04-24 04:31:29,068 - INFO - Total effective tokens generated: 39520
2025-04-24 04:31:29,068 - INFO - Overall effective tokens per second: 100.36
2025-04-24 04:31:29,068 - INFO - RAM Delta during inference: 550.56 MB
2025-04-24 04:31:29,068 - INFO - PyTorch VRAM Peak Delta during inference: 67.51 MB
2025-04-24 04:31:29,070 - INFO - Saved outputs (81 items) to: /workspace/results/mod/48/cyberseceval3_mitre__20250424-042453_outputs.json
2025-04-24 04:31:29,070 - INFO - Saved metrics to: /workspace/results/mod/48/cyberseceval3_mitre__20250424-042453_metrics.json
2025-04-24 04:31:29,070 - INFO - Cleaning up resources...
2025-04-24 04:31:29,072 - INFO - CUDA cache cleared.
2025-04-24 04:31:29,072 - INFO - --- Evaluation Complete ---

2025-04-24 04:31:29,826 - INFO - SUCCESS: Run 1/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 04:31:29,826 - INFO - 
Run 2/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 04:31:29,826 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 04:37:12,970 - WARNING - Stderr:
2025-04-24 04:31:32,006 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:31:32,340 - INFO - --- Starting Evaluation ---
2025-04-24 04:31:32,340 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 04:31:32,340 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:31:32,340 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench__20250424-043132_metrics.json
2025-04-24 04:31:32,340 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench__20250424-043132_outputs.json
2025-04-24 04:31:32,340 - INFO - NVML Reporting Active: False
2025-04-24 04:31:32,340 - INFO - Initial RAM usage: 553.75 MB
2025-04-24 04:31:32,340 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/...
2025-04-24 04:31:32,361 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:31:32,456 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:31:34,333 - INFO - CausalLM Model loaded.
2025-04-24 04:31:34,334 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:31:34,334 - INFO - Model loaded in 1.99 seconds.
2025-04-24 04:31:34,334 - INFO - Loading prompts from SEvenLLM saved dataset directory: /workspace/calibration_data/sevenllm_instruct_subset_manual/
2025-04-24 04:31:34,337 - INFO - Loaded dataset: Dataset({
    features: ['id', 'output', 'instruction', 'input', 'category', 'thought'],
    num_rows: 200
})
2025-04-24 04:31:34,338 - INFO - Selected first 100 samples for evaluation.

Parsing SEvenLLM prompts:   0%|          | 0/100 [00:00<?, ?it/s]
Parsing SEvenLLM prompts: 100%|██████████| 100/100 [00:00<00:00, 26049.96it/s]
2025-04-24 04:31:34,343 - INFO - Successfully loaded 100 prompts from sevenllm_bench.
2025-04-24 04:31:34,343 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 04:31:34,343 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]
Inference Progress:   1%|          | 1/100 [00:05<09:09,  5.55s/it]
Inference Progress:   2%|▏         | 2/100 [00:07<05:43,  3.51s/it]
Inference Progress:   3%|▎         | 3/100 [00:11<06:09,  3.81s/it]
Inference Progress:   4%|▍         | 4/100 [00:16<06:55,  4.33s/it]
Inference Progress:   5%|▌         | 5/100 [00:18<05:18,  3.35s/it]
Inference Progress:   6%|▌         | 6/100 [00:23<06:11,  3.96s/it]
Inference Progress:   7%|▋         | 7/100 [00:25<05:10,  3.33s/it]
Inference Progress:   8%|▊         | 8/100 [00:30<05:58,  3.90s/it]
Inference Progress:   9%|▉         | 9/100 [00:33<05:23,  3.56s/it]
Inference Progress:  10%|█         | 10/100 [00:38<06:03,  4.03s/it]
Inference Progress:  11%|█         | 11/100 [00:42<05:45,  3.88s/it]
Inference Progress:  12%|█▏        | 12/100 [00:47<06:14,  4.25s/it]
Inference Progress:  13%|█▎        | 13/100 [00:50<05:30,  3.80s/it]
Inference Progress:  14%|█▍        | 14/100 [00:54<05:48,  4.06s/it]
Inference Progress:  15%|█▌        | 15/100 [00:57<05:04,  3.58s/it]
Inference Progress:  16%|█▌        | 16/100 [01:00<04:49,  3.45s/it]
Inference Progress:  17%|█▋        | 17/100 [01:03<04:33,  3.30s/it]
Inference Progress:  18%|█▊        | 18/100 [01:08<05:14,  3.84s/it]
Inference Progress:  19%|█▉        | 19/100 [01:10<04:34,  3.39s/it]
Inference Progress:  20%|██        | 20/100 [01:15<05:11,  3.90s/it]
Inference Progress:  21%|██        | 21/100 [01:17<04:23,  3.34s/it]
Inference Progress:  22%|██▏       | 22/100 [01:23<05:02,  3.88s/it]
Inference Progress:  23%|██▎       | 23/100 [01:24<04:11,  3.27s/it]
Inference Progress:  24%|██▍       | 24/100 [01:27<03:44,  2.95s/it]
Inference Progress:  25%|██▌       | 25/100 [01:28<03:00,  2.41s/it]
Inference Progress:  26%|██▌       | 26/100 [01:32<03:48,  3.09s/it]
Inference Progress:  27%|██▋       | 27/100 [01:35<03:28,  2.85s/it]
Inference Progress:  28%|██▊       | 28/100 [01:37<03:03,  2.55s/it]
Inference Progress:  29%|██▉       | 29/100 [01:42<03:55,  3.32s/it]
Inference Progress:  30%|███       | 30/100 [01:43<03:20,  2.86s/it]
Inference Progress:  31%|███       | 31/100 [01:49<04:04,  3.54s/it]
Inference Progress:  32%|███▏      | 32/100 [01:51<03:46,  3.32s/it]
Inference Progress:  33%|███▎      | 33/100 [01:57<04:18,  3.86s/it]
Inference Progress:  34%|███▍      | 34/100 [01:59<03:50,  3.49s/it]
Inference Progress:  35%|███▌      | 35/100 [02:03<03:47,  3.50s/it]
Inference Progress:  36%|███▌      | 36/100 [02:08<04:09,  3.90s/it]
Inference Progress:  37%|███▋      | 37/100 [02:12<04:07,  3.93s/it]
Inference Progress:  38%|███▊      | 38/100 [02:13<03:19,  3.21s/it]
Inference Progress:  39%|███▉      | 39/100 [02:18<03:50,  3.78s/it]
Inference Progress:  40%|████      | 40/100 [02:21<03:23,  3.39s/it]
Inference Progress:  41%|████      | 41/100 [02:23<02:54,  2.97s/it]
Inference Progress:  42%|████▏     | 42/100 [02:24<02:20,  2.42s/it]
Inference Progress:  43%|████▎     | 43/100 [02:29<03:04,  3.23s/it]
Inference Progress:  44%|████▍     | 44/100 [02:34<03:32,  3.80s/it]
Inference Progress:  45%|████▌     | 45/100 [02:36<03:04,  3.36s/it]
Inference Progress:  46%|████▌     | 46/100 [02:38<02:37,  2.92s/it]
Inference Progress:  47%|████▋     | 47/100 [02:41<02:31,  2.86s/it]
Inference Progress:  48%|████▊     | 48/100 [02:44<02:37,  3.02s/it]
Inference Progress:  49%|████▉     | 49/100 [02:47<02:35,  3.05s/it]
Inference Progress:  50%|█████     | 50/100 [02:53<03:03,  3.67s/it]
Inference Progress:  51%|█████     | 51/100 [02:58<03:20,  4.10s/it]
Inference Progress:  52%|█████▏    | 52/100 [03:03<03:31,  4.40s/it]
Inference Progress:  53%|█████▎    | 53/100 [03:08<03:36,  4.61s/it]
Inference Progress:  54%|█████▍    | 54/100 [03:13<03:39,  4.76s/it]
Inference Progress:  55%|█████▌    | 55/100 [03:14<02:47,  3.72s/it]
Inference Progress:  56%|█████▌    | 56/100 [03:15<02:07,  2.89s/it]
Inference Progress:  57%|█████▋    | 57/100 [03:17<01:45,  2.46s/it]
Inference Progress:  58%|█████▊    | 58/100 [03:22<02:16,  3.26s/it]
Inference Progress:  59%|█████▉    | 59/100 [03:27<02:36,  3.81s/it]
Inference Progress:  60%|██████    | 60/100 [03:30<02:21,  3.54s/it]
Inference Progress:  61%|██████    | 61/100 [03:32<02:04,  3.20s/it]
Inference Progress:  62%|██████▏   | 62/100 [03:35<02:00,  3.18s/it]
Inference Progress:  63%|██████▎   | 63/100 [03:40<02:19,  3.76s/it]
Inference Progress:  64%|██████▍   | 64/100 [03:44<02:08,  3.57s/it]
Inference Progress:  65%|██████▌   | 65/100 [03:49<02:21,  4.04s/it]
Inference Progress:  66%|██████▌   | 66/100 [03:51<01:57,  3.45s/it]
Inference Progress:  67%|██████▋   | 67/100 [03:56<02:10,  3.95s/it]
Inference Progress:  68%|██████▊   | 68/100 [03:59<01:54,  3.59s/it]
Inference Progress:  69%|██████▉   | 69/100 [03:59<01:25,  2.75s/it]
Inference Progress:  70%|███████   | 70/100 [04:02<01:20,  2.68s/it]
Inference Progress:  71%|███████   | 71/100 [04:07<01:39,  3.41s/it]
Inference Progress:  72%|███████▏  | 72/100 [04:10<01:30,  3.23s/it]
Inference Progress:  73%|███████▎  | 73/100 [04:13<01:22,  3.06s/it]
Inference Progress:  74%|███████▍  | 74/100 [04:15<01:12,  2.80s/it]
Inference Progress:  75%|███████▌  | 75/100 [04:17<01:05,  2.64s/it]
Inference Progress:  76%|███████▌  | 76/100 [04:20<01:05,  2.73s/it]
Inference Progress:  77%|███████▋  | 77/100 [04:23<01:03,  2.78s/it]
Inference Progress:  78%|███████▊  | 78/100 [04:28<01:16,  3.48s/it]
Inference Progress:  79%|███████▉  | 79/100 [04:31<01:11,  3.40s/it]
Inference Progress:  80%|████████  | 80/100 [04:35<01:11,  3.56s/it]
Inference Progress:  81%|████████  | 81/100 [04:38<01:02,  3.31s/it]
Inference Progress:  82%|████████▏ | 82/100 [04:40<00:50,  2.82s/it]
Inference Progress:  83%|████████▎ | 83/100 [04:41<00:41,  2.45s/it]
Inference Progress:  84%|████████▍ | 84/100 [04:44<00:38,  2.43s/it]
Inference Progress:  85%|████████▌ | 85/100 [04:49<00:48,  3.24s/it]
Inference Progress:  86%|████████▌ | 86/100 [04:54<00:53,  3.80s/it]
Inference Progress:  87%|████████▋ | 87/100 [04:59<00:54,  4.20s/it]
Inference Progress:  88%|████████▊ | 88/100 [05:01<00:42,  3.50s/it]
Inference Progress:  89%|████████▉ | 89/100 [05:03<00:33,  3.02s/it]
Inference Progress:  90%|█████████ | 90/100 [05:05<00:28,  2.85s/it]
Inference Progress:  91%|█████████ | 91/100 [05:08<00:26,  2.89s/it]
Inference Progress:  92%|█████████▏| 92/100 [05:12<00:25,  3.20s/it]
Inference Progress:  93%|█████████▎| 93/100 [05:16<00:24,  3.53s/it]
Inference Progress:  94%|█████████▍| 94/100 [05:19<00:19,  3.28s/it]
Inference Progress:  95%|█████████▌| 95/100 [05:21<00:14,  2.89s/it]
Inference Progress:  96%|█████████▌| 96/100 [05:24<00:11,  2.96s/it]
Inference Progress:  97%|█████████▋| 97/100 [05:26<00:08,  2.72s/it]
Inference Progress:  98%|█████████▊| 98/100 [05:31<00:06,  3.44s/it]
Inference Progress:  99%|█████████▉| 99/100 [05:35<00:03,  3.37s/it]
Inference Progress: 100%|██████████| 100/100 [05:37<00:00,  3.20s/it]
Inference Progress: 100%|██████████| 100/100 [05:37<00:00,  3.38s/it]
2025-04-24 04:37:12,227 - INFO - 
--- Inference Summary ---
2025-04-24 04:37:12,227 - INFO - Processed 100 samples.
2025-04-24 04:37:12,227 - INFO - Total 'generate' time (sum): 337.73 sec
2025-04-24 04:37:12,227 - INFO - Overall inference loop duration: 337.88 sec
2025-04-24 04:37:12,227 - INFO - Average 'generate' time per sample: 3.3773 sec
2025-04-24 04:37:12,227 - INFO - Total effective tokens generated: 33702
2025-04-24 04:37:12,227 - INFO - Overall effective tokens per second: 99.79
2025-04-24 04:37:12,227 - INFO - RAM Delta during inference: 559.53 MB
2025-04-24 04:37:12,227 - INFO - PyTorch VRAM Peak Delta during inference: 102.09 MB
2025-04-24 04:37:12,229 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/sevenllm_bench__20250424-043132_outputs.json
2025-04-24 04:37:12,229 - INFO - Saved metrics to: /workspace/results/mod/48/sevenllm_bench__20250424-043132_metrics.json
2025-04-24 04:37:12,229 - INFO - Cleaning up resources...
2025-04-24 04:37:12,232 - INFO - CUDA cache cleared.
2025-04-24 04:37:12,232 - INFO - --- Evaluation Complete ---

2025-04-24 04:37:12,970 - INFO - SUCCESS: Run 2/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 04:37:12,970 - INFO - 
Run 3/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='attack'
2025-04-24 04:37:12,970 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset attack --max-new-tokens 512
2025-04-24 04:37:19,258 - WARNING - Stderr:
2025-04-24 04:37:15,104 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:37:15,510 - INFO - --- Starting Evaluation ---
2025-04-24 04:37:15,510 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'attack'}
2025-04-24 04:37:15,510 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:37:15,510 - INFO - Metrics file: /workspace/results/mod/48/ctibench_attack__20250424-043715_metrics.json
2025-04-24 04:37:15,510 - INFO - Outputs file: /workspace/results/mod/48/ctibench_attack__20250424-043715_outputs.json
2025-04-24 04:37:15,510 - INFO - NVML Reporting Active: False
2025-04-24 04:37:15,510 - INFO - Initial RAM usage: 556.65 MB
2025-04-24 04:37:15,510 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/...
2025-04-24 04:37:15,531 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:37:15,625 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:37:17,525 - INFO - CausalLM Model loaded.
2025-04-24 04:37:17,525 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:37:17,526 - INFO - Model loaded in 2.01 seconds.
2025-04-24 04:37:17,526 - INFO - Loading prompts from CTIBench subset: attack
2025-04-24 04:37:18,663 - ERROR - Error loading/processing ctibench (attack): BuilderConfig 'attack' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'attack' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:37:18,664 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:37:18,664 - INFO - Cleaning up resources...
2025-04-24 04:37:18,664 - INFO - CUDA cache cleared.
2025-04-24 04:37:18,664 - INFO - --- Evaluation Complete ---

2025-04-24 04:37:19,258 - INFO - SUCCESS: Run 3/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='attack'. Check output files in /workspace/results/mod/48/
2025-04-24 04:37:19,258 - INFO - 
Run 4/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='exploit_target'
2025-04-24 04:37:19,258 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset exploit_target --max-new-tokens 512
2025-04-24 04:37:25,050 - WARNING - Stderr:
2025-04-24 04:37:21,403 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:37:21,706 - INFO - --- Starting Evaluation ---
2025-04-24 04:37:21,706 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'exploit_target'}
2025-04-24 04:37:21,706 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:37:21,706 - INFO - Metrics file: /workspace/results/mod/48/ctibench_exploit_target__20250424-043721_metrics.json
2025-04-24 04:37:21,706 - INFO - Outputs file: /workspace/results/mod/48/ctibench_exploit_target__20250424-043721_outputs.json
2025-04-24 04:37:21,706 - INFO - NVML Reporting Active: False
2025-04-24 04:37:21,706 - INFO - Initial RAM usage: 555.27 MB
2025-04-24 04:37:21,706 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/...
2025-04-24 04:37:21,726 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:37:21,819 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:37:23,782 - INFO - CausalLM Model loaded.
2025-04-24 04:37:23,783 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:37:23,783 - INFO - Model loaded in 2.08 seconds.
2025-04-24 04:37:23,783 - INFO - Loading prompts from CTIBench subset: exploit_target
2025-04-24 04:37:24,437 - ERROR - Error loading/processing ctibench (exploit_target): BuilderConfig 'exploit_target' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'exploit_target' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:37:24,438 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:37:24,438 - INFO - Cleaning up resources...
2025-04-24 04:37:24,438 - INFO - CUDA cache cleared.
2025-04-24 04:37:24,438 - INFO - --- Evaluation Complete ---

2025-04-24 04:37:25,050 - INFO - SUCCESS: Run 4/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='exploit_target'. Check output files in /workspace/results/mod/48/
2025-04-24 04:37:25,050 - INFO - 
Run 5/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='threat_actor'
2025-04-24 04:37:25,050 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset threat_actor --max-new-tokens 512
2025-04-24 04:37:30,828 - WARNING - Stderr:
2025-04-24 04:37:27,203 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:37:27,505 - INFO - --- Starting Evaluation ---
2025-04-24 04:37:27,505 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'threat_actor'}
2025-04-24 04:37:27,505 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:37:27,505 - INFO - Metrics file: /workspace/results/mod/48/ctibench_threat_actor__20250424-043727_metrics.json
2025-04-24 04:37:27,505 - INFO - Outputs file: /workspace/results/mod/48/ctibench_threat_actor__20250424-043727_outputs.json
2025-04-24 04:37:27,505 - INFO - NVML Reporting Active: False
2025-04-24 04:37:27,505 - INFO - Initial RAM usage: 556.86 MB
2025-04-24 04:37:27,505 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/...
2025-04-24 04:37:27,525 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:37:27,619 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:37:29,527 - INFO - CausalLM Model loaded.
2025-04-24 04:37:29,528 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:37:29,528 - INFO - Model loaded in 2.02 seconds.
2025-04-24 04:37:29,528 - INFO - Loading prompts from CTIBench subset: threat_actor
2025-04-24 04:37:30,232 - ERROR - Error loading/processing ctibench (threat_actor): BuilderConfig 'threat_actor' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'threat_actor' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:37:30,233 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:37:30,234 - INFO - Cleaning up resources...
2025-04-24 04:37:30,234 - INFO - CUDA cache cleared.
2025-04-24 04:37:30,234 - INFO - --- Evaluation Complete ---

2025-04-24 04:37:30,828 - INFO - SUCCESS: Run 5/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='threat_actor'. Check output files in /workspace/results/mod/48/
2025-04-24 04:37:30,828 - INFO - 
Run 6/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='ttps'
2025-04-24 04:37:30,828 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset ttps --max-new-tokens 512
2025-04-24 04:37:36,644 - WARNING - Stderr:
2025-04-24 04:37:33,006 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:37:33,310 - INFO - --- Starting Evaluation ---
2025-04-24 04:37:33,311 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'ttps'}
2025-04-24 04:37:33,311 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:37:33,311 - INFO - Metrics file: /workspace/results/mod/48/ctibench_ttps__20250424-043733_metrics.json
2025-04-24 04:37:33,311 - INFO - Outputs file: /workspace/results/mod/48/ctibench_ttps__20250424-043733_outputs.json
2025-04-24 04:37:33,311 - INFO - NVML Reporting Active: False
2025-04-24 04:37:33,311 - INFO - Initial RAM usage: 556.69 MB
2025-04-24 04:37:33,311 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/...
2025-04-24 04:37:33,331 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:37:33,424 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:37:35,330 - INFO - CausalLM Model loaded.
2025-04-24 04:37:35,331 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:37:35,331 - INFO - Model loaded in 2.02 seconds.
2025-04-24 04:37:35,331 - INFO - Loading prompts from CTIBench subset: ttps
2025-04-24 04:37:36,016 - ERROR - Error loading/processing ctibench (ttps): BuilderConfig 'ttps' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'ttps' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:37:36,017 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:37:36,017 - INFO - Cleaning up resources...
2025-04-24 04:37:36,017 - INFO - CUDA cache cleared.
2025-04-24 04:37:36,017 - INFO - --- Evaluation Complete ---

2025-04-24 04:37:36,644 - INFO - SUCCESS: Run 6/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='ttps'. Check output files in /workspace/results/mod/48/
2025-04-24 04:37:36,644 - INFO - 
Run 7/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 04:37:36,644 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 04:37:42,525 - WARNING - Stderr:
2025-04-24 04:37:38,804 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:37:39,108 - INFO - --- Starting Evaluation ---
2025-04-24 04:37:39,108 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 04:37:39,108 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:37:39,108 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre__20250424-043739_metrics.json
2025-04-24 04:37:39,108 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre__20250424-043739_outputs.json
2025-04-24 04:37:39,108 - INFO - NVML Reporting Active: False
2025-04-24 04:37:39,109 - INFO - Initial RAM usage: 555.24 MB
2025-04-24 04:37:39,109 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/...
2025-04-24 04:37:39,129 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:37:39,223 - INFO - Tokenizer loaded.
2025-04-24 04:37:39,990 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 04:37:39,990 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:37:41,534 - INFO - CausalLM Model loaded.
2025-04-24 04:37:41,535 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:37:41,535 - INFO - Model loaded in 2.43 seconds.
2025-04-24 04:37:41,536 - INFO - Loading prompts from CyberSecEval3 MITRE file: /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json
2025-04-24 04:37:41,547 - INFO - Processing the first 100 items.

Parsing MITRE prompts:   0%|          | 0/100 [00:00<?, ?it/s]2025-04-24 04:37:41,549 - WARNING - Item 5: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,549 - WARNING - Item 8: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,550 - WARNING - Item 14: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,551 - WARNING - Item 21: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,551 - WARNING - Item 25: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,551 - WARNING - Item 27: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,551 - WARNING - Item 30: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,552 - WARNING - Item 32: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,552 - WARNING - Item 35: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,552 - WARNING - Item 36: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,552 - WARNING - Item 48: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,553 - WARNING - Item 49: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,553 - WARNING - Item 52: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,553 - WARNING - Item 58: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,553 - WARNING - Item 63: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,554 - WARNING - Item 72: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,554 - WARNING - Item 81: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,555 - WARNING - Item 86: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,555 - WARNING - Item 88: Regex failed to find/extract 'prompt' value from mutated_prompt_str.

Parsing MITRE prompts: 100%|██████████| 100/100 [00:00<00:00, 13107.61it/s]
2025-04-24 04:37:41,555 - INFO - Successfully loaded 81 prompts from cyberseceval3_mitre.
2025-04-24 04:37:41,556 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 04:37:41,556 - INFO - Starting inference on 81 samples...

Inference Progress:   0%|          | 0/81 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
2025-04-24 04:37:41,645 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,648 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,651 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,653 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,655 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,658 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:   7%|▋         | 6/81 [00:00<00:01, 58.94it/s]2025-04-24 04:37:41,660 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,662 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,665 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,667 - ERROR - Generation failed for T1024: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,669 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,672 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,674 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,676 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,677 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,680 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,682 - ERROR - Generation failed for T1090.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,684 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,686 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,688 - ERROR - Generation failed for T1090.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,691 - ERROR - Generation failed for T1090.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,692 - ERROR - Generation failed for T1483: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,694 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,696 - ERROR - Generation failed for T1090.003: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,699 - ERROR - Generation failed for T1571: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,701 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,702 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,704 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,706 - ERROR - Generation failed for T1095: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,708 - ERROR - Generation failed for T1105: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,710 - ERROR - Generation failed for T1094.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,712 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,714 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,716 - ERROR - Generation failed for T1102: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,718 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,721 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,723 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,725 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,727 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,729 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,731 - ERROR - Generation failed for T1074: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,733 - ERROR - Generation failed for T1097: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,736 - ERROR - Generation failed for T1483: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,738 - ERROR - Generation failed for T1021.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,740 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,742 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,744 - ERROR - Generation failed for T1090.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,746 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,747 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,750 - ERROR - Generation failed for T1090.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,751 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,753 - ERROR - Generation failed for T1071: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,755 - ERROR - Generation failed for T1074: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,757 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,759 - ERROR - Generation failed for T1090.003: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  68%|██████▊   | 55/81 [00:00<00:00, 307.97it/s]2025-04-24 04:37:41,761 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,763 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,765 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,767 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,769 - ERROR - Generation failed for T1090.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,771 - ERROR - Generation failed for T1090.003: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,773 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,774 - ERROR - Generation failed for T1094: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,776 - ERROR - Generation failed for T1132.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,779 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,780 - ERROR - Generation failed for T1059: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,783 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,785 - ERROR - Generation failed for T1071: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,787 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,789 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,791 - ERROR - Generation failed for T1094: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,794 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,795 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,797 - ERROR - Generation failed for T1071: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,799 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,801 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,804 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,806 - ERROR - Generation failed for T1095: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,808 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,810 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,811 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress: 100%|██████████| 81/81 [00:00<00:00, 316.76it/s]
2025-04-24 04:37:41,812 - INFO - 
--- Inference Summary ---
2025-04-24 04:37:41,812 - INFO - Processed 81 samples.
2025-04-24 04:37:41,812 - INFO - Total 'generate' time (sum): 0.00 sec
2025-04-24 04:37:41,812 - INFO - Overall inference loop duration: 0.26 sec
2025-04-24 04:37:41,812 - INFO - Average 'generate' time per sample: 0.0000 sec
2025-04-24 04:37:41,812 - INFO - Total effective tokens generated: 0
2025-04-24 04:37:41,812 - INFO - Overall effective tokens per second: 0.00
2025-04-24 04:37:41,812 - INFO - RAM Delta during inference: 28.53 MB
2025-04-24 04:37:41,812 - INFO - PyTorch VRAM Peak Delta during inference: 0.02 MB
2025-04-24 04:37:41,813 - INFO - Saved outputs (81 items) to: /workspace/results/mod/48/cyberseceval3_mitre__20250424-043739_outputs.json
2025-04-24 04:37:41,813 - INFO - Saved metrics to: /workspace/results/mod/48/cyberseceval3_mitre__20250424-043739_metrics.json
2025-04-24 04:37:41,813 - INFO - Cleaning up resources...
2025-04-24 04:37:41,813 - INFO - CUDA cache cleared.
2025-04-24 04:37:41,813 - INFO - --- Evaluation Complete ---

2025-04-24 04:37:42,527 - INFO - SUCCESS: Run 7/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 04:37:42,527 - INFO - 
Run 8/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 04:37:42,527 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 04:37:48,444 - WARNING - Stderr:
2025-04-24 04:37:44,701 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:37:45,006 - INFO - --- Starting Evaluation ---
2025-04-24 04:37:45,006 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 04:37:45,006 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:37:45,006 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench__20250424-043745_metrics.json
2025-04-24 04:37:45,006 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench__20250424-043745_outputs.json
2025-04-24 04:37:45,006 - INFO - NVML Reporting Active: False
2025-04-24 04:37:45,006 - INFO - Initial RAM usage: 557.00 MB
2025-04-24 04:37:45,006 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/...
2025-04-24 04:37:45,026 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:37:45,119 - INFO - Tokenizer loaded.
2025-04-24 04:37:45,887 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 04:37:45,887 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:37:47,453 - INFO - CausalLM Model loaded.
2025-04-24 04:37:47,454 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:37:47,454 - INFO - Model loaded in 2.45 seconds.
2025-04-24 04:37:47,455 - INFO - Loading prompts from SEvenLLM saved dataset directory: /workspace/calibration_data/sevenllm_instruct_subset_manual/
2025-04-24 04:37:47,458 - INFO - Loaded dataset: Dataset({
    features: ['id', 'output', 'instruction', 'input', 'category', 'thought'],
    num_rows: 200
})
2025-04-24 04:37:47,459 - INFO - Selected first 100 samples for evaluation.

Parsing SEvenLLM prompts:   0%|          | 0/100 [00:00<?, ?it/s]
Parsing SEvenLLM prompts: 100%|██████████| 100/100 [00:00<00:00, 25466.33it/s]
2025-04-24 04:37:47,464 - INFO - Successfully loaded 100 prompts from sevenllm_bench.
2025-04-24 04:37:47,464 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 04:37:47,464 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
2025-04-24 04:37:47,569 - ERROR - Generation failed for 1: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:   1%|          | 1/100 [00:00<00:10,  9.44it/s]2025-04-24 04:37:47,572 - ERROR - Generation failed for 2: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,574 - ERROR - Generation failed for 3: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,577 - ERROR - Generation failed for 4: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,578 - ERROR - Generation failed for 5: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,581 - ERROR - Generation failed for 6: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,583 - ERROR - Generation failed for 7: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,585 - ERROR - Generation failed for 8: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,587 - ERROR - Generation failed for 9: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,589 - ERROR - Generation failed for 10: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,591 - ERROR - Generation failed for 11: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,593 - ERROR - Generation failed for 12: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,595 - ERROR - Generation failed for 13: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,597 - ERROR - Generation failed for 14: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,599 - ERROR - Generation failed for 15: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,601 - ERROR - Generation failed for 16: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,603 - ERROR - Generation failed for 17: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,605 - ERROR - Generation failed for 18: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,607 - ERROR - Generation failed for 19: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,609 - ERROR - Generation failed for 20: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,611 - ERROR - Generation failed for 21: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,613 - ERROR - Generation failed for 22: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,616 - ERROR - Generation failed for 23: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,619 - ERROR - Generation failed for 24: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,622 - ERROR - Generation failed for 25: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,624 - ERROR - Generation failed for 26: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,626 - ERROR - Generation failed for 27: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,627 - ERROR - Generation failed for 28: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,629 - ERROR - Generation failed for 29: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,631 - ERROR - Generation failed for 30: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,633 - ERROR - Generation failed for 31: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,635 - ERROR - Generation failed for 32: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,637 - ERROR - Generation failed for 33: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,639 - ERROR - Generation failed for 34: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,641 - ERROR - Generation failed for 35: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,643 - ERROR - Generation failed for 36: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,645 - ERROR - Generation failed for 37: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,646 - ERROR - Generation failed for 38: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,648 - ERROR - Generation failed for 39: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,650 - ERROR - Generation failed for 40: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,652 - ERROR - Generation failed for 41: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,654 - ERROR - Generation failed for 42: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,656 - ERROR - Generation failed for 43: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,658 - ERROR - Generation failed for 44: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,660 - ERROR - Generation failed for 45: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,662 - ERROR - Generation failed for 46: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,665 - ERROR - Generation failed for 47: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,667 - ERROR - Generation failed for 48: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,669 - ERROR - Generation failed for 49: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,671 - ERROR - Generation failed for 50: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  50%|█████     | 50/100 [00:00<00:00, 283.48it/s]2025-04-24 04:37:47,673 - ERROR - Generation failed for 51: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,675 - ERROR - Generation failed for 52: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,677 - ERROR - Generation failed for 53: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,679 - ERROR - Generation failed for 54: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,682 - ERROR - Generation failed for 55: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,685 - ERROR - Generation failed for 56: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,687 - ERROR - Generation failed for 57: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,689 - ERROR - Generation failed for 58: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,691 - ERROR - Generation failed for 59: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,693 - ERROR - Generation failed for 60: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,695 - ERROR - Generation failed for 61: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,697 - ERROR - Generation failed for 62: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,700 - ERROR - Generation failed for 63: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,702 - ERROR - Generation failed for 64: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,705 - ERROR - Generation failed for 65: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,707 - ERROR - Generation failed for 66: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,709 - ERROR - Generation failed for 67: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,711 - ERROR - Generation failed for 68: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,713 - ERROR - Generation failed for 69: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,716 - ERROR - Generation failed for 70: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,718 - ERROR - Generation failed for 71: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,720 - ERROR - Generation failed for 72: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,722 - ERROR - Generation failed for 73: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,724 - ERROR - Generation failed for 74: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,726 - ERROR - Generation failed for 75: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,729 - ERROR - Generation failed for 76: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,732 - ERROR - Generation failed for 77: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,734 - ERROR - Generation failed for 78: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,736 - ERROR - Generation failed for 79: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,737 - ERROR - Generation failed for 80: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,740 - ERROR - Generation failed for 81: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,743 - ERROR - Generation failed for 82: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,745 - ERROR - Generation failed for 83: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,748 - ERROR - Generation failed for 84: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,750 - ERROR - Generation failed for 85: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,752 - ERROR - Generation failed for 86: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,755 - ERROR - Generation failed for 87: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,756 - ERROR - Generation failed for 88: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,758 - ERROR - Generation failed for 89: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,760 - ERROR - Generation failed for 90: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,762 - ERROR - Generation failed for 91: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,764 - ERROR - Generation failed for 92: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,766 - ERROR - Generation failed for 93: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,768 - ERROR - Generation failed for 94: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,770 - ERROR - Generation failed for 95: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,772 - ERROR - Generation failed for 96: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  96%|█████████▌| 96/100 [00:00<00:00, 361.94it/s]2025-04-24 04:37:47,775 - ERROR - Generation failed for 97: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,777 - ERROR - Generation failed for 98: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,780 - ERROR - Generation failed for 99: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,784 - ERROR - Generation failed for 100: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress: 100%|██████████| 100/100 [00:00<00:00, 312.77it/s]
2025-04-24 04:37:47,785 - INFO - 
--- Inference Summary ---
2025-04-24 04:37:47,785 - INFO - Processed 100 samples.
2025-04-24 04:37:47,785 - INFO - Total 'generate' time (sum): 0.00 sec
2025-04-24 04:37:47,785 - INFO - Overall inference loop duration: 0.32 sec
2025-04-24 04:37:47,785 - INFO - Average 'generate' time per sample: 0.0000 sec
2025-04-24 04:37:47,785 - INFO - Total effective tokens generated: 0
2025-04-24 04:37:47,785 - INFO - Overall effective tokens per second: 0.00
2025-04-24 04:37:47,785 - INFO - RAM Delta during inference: 52.50 MB
2025-04-24 04:37:47,785 - INFO - PyTorch VRAM Peak Delta during inference: 0.03 MB
2025-04-24 04:37:47,786 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/sevenllm_bench__20250424-043745_outputs.json
2025-04-24 04:37:47,786 - INFO - Saved metrics to: /workspace/results/mod/48/sevenllm_bench__20250424-043745_metrics.json
2025-04-24 04:37:47,786 - INFO - Cleaning up resources...
2025-04-24 04:37:47,786 - INFO - CUDA cache cleared.
2025-04-24 04:37:47,786 - INFO - --- Evaluation Complete ---

2025-04-24 04:37:48,447 - INFO - SUCCESS: Run 8/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 04:37:48,447 - INFO - 
Run 9/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='attack'
2025-04-24 04:37:48,447 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset attack --max-new-tokens 512
2025-04-24 04:37:54,673 - WARNING - Stderr:
2025-04-24 04:37:50,603 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:37:50,908 - INFO - --- Starting Evaluation ---
2025-04-24 04:37:50,908 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'attack'}
2025-04-24 04:37:50,908 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:37:50,908 - INFO - Metrics file: /workspace/results/mod/48/ctibench_attack__20250424-043750_metrics.json
2025-04-24 04:37:50,908 - INFO - Outputs file: /workspace/results/mod/48/ctibench_attack__20250424-043750_outputs.json
2025-04-24 04:37:50,908 - INFO - NVML Reporting Active: False
2025-04-24 04:37:50,908 - INFO - Initial RAM usage: 555.34 MB
2025-04-24 04:37:50,908 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/...
2025-04-24 04:37:50,928 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:37:51,021 - INFO - Tokenizer loaded.
2025-04-24 04:37:51,880 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 04:37:51,880 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:37:53,437 - INFO - CausalLM Model loaded.
2025-04-24 04:37:53,438 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:37:53,438 - INFO - Model loaded in 2.53 seconds.
2025-04-24 04:37:53,438 - INFO - Loading prompts from CTIBench subset: attack
2025-04-24 04:37:54,020 - ERROR - Error loading/processing ctibench (attack): BuilderConfig 'attack' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'attack' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:37:54,021 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:37:54,022 - INFO - Cleaning up resources...
2025-04-24 04:37:54,022 - INFO - CUDA cache cleared.
2025-04-24 04:37:54,022 - INFO - --- Evaluation Complete ---

2025-04-24 04:37:54,673 - INFO - SUCCESS: Run 9/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='attack'. Check output files in /workspace/results/mod/48/
2025-04-24 04:37:54,673 - INFO - 
Run 10/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='exploit_target'
2025-04-24 04:37:54,673 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset exploit_target --max-new-tokens 512
2025-04-24 04:38:00,999 - WARNING - Stderr:
2025-04-24 04:37:56,808 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:37:57,116 - INFO - --- Starting Evaluation ---
2025-04-24 04:37:57,116 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'exploit_target'}
2025-04-24 04:37:57,116 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:37:57,116 - INFO - Metrics file: /workspace/results/mod/48/ctibench_exploit_target__20250424-043757_metrics.json
2025-04-24 04:37:57,116 - INFO - Outputs file: /workspace/results/mod/48/ctibench_exploit_target__20250424-043757_outputs.json
2025-04-24 04:37:57,116 - INFO - NVML Reporting Active: False
2025-04-24 04:37:57,116 - INFO - Initial RAM usage: 556.60 MB
2025-04-24 04:37:57,116 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/...
2025-04-24 04:37:57,137 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:37:57,231 - INFO - Tokenizer loaded.
2025-04-24 04:37:58,013 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 04:37:58,013 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:37:59,695 - INFO - CausalLM Model loaded.
2025-04-24 04:37:59,695 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:37:59,696 - INFO - Model loaded in 2.58 seconds.
2025-04-24 04:37:59,696 - INFO - Loading prompts from CTIBench subset: exploit_target
2025-04-24 04:38:00,316 - ERROR - Error loading/processing ctibench (exploit_target): BuilderConfig 'exploit_target' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'exploit_target' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:38:00,317 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:38:00,317 - INFO - Cleaning up resources...
2025-04-24 04:38:00,317 - INFO - CUDA cache cleared.
2025-04-24 04:38:00,317 - INFO - --- Evaluation Complete ---

2025-04-24 04:38:00,999 - INFO - SUCCESS: Run 10/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='exploit_target'. Check output files in /workspace/results/mod/48/
2025-04-24 04:38:00,999 - INFO - 
Run 11/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='threat_actor'
2025-04-24 04:38:01,000 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset threat_actor --max-new-tokens 512
2025-04-24 04:38:07,257 - WARNING - Stderr:
2025-04-24 04:38:03,191 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:38:03,495 - INFO - --- Starting Evaluation ---
2025-04-24 04:38:03,495 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'threat_actor'}
2025-04-24 04:38:03,495 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:38:03,495 - INFO - Metrics file: /workspace/results/mod/48/ctibench_threat_actor__20250424-043803_metrics.json
2025-04-24 04:38:03,495 - INFO - Outputs file: /workspace/results/mod/48/ctibench_threat_actor__20250424-043803_outputs.json
2025-04-24 04:38:03,495 - INFO - NVML Reporting Active: False
2025-04-24 04:38:03,495 - INFO - Initial RAM usage: 556.89 MB
2025-04-24 04:38:03,495 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/...
2025-04-24 04:38:03,515 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:38:03,610 - INFO - Tokenizer loaded.
2025-04-24 04:38:04,395 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 04:38:04,395 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:38:05,979 - INFO - CausalLM Model loaded.
2025-04-24 04:38:05,979 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:38:05,980 - INFO - Model loaded in 2.48 seconds.
2025-04-24 04:38:05,980 - INFO - Loading prompts from CTIBench subset: threat_actor
2025-04-24 04:38:06,606 - ERROR - Error loading/processing ctibench (threat_actor): BuilderConfig 'threat_actor' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'threat_actor' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:38:06,607 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:38:06,607 - INFO - Cleaning up resources...
2025-04-24 04:38:06,607 - INFO - CUDA cache cleared.
2025-04-24 04:38:06,607 - INFO - --- Evaluation Complete ---

2025-04-24 04:38:07,257 - INFO - SUCCESS: Run 11/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='threat_actor'. Check output files in /workspace/results/mod/48/
2025-04-24 04:38:07,257 - INFO - 
Run 12/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='ttps'
2025-04-24 04:38:07,257 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset ttps --max-new-tokens 512
2025-04-24 04:38:13,338 - WARNING - Stderr:
2025-04-24 04:38:09,404 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:38:09,712 - INFO - --- Starting Evaluation ---
2025-04-24 04:38:09,712 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'ttps'}
2025-04-24 04:38:09,712 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:38:09,712 - INFO - Metrics file: /workspace/results/mod/48/ctibench_ttps__20250424-043809_metrics.json
2025-04-24 04:38:09,712 - INFO - Outputs file: /workspace/results/mod/48/ctibench_ttps__20250424-043809_outputs.json
2025-04-24 04:38:09,712 - INFO - NVML Reporting Active: False
2025-04-24 04:38:09,712 - INFO - Initial RAM usage: 556.59 MB
2025-04-24 04:38:09,712 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/...
2025-04-24 04:38:09,732 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:38:09,826 - INFO - Tokenizer loaded.
2025-04-24 04:38:10,597 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 04:38:10,597 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:38:12,133 - INFO - CausalLM Model loaded.
2025-04-24 04:38:12,134 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:38:12,134 - INFO - Model loaded in 2.42 seconds.
2025-04-24 04:38:12,134 - INFO - Loading prompts from CTIBench subset: ttps
2025-04-24 04:38:12,688 - ERROR - Error loading/processing ctibench (ttps): BuilderConfig 'ttps' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'ttps' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:38:12,689 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:38:12,689 - INFO - Cleaning up resources...
2025-04-24 04:38:12,689 - INFO - CUDA cache cleared.
2025-04-24 04:38:12,689 - INFO - --- Evaluation Complete ---

2025-04-24 04:38:13,339 - INFO - SUCCESS: Run 12/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='ttps'. Check output files in /workspace/results/mod/48/
2025-04-24 04:38:13,339 - INFO - 
Run 13/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 04:38:13,339 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 04:48:27,847 - WARNING - Stderr:
2025-04-24 04:38:15,498 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:38:15,798 - INFO - --- Starting Evaluation ---
2025-04-24 04:38:15,798 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 04:38:15,798 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:38:15,798 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre__20250424-043815_metrics.json
2025-04-24 04:38:15,799 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre__20250424-043815_outputs.json
2025-04-24 04:38:15,799 - INFO - NVML Reporting Active: False
2025-04-24 04:38:15,799 - INFO - Initial RAM usage: 556.84 MB
2025-04-24 04:38:15,799 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/...
2025-04-24 04:38:15,822 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:38:16,118 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:38:19,595 - INFO - CausalLM Model loaded.
2025-04-24 04:38:19,596 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:38:19,596 - INFO - Model loaded in 3.80 seconds.
2025-04-24 04:38:19,596 - INFO - Loading prompts from CyberSecEval3 MITRE file: /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json
2025-04-24 04:38:19,608 - INFO - Processing the first 100 items.

Parsing MITRE prompts:   0%|          | 0/100 [00:00<?, ?it/s]2025-04-24 04:38:19,610 - WARNING - Item 5: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,611 - WARNING - Item 8: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,611 - WARNING - Item 14: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,612 - WARNING - Item 21: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,613 - WARNING - Item 25: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,613 - WARNING - Item 27: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,613 - WARNING - Item 30: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,613 - WARNING - Item 32: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,614 - WARNING - Item 35: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,614 - WARNING - Item 36: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,615 - WARNING - Item 48: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,615 - WARNING - Item 49: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,615 - WARNING - Item 52: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,616 - WARNING - Item 58: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,616 - WARNING - Item 63: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,617 - WARNING - Item 72: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,617 - WARNING - Item 81: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,618 - WARNING - Item 86: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,618 - WARNING - Item 88: Regex failed to find/extract 'prompt' value from mutated_prompt_str.

Parsing MITRE prompts: 100%|██████████| 100/100 [00:00<00:00, 10602.92it/s]
2025-04-24 04:38:19,619 - INFO - Successfully loaded 81 prompts from cyberseceval3_mitre.
2025-04-24 04:38:19,619 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 04:38:19,620 - INFO - Starting inference on 81 samples...

Inference Progress:   0%|          | 0/81 [00:00<?, ?it/s]
Inference Progress:   1%|          | 1/81 [00:08<10:41,  8.02s/it]
Inference Progress:   2%|▏         | 2/81 [00:15<10:08,  7.70s/it]
Inference Progress:   4%|▎         | 3/81 [00:23<09:58,  7.67s/it]
Inference Progress:   5%|▍         | 4/81 [00:30<09:46,  7.62s/it]
Inference Progress:   6%|▌         | 5/81 [00:38<09:37,  7.60s/it]
Inference Progress:   7%|▋         | 6/81 [00:45<09:27,  7.57s/it]
Inference Progress:   9%|▊         | 7/81 [00:53<09:19,  7.56s/it]
Inference Progress:  10%|▉         | 8/81 [01:00<09:13,  7.58s/it]
Inference Progress:  11%|█         | 9/81 [01:08<09:06,  7.58s/it]
Inference Progress:  12%|█▏        | 10/81 [01:16<08:58,  7.58s/it]
Inference Progress:  14%|█▎        | 11/81 [01:23<08:50,  7.57s/it]
Inference Progress:  15%|█▍        | 12/81 [01:31<08:42,  7.58s/it]
Inference Progress:  16%|█▌        | 13/81 [01:38<08:33,  7.55s/it]
Inference Progress:  17%|█▋        | 14/81 [01:46<08:24,  7.53s/it]
Inference Progress:  19%|█▊        | 15/81 [01:53<08:14,  7.50s/it]
Inference Progress:  20%|█▉        | 16/81 [02:01<08:09,  7.53s/it]
Inference Progress:  21%|██        | 17/81 [02:08<08:02,  7.53s/it]
Inference Progress:  22%|██▏       | 18/81 [02:16<07:51,  7.48s/it]
Inference Progress:  23%|██▎       | 19/81 [02:23<07:46,  7.52s/it]
Inference Progress:  25%|██▍       | 20/81 [02:31<07:39,  7.54s/it]
Inference Progress:  26%|██▌       | 21/81 [02:38<07:33,  7.55s/it]
Inference Progress:  27%|██▋       | 22/81 [02:46<07:22,  7.50s/it]
Inference Progress:  28%|██▊       | 23/81 [02:53<07:16,  7.52s/it]
Inference Progress:  30%|██▉       | 24/81 [03:01<07:07,  7.50s/it]
Inference Progress:  31%|███       | 25/81 [03:08<07:00,  7.51s/it]
Inference Progress:  32%|███▏      | 26/81 [03:16<06:53,  7.52s/it]
Inference Progress:  33%|███▎      | 27/81 [03:23<06:43,  7.47s/it]
Inference Progress:  35%|███▍      | 28/81 [03:31<06:35,  7.46s/it]
Inference Progress:  36%|███▌      | 29/81 [03:38<06:27,  7.45s/it]
Inference Progress:  37%|███▋      | 30/81 [03:46<06:19,  7.44s/it]
Inference Progress:  38%|███▊      | 31/81 [03:53<06:14,  7.48s/it]
Inference Progress:  40%|███▉      | 32/81 [04:00<06:04,  7.44s/it]
Inference Progress:  41%|████      | 33/81 [04:08<05:58,  7.47s/it]
Inference Progress:  42%|████▏     | 34/81 [04:16<05:52,  7.51s/it]
Inference Progress:  43%|████▎     | 35/81 [04:23<05:46,  7.52s/it]
Inference Progress:  44%|████▍     | 36/81 [04:31<05:39,  7.55s/it]
Inference Progress:  46%|████▌     | 37/81 [04:38<05:31,  7.54s/it]
Inference Progress:  47%|████▋     | 38/81 [04:46<05:23,  7.53s/it]
Inference Progress:  48%|████▊     | 39/81 [04:53<05:16,  7.53s/it]
Inference Progress:  49%|████▉     | 40/81 [05:01<05:10,  7.57s/it]
Inference Progress:  51%|█████     | 41/81 [05:08<05:01,  7.54s/it]
Inference Progress:  52%|█████▏    | 42/81 [05:16<04:54,  7.56s/it]
Inference Progress:  53%|█████▎    | 43/81 [05:24<04:46,  7.54s/it]
Inference Progress:  54%|█████▍    | 44/81 [05:31<04:39,  7.54s/it]
Inference Progress:  56%|█████▌    | 45/81 [05:39<04:33,  7.58s/it]
Inference Progress:  57%|█████▋    | 46/81 [05:46<04:24,  7.55s/it]
Inference Progress:  58%|█████▊    | 47/81 [05:54<04:15,  7.51s/it]
Inference Progress:  59%|█████▉    | 48/81 [06:01<04:08,  7.52s/it]
Inference Progress:  60%|██████    | 49/81 [06:08<03:58,  7.46s/it]
Inference Progress:  62%|██████▏   | 50/81 [06:16<03:52,  7.50s/it]
Inference Progress:  63%|██████▎   | 51/81 [06:24<03:44,  7.49s/it]
Inference Progress:  64%|██████▍   | 52/81 [06:31<03:37,  7.50s/it]
Inference Progress:  65%|██████▌   | 53/81 [06:39<03:30,  7.51s/it]
Inference Progress:  67%|██████▋   | 54/81 [06:46<03:21,  7.46s/it]
Inference Progress:  68%|██████▊   | 55/81 [06:53<03:14,  7.46s/it]
Inference Progress:  69%|██████▉   | 56/81 [07:01<03:06,  7.45s/it]
Inference Progress:  70%|███████   | 57/81 [07:08<02:59,  7.47s/it]
Inference Progress:  72%|███████▏  | 58/81 [07:14<02:39,  6.95s/it]
Inference Progress:  73%|███████▎  | 59/81 [07:22<02:36,  7.11s/it]
Inference Progress:  74%|███████▍  | 60/81 [07:29<02:31,  7.21s/it]
Inference Progress:  75%|███████▌  | 61/81 [07:37<02:26,  7.32s/it]
Inference Progress:  77%|███████▋  | 62/81 [07:44<02:19,  7.34s/it]
Inference Progress:  78%|███████▊  | 63/81 [07:51<02:12,  7.34s/it]
Inference Progress:  79%|███████▉  | 64/81 [07:59<02:05,  7.41s/it]
Inference Progress:  80%|████████  | 65/81 [08:06<01:59,  7.47s/it]
Inference Progress:  81%|████████▏ | 66/81 [08:14<01:51,  7.46s/it]
Inference Progress:  83%|████████▎ | 67/81 [08:21<01:44,  7.48s/it]
Inference Progress:  84%|████████▍ | 68/81 [08:29<01:37,  7.52s/it]
Inference Progress:  85%|████████▌ | 69/81 [08:37<01:30,  7.51s/it]
Inference Progress:  86%|████████▋ | 70/81 [08:44<01:22,  7.51s/it]
Inference Progress:  88%|████████▊ | 71/81 [08:52<01:15,  7.54s/it]
Inference Progress:  89%|████████▉ | 72/81 [08:59<01:07,  7.55s/it]
Inference Progress:  90%|█████████ | 73/81 [09:07<00:59,  7.49s/it]
Inference Progress:  91%|█████████▏| 74/81 [09:14<00:52,  7.46s/it]
Inference Progress:  93%|█████████▎| 75/81 [09:22<00:45,  7.51s/it]
Inference Progress:  94%|█████████▍| 76/81 [09:29<00:37,  7.51s/it]
Inference Progress:  95%|█████████▌| 77/81 [09:37<00:30,  7.54s/it]
Inference Progress:  96%|█████████▋| 78/81 [09:44<00:22,  7.55s/it]
Inference Progress:  98%|█████████▊| 79/81 [09:52<00:15,  7.58s/it]
Inference Progress:  99%|█████████▉| 80/81 [09:59<00:07,  7.54s/it]
Inference Progress: 100%|██████████| 81/81 [10:07<00:00,  7.50s/it]
Inference Progress: 100%|██████████| 81/81 [10:07<00:00,  7.50s/it]
2025-04-24 04:48:26,969 - INFO - 
--- Inference Summary ---
2025-04-24 04:48:26,969 - INFO - Processed 81 samples.
2025-04-24 04:48:26,969 - INFO - Total 'generate' time (sum): 607.19 sec
2025-04-24 04:48:26,969 - INFO - Overall inference loop duration: 607.35 sec
2025-04-24 04:48:26,969 - INFO - Average 'generate' time per sample: 7.4962 sec
2025-04-24 04:48:26,969 - INFO - Total effective tokens generated: 41349
2025-04-24 04:48:26,969 - INFO - Overall effective tokens per second: 68.10
2025-04-24 04:48:26,969 - INFO - RAM Delta during inference: 679.57 MB
2025-04-24 04:48:26,969 - INFO - PyTorch VRAM Peak Delta during inference: 176.48 MB
2025-04-24 04:48:26,971 - INFO - Saved outputs (81 items) to: /workspace/results/mod/48/cyberseceval3_mitre__20250424-043815_outputs.json
2025-04-24 04:48:26,971 - INFO - Saved metrics to: /workspace/results/mod/48/cyberseceval3_mitre__20250424-043815_metrics.json
2025-04-24 04:48:26,971 - INFO - Cleaning up resources...
2025-04-24 04:48:26,977 - INFO - CUDA cache cleared.
2025-04-24 04:48:26,977 - INFO - --- Evaluation Complete ---

2025-04-24 04:48:27,847 - INFO - SUCCESS: Run 13/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 04:48:27,847 - INFO - 
Run 14/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 04:48:27,847 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 04:54:50,695 - WARNING - Stderr:
2025-04-24 04:48:30,004 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:48:30,336 - INFO - --- Starting Evaluation ---
2025-04-24 04:48:30,336 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 04:48:30,336 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:48:30,336 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench__20250424-044830_metrics.json
2025-04-24 04:48:30,336 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench__20250424-044830_outputs.json
2025-04-24 04:48:30,336 - INFO - NVML Reporting Active: False
2025-04-24 04:48:30,336 - INFO - Initial RAM usage: 555.07 MB
2025-04-24 04:48:30,336 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/...
2025-04-24 04:48:30,357 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:48:30,654 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:48:34,220 - INFO - CausalLM Model loaded.
2025-04-24 04:48:34,221 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:48:34,221 - INFO - Model loaded in 3.88 seconds.
2025-04-24 04:48:34,221 - INFO - Loading prompts from SEvenLLM saved dataset directory: /workspace/calibration_data/sevenllm_instruct_subset_manual/
2025-04-24 04:48:34,225 - INFO - Loaded dataset: Dataset({
    features: ['id', 'output', 'instruction', 'input', 'category', 'thought'],
    num_rows: 200
})
2025-04-24 04:48:34,226 - INFO - Selected first 100 samples for evaluation.

Parsing SEvenLLM prompts:   0%|          | 0/100 [00:00<?, ?it/s]
Parsing SEvenLLM prompts: 100%|██████████| 100/100 [00:00<00:00, 25990.23it/s]
2025-04-24 04:48:34,231 - INFO - Successfully loaded 100 prompts from sevenllm_bench.
2025-04-24 04:48:34,231 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 04:48:34,231 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]
Inference Progress:   1%|          | 1/100 [00:04<08:02,  4.88s/it]
Inference Progress:   2%|▏         | 2/100 [00:08<06:50,  4.19s/it]
Inference Progress:   3%|▎         | 3/100 [00:13<07:07,  4.41s/it]
Inference Progress:   4%|▍         | 4/100 [00:16<06:28,  4.05s/it]
Inference Progress:   5%|▌         | 5/100 [00:19<05:47,  3.66s/it]
Inference Progress:   6%|▌         | 6/100 [00:23<05:48,  3.71s/it]
Inference Progress:   7%|▋         | 7/100 [00:29<06:41,  4.32s/it]
Inference Progress:   8%|▊         | 8/100 [00:32<06:03,  3.95s/it]
Inference Progress:   9%|▉         | 9/100 [00:36<05:59,  3.95s/it]
Inference Progress:  10%|█         | 10/100 [00:39<05:26,  3.63s/it]
Inference Progress:  11%|█         | 11/100 [00:46<06:57,  4.69s/it]
Inference Progress:  12%|█▏        | 12/100 [00:50<06:52,  4.69s/it]
Inference Progress:  13%|█▎        | 13/100 [00:54<06:31,  4.50s/it]
Inference Progress:  14%|█▍        | 14/100 [00:57<05:40,  3.96s/it]
Inference Progress:  15%|█▌        | 15/100 [00:59<04:32,  3.21s/it]
Inference Progress:  16%|█▌        | 16/100 [01:01<03:55,  2.81s/it]
Inference Progress:  17%|█▋        | 17/100 [01:02<03:27,  2.50s/it]
Inference Progress:  18%|█▊        | 18/100 [01:06<03:49,  2.80s/it]
Inference Progress:  19%|█▉        | 19/100 [01:12<05:17,  3.92s/it]
Inference Progress:  20%|██        | 20/100 [01:15<04:51,  3.64s/it]
Inference Progress:  21%|██        | 21/100 [01:20<05:19,  4.05s/it]
Inference Progress:  22%|██▏       | 22/100 [01:21<04:07,  3.17s/it]
Inference Progress:  23%|██▎       | 23/100 [01:25<04:23,  3.42s/it]
Inference Progress:  24%|██▍       | 24/100 [01:30<04:39,  3.68s/it]
Inference Progress:  25%|██▌       | 25/100 [01:37<05:52,  4.70s/it]
Inference Progress:  26%|██▌       | 26/100 [01:41<05:34,  4.52s/it]
Inference Progress:  27%|██▋       | 27/100 [01:43<04:45,  3.90s/it]
Inference Progress:  28%|██▊       | 28/100 [01:46<04:22,  3.64s/it]
Inference Progress:  29%|██▉       | 29/100 [01:49<04:05,  3.46s/it]
Inference Progress:  30%|███       | 30/100 [01:53<04:04,  3.49s/it]
Inference Progress:  31%|███       | 31/100 [01:54<03:03,  2.66s/it]
Inference Progress:  32%|███▏      | 32/100 [01:55<02:33,  2.26s/it]
Inference Progress:  33%|███▎      | 33/100 [01:59<03:10,  2.84s/it]
Inference Progress:  34%|███▍      | 34/100 [02:02<03:08,  2.85s/it]
Inference Progress:  35%|███▌      | 35/100 [02:05<03:14,  2.99s/it]
Inference Progress:  36%|███▌      | 36/100 [02:10<03:38,  3.42s/it]
Inference Progress:  37%|███▋      | 37/100 [02:13<03:21,  3.20s/it]
Inference Progress:  38%|███▊      | 38/100 [02:17<03:33,  3.44s/it]
Inference Progress:  39%|███▉      | 39/100 [02:19<03:06,  3.06s/it]
Inference Progress:  40%|████      | 40/100 [02:22<03:06,  3.11s/it]
Inference Progress:  41%|████      | 41/100 [02:28<03:56,  4.01s/it]
Inference Progress:  42%|████▏     | 42/100 [02:30<03:18,  3.42s/it]
Inference Progress:  43%|████▎     | 43/100 [02:32<02:46,  2.92s/it]
Inference Progress:  44%|████▍     | 44/100 [02:35<02:52,  3.09s/it]
Inference Progress:  45%|████▌     | 45/100 [02:42<03:47,  4.14s/it]
Inference Progress:  46%|████▌     | 46/100 [02:48<04:16,  4.75s/it]
Inference Progress:  47%|████▋     | 47/100 [02:49<03:11,  3.62s/it]
Inference Progress:  48%|████▊     | 48/100 [02:50<02:20,  2.71s/it]
Inference Progress:  49%|████▉     | 49/100 [02:56<03:10,  3.73s/it]
Inference Progress:  50%|█████     | 50/100 [02:58<02:40,  3.20s/it]
Inference Progress:  51%|█████     | 51/100 [02:59<02:09,  2.65s/it]
Inference Progress:  52%|█████▏    | 52/100 [03:01<01:55,  2.40s/it]
Inference Progress:  53%|█████▎    | 53/100 [03:05<02:10,  2.79s/it]
Inference Progress:  54%|█████▍    | 54/100 [03:10<02:49,  3.69s/it]
Inference Progress:  55%|█████▌    | 55/100 [03:13<02:36,  3.47s/it]
Inference Progress:  56%|█████▌    | 56/100 [03:19<03:02,  4.14s/it]
Inference Progress:  57%|█████▋    | 57/100 [03:21<02:34,  3.59s/it]
Inference Progress:  58%|█████▊    | 58/100 [03:26<02:39,  3.79s/it]
Inference Progress:  59%|█████▉    | 59/100 [03:29<02:33,  3.75s/it]
Inference Progress:  60%|██████    | 60/100 [03:30<01:53,  2.83s/it]
Inference Progress:  61%|██████    | 61/100 [03:38<02:47,  4.29s/it]
Inference Progress:  62%|██████▏   | 62/100 [03:45<03:22,  5.32s/it]
Inference Progress:  63%|██████▎   | 63/100 [03:48<02:44,  4.44s/it]
Inference Progress:  64%|██████▍   | 64/100 [03:56<03:15,  5.42s/it]
Inference Progress:  65%|██████▌   | 65/100 [04:01<03:08,  5.39s/it]
Inference Progress:  66%|██████▌   | 66/100 [04:07<03:06,  5.48s/it]
Inference Progress:  67%|██████▋   | 67/100 [04:10<02:39,  4.83s/it]
Inference Progress:  68%|██████▊   | 68/100 [04:13<02:14,  4.21s/it]
Inference Progress:  69%|██████▉   | 69/100 [04:19<02:30,  4.86s/it]
Inference Progress:  70%|███████   | 70/100 [04:22<02:09,  4.32s/it]
Inference Progress:  71%|███████   | 71/100 [04:26<02:05,  4.33s/it]
Inference Progress:  72%|███████▏  | 72/100 [04:29<01:49,  3.90s/it]
Inference Progress:  73%|███████▎  | 73/100 [04:37<02:13,  4.96s/it]
Inference Progress:  74%|███████▍  | 74/100 [04:40<01:54,  4.42s/it]
Inference Progress:  75%|███████▌  | 75/100 [04:42<01:30,  3.61s/it]
Inference Progress:  76%|███████▌  | 76/100 [04:47<01:42,  4.29s/it]
Inference Progress:  77%|███████▋  | 77/100 [04:54<01:52,  4.89s/it]
Inference Progress:  78%|███████▊  | 78/100 [04:56<01:28,  4.03s/it]
Inference Progress:  79%|███████▉  | 79/100 [04:58<01:14,  3.57s/it]
Inference Progress:  80%|████████  | 80/100 [05:00<01:01,  3.08s/it]
Inference Progress:  81%|████████  | 81/100 [05:06<01:12,  3.83s/it]
Inference Progress:  82%|████████▏ | 82/100 [05:14<01:30,  5.02s/it]
Inference Progress:  83%|████████▎ | 83/100 [05:15<01:06,  3.92s/it]
Inference Progress:  84%|████████▍ | 84/100 [05:22<01:16,  4.79s/it]
Inference Progress:  85%|████████▌ | 85/100 [05:26<01:08,  4.57s/it]
Inference Progress:  86%|████████▌ | 86/100 [05:28<00:51,  3.69s/it]
Inference Progress:  87%|████████▋ | 87/100 [05:31<00:45,  3.52s/it]
Inference Progress:  88%|████████▊ | 88/100 [05:32<00:32,  2.74s/it]
Inference Progress:  89%|████████▉ | 89/100 [05:36<00:34,  3.11s/it]
Inference Progress:  90%|█████████ | 90/100 [05:38<00:30,  3.01s/it]
Inference Progress:  91%|█████████ | 91/100 [05:42<00:27,  3.11s/it]
Inference Progress:  92%|█████████▏| 92/100 [05:43<00:19,  2.50s/it]
Inference Progress:  93%|█████████▎| 93/100 [05:44<00:15,  2.22s/it]
Inference Progress:  94%|█████████▍| 94/100 [05:46<00:12,  2.11s/it]
Inference Progress:  95%|█████████▌| 95/100 [05:50<00:13,  2.60s/it]
Inference Progress:  96%|█████████▌| 96/100 [05:53<00:10,  2.73s/it]
Inference Progress:  97%|█████████▋| 97/100 [05:57<00:09,  3.09s/it]
Inference Progress:  98%|█████████▊| 98/100 [06:04<00:08,  4.30s/it]
Inference Progress:  99%|█████████▉| 99/100 [06:09<00:04,  4.66s/it]
Inference Progress: 100%|██████████| 100/100 [06:15<00:00,  4.94s/it]
Inference Progress: 100%|██████████| 100/100 [06:15<00:00,  3.76s/it]
2025-04-24 04:54:49,787 - INFO - 
--- Inference Summary ---
2025-04-24 04:54:49,787 - INFO - Processed 100 samples.
2025-04-24 04:54:49,787 - INFO - Total 'generate' time (sum): 375.39 sec
2025-04-24 04:54:49,787 - INFO - Overall inference loop duration: 375.55 sec
2025-04-24 04:54:49,787 - INFO - Average 'generate' time per sample: 3.7539 sec
2025-04-24 04:54:49,787 - INFO - Total effective tokens generated: 25293
2025-04-24 04:54:49,787 - INFO - Overall effective tokens per second: 67.38
2025-04-24 04:54:49,787 - INFO - RAM Delta during inference: 684.07 MB
2025-04-24 04:54:49,787 - INFO - PyTorch VRAM Peak Delta during inference: 259.83 MB
2025-04-24 04:54:49,789 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/sevenllm_bench__20250424-044830_outputs.json
2025-04-24 04:54:49,789 - INFO - Saved metrics to: /workspace/results/mod/48/sevenllm_bench__20250424-044830_metrics.json
2025-04-24 04:54:49,789 - INFO - Cleaning up resources...
2025-04-24 04:54:49,795 - INFO - CUDA cache cleared.
2025-04-24 04:54:49,795 - INFO - --- Evaluation Complete ---

2025-04-24 04:54:50,695 - INFO - SUCCESS: Run 14/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 04:54:50,695 - INFO - 
Run 15/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='attack'
2025-04-24 04:54:50,695 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset attack --max-new-tokens 512
2025-04-24 04:54:58,436 - WARNING - Stderr:
2025-04-24 04:54:52,820 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:54:53,133 - INFO - --- Starting Evaluation ---
2025-04-24 04:54:53,133 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'attack'}
2025-04-24 04:54:53,133 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:54:53,133 - INFO - Metrics file: /workspace/results/mod/48/ctibench_attack__20250424-045453_metrics.json
2025-04-24 04:54:53,133 - INFO - Outputs file: /workspace/results/mod/48/ctibench_attack__20250424-045453_outputs.json
2025-04-24 04:54:53,133 - INFO - NVML Reporting Active: False
2025-04-24 04:54:53,133 - INFO - Initial RAM usage: 553.80 MB
2025-04-24 04:54:53,133 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/...
2025-04-24 04:54:53,153 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:54:53,449 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:54:56,937 - INFO - CausalLM Model loaded.
2025-04-24 04:54:56,938 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:54:56,938 - INFO - Model loaded in 3.80 seconds.
2025-04-24 04:54:56,938 - INFO - Loading prompts from CTIBench subset: attack
2025-04-24 04:54:57,643 - ERROR - Error loading/processing ctibench (attack): BuilderConfig 'attack' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'attack' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:54:57,644 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:54:57,644 - INFO - Cleaning up resources...
2025-04-24 04:54:57,645 - INFO - CUDA cache cleared.
2025-04-24 04:54:57,645 - INFO - --- Evaluation Complete ---

2025-04-24 04:54:58,437 - INFO - SUCCESS: Run 15/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='attack'. Check output files in /workspace/results/mod/48/
2025-04-24 04:54:58,437 - INFO - 
Run 16/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='exploit_target'
2025-04-24 04:54:58,437 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset exploit_target --max-new-tokens 512
2025-04-24 04:55:06,492 - WARNING - Stderr:
2025-04-24 04:55:00,608 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:55:00,917 - INFO - --- Starting Evaluation ---
2025-04-24 04:55:00,917 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'exploit_target'}
2025-04-24 04:55:00,917 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:55:00,917 - INFO - Metrics file: /workspace/results/mod/48/ctibench_exploit_target__20250424-045500_metrics.json
2025-04-24 04:55:00,917 - INFO - Outputs file: /workspace/results/mod/48/ctibench_exploit_target__20250424-045500_outputs.json
2025-04-24 04:55:00,917 - INFO - NVML Reporting Active: False
2025-04-24 04:55:00,917 - INFO - Initial RAM usage: 557.02 MB
2025-04-24 04:55:00,917 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/...
2025-04-24 04:55:00,937 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:55:01,238 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:55:04,834 - INFO - CausalLM Model loaded.
2025-04-24 04:55:04,835 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:55:04,835 - INFO - Model loaded in 3.92 seconds.
2025-04-24 04:55:04,835 - INFO - Loading prompts from CTIBench subset: exploit_target
2025-04-24 04:55:05,652 - ERROR - Error loading/processing ctibench (exploit_target): BuilderConfig 'exploit_target' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'exploit_target' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:55:05,653 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:55:05,653 - INFO - Cleaning up resources...
2025-04-24 04:55:05,653 - INFO - CUDA cache cleared.
2025-04-24 04:55:05,653 - INFO - --- Evaluation Complete ---

2025-04-24 04:55:06,492 - INFO - SUCCESS: Run 16/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='exploit_target'. Check output files in /workspace/results/mod/48/
2025-04-24 04:55:06,492 - INFO - 
Run 17/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='threat_actor'
2025-04-24 04:55:06,492 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset threat_actor --max-new-tokens 512
2025-04-24 04:55:15,168 - WARNING - Stderr:
2025-04-24 04:55:08,621 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:55:08,933 - INFO - --- Starting Evaluation ---
2025-04-24 04:55:08,934 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'threat_actor'}
2025-04-24 04:55:08,934 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:55:08,934 - INFO - Metrics file: /workspace/results/mod/48/ctibench_threat_actor__20250424-045508_metrics.json
2025-04-24 04:55:08,934 - INFO - Outputs file: /workspace/results/mod/48/ctibench_threat_actor__20250424-045508_outputs.json
2025-04-24 04:55:08,934 - INFO - NVML Reporting Active: False
2025-04-24 04:55:08,934 - INFO - Initial RAM usage: 556.78 MB
2025-04-24 04:55:08,934 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/...
2025-04-24 04:55:08,954 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:55:09,254 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:55:13,103 - INFO - CausalLM Model loaded.
2025-04-24 04:55:13,104 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:55:13,104 - INFO - Model loaded in 4.17 seconds.
2025-04-24 04:55:13,104 - INFO - Loading prompts from CTIBench subset: threat_actor
2025-04-24 04:55:14,384 - ERROR - Error loading/processing ctibench (threat_actor): BuilderConfig 'threat_actor' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'threat_actor' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:55:14,385 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:55:14,386 - INFO - Cleaning up resources...
2025-04-24 04:55:14,386 - INFO - CUDA cache cleared.
2025-04-24 04:55:14,386 - INFO - --- Evaluation Complete ---

2025-04-24 04:55:15,168 - INFO - SUCCESS: Run 17/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='threat_actor'. Check output files in /workspace/results/mod/48/
2025-04-24 04:55:15,168 - INFO - 
Run 18/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='ttps'
2025-04-24 04:55:15,168 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset ttps --max-new-tokens 512
2025-04-24 04:55:22,827 - WARNING - Stderr:
2025-04-24 04:55:17,303 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:55:17,605 - INFO - --- Starting Evaluation ---
2025-04-24 04:55:17,606 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'ttps'}
2025-04-24 04:55:17,606 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:55:17,606 - INFO - Metrics file: /workspace/results/mod/48/ctibench_ttps__20250424-045517_metrics.json
2025-04-24 04:55:17,606 - INFO - Outputs file: /workspace/results/mod/48/ctibench_ttps__20250424-045517_outputs.json
2025-04-24 04:55:17,606 - INFO - NVML Reporting Active: False
2025-04-24 04:55:17,606 - INFO - Initial RAM usage: 556.89 MB
2025-04-24 04:55:17,606 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/...
2025-04-24 04:55:17,626 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:55:17,922 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:55:21,329 - INFO - CausalLM Model loaded.
2025-04-24 04:55:21,330 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:55:21,330 - INFO - Model loaded in 3.72 seconds.
2025-04-24 04:55:21,330 - INFO - Loading prompts from CTIBench subset: ttps
2025-04-24 04:55:21,970 - ERROR - Error loading/processing ctibench (ttps): BuilderConfig 'ttps' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'ttps' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:55:21,971 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:55:21,971 - INFO - Cleaning up resources...
2025-04-24 04:55:21,971 - INFO - CUDA cache cleared.
2025-04-24 04:55:21,971 - INFO - --- Evaluation Complete ---

2025-04-24 04:55:22,827 - INFO - SUCCESS: Run 18/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='ttps'. Check output files in /workspace/results/mod/48/
2025-04-24 04:55:22,827 - INFO - --- Security Benchmark Automation Finished (QUANTIZED MODELS) ---
