2025-04-24 11:48:18,324 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 11:48:18,326 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 11:48:18,327 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 11:48:18,328 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 11:48:18,329 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 11:48:18,330 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_114818.log
2025-04-24 11:48:18,331 - INFO - Models to run: ['Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 11:48:18,332 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 11:48:18,334 - INFO - Running 100 samples per benchmark.
2025-04-24 11:48:18,335 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 11:48:18,337 - INFO - 
Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:48:18,343 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:48:54,903 - WARNING - Stderr:
2025-04-24 11:48:37,718 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:48:37,870 - INFO - --- Starting Evaluation ---
2025-04-24 11:48:37,870 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:48:37,870 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:48:37,870 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-114837_metrics.json
2025-04-24 11:48:37,870 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-114837_outputs.json
2025-04-24 11:48:37,870 - INFO - NVML Reporting Active: False
2025-04-24 11:48:37,870 - INFO - Initial RAM usage: 515.25 MB
2025-04-24 11:48:37,870 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:48:37,892 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:48:38,054 - INFO - Tokenizer loaded.
2025-04-24 11:48:48,109 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:48:48,109 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 11:48:49,133 - WARNING - CUDA extension not installed.
2025-04-24 11:48:49,138 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 11:48:53,655 - INFO - CausalLM Model loaded.
2025-04-24 11:48:53,656 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 11:48:53,657 - INFO - Model loaded in 15.79 seconds.
2025-04-24 11:48:53,657 - INFO - Loading prompts from CyberSecEval3 MITRE file: /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json
2025-04-24 11:48:53,672 - INFO - Processing the first 100 items.

Parsing MITRE prompts:   0%|          | 0/100 [00:00<?, ?it/s]2025-04-24 11:48:53,675 - WARNING - Item 5: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,675 - WARNING - Item 8: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,676 - WARNING - Item 14: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,677 - WARNING - Item 21: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,677 - WARNING - Item 25: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,678 - WARNING - Item 27: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,678 - WARNING - Item 30: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,678 - WARNING - Item 32: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,679 - WARNING - Item 35: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,679 - WARNING - Item 36: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,680 - WARNING - Item 48: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,680 - WARNING - Item 49: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,680 - WARNING - Item 52: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,681 - WARNING - Item 58: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,681 - WARNING - Item 63: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,682 - WARNING - Item 72: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,682 - WARNING - Item 81: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,683 - WARNING - Item 86: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,683 - WARNING - Item 88: Regex failed to find/extract 'prompt' value from mutated_prompt_str.

Parsing MITRE prompts: 100%|██████████| 100/100 [00:00<00:00, 10103.84it/s]
2025-04-24 11:48:53,684 - INFO - Successfully loaded 81 prompts from cyberseceval3_mitre.
2025-04-24 11:48:53,684 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 11:48:53,685 - INFO - Starting inference on 81 samples...

Inference Progress:   0%|          | 0/81 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
2025-04-24 11:48:53,757 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,768 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,772 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,776 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,780 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,783 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:   7%|▋         | 6/81 [00:00<00:01, 59.94it/s]2025-04-24 11:48:53,787 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,792 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,796 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,800 - ERROR - Generation failed for T1024: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,815 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,820 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,824 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,828 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,832 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,836 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,840 - ERROR - Generation failed for T1090.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,843 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,848 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,852 - ERROR - Generation failed for T1090.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,856 - ERROR - Generation failed for T1090.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,860 - ERROR - Generation failed for T1483: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,863 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,867 - ERROR - Generation failed for T1090.003: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,871 - ERROR - Generation failed for T1571: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,875 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,878 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,881 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,885 - ERROR - Generation failed for T1095: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  36%|███▌      | 29/81 [00:00<00:00, 158.70it/s]2025-04-24 11:48:53,888 - ERROR - Generation failed for T1105: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,893 - ERROR - Generation failed for T1094.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,896 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,899 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,915 - ERROR - Generation failed for T1102: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,920 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,924 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,929 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,933 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,938 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,942 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,946 - ERROR - Generation failed for T1074: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,951 - ERROR - Generation failed for T1097: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,955 - ERROR - Generation failed for T1483: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,959 - ERROR - Generation failed for T1021.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,964 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,968 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,971 - ERROR - Generation failed for T1090.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,975 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,978 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,983 - ERROR - Generation failed for T1090.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,986 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  63%|██████▎   | 51/81 [00:00<00:00, 185.37it/s]2025-04-24 11:48:53,990 - ERROR - Generation failed for T1071: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,994 - ERROR - Generation failed for T1074: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,999 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,015 - ERROR - Generation failed for T1090.003: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,019 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,024 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,029 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,034 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,038 - ERROR - Generation failed for T1090.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,043 - ERROR - Generation failed for T1090.003: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,047 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,050 - ERROR - Generation failed for T1094: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,055 - ERROR - Generation failed for T1132.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,060 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,064 - ERROR - Generation failed for T1059: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,069 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,074 - ERROR - Generation failed for T1071: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,078 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,083 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,088 - ERROR - Generation failed for T1094: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  88%|████████▊ | 71/81 [00:00<00:00, 190.14it/s]2025-04-24 11:48:54,092 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,095 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,099 - ERROR - Generation failed for T1071: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,114 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,119 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,125 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,130 - ERROR - Generation failed for T1095: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,135 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,139 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,143 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress: 100%|██████████| 81/81 [00:00<00:00, 176.00it/s]
2025-04-24 11:48:54,146 - INFO - 
--- Inference Summary ---
2025-04-24 11:48:54,146 - INFO - Processed 81 samples.
2025-04-24 11:48:54,146 - INFO - Total 'generate' time (sum): 0.00 sec
2025-04-24 11:48:54,146 - INFO - Overall inference loop duration: 0.46 sec
2025-04-24 11:48:54,146 - INFO - Average 'generate' time per sample: 0.0000 sec
2025-04-24 11:48:54,146 - INFO - Total effective tokens generated: 0
2025-04-24 11:48:54,146 - INFO - Overall effective tokens per second: 0.00
2025-04-24 11:48:54,146 - INFO - RAM Delta during inference: 45.75 MB
2025-04-24 11:48:54,146 - INFO - PyTorch VRAM Peak Delta during inference: 0.02 MB
2025-04-24 11:48:54,155 - INFO - Saved outputs (81 items) to: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-114837_outputs.json
2025-04-24 11:48:54,160 - INFO - Saved metrics to: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-114837_metrics.json
2025-04-24 11:48:54,160 - INFO - Cleaning up resources...
2025-04-24 11:48:54,160 - INFO - CUDA cache cleared.
2025-04-24 11:48:54,160 - INFO - --- Evaluation Complete ---

2025-04-24 11:48:54,919 - INFO - SUCCESS: Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 11:48:54,920 - INFO - 
Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 11:48:54,921 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:49:27,128 - WARNING - Stderr:
2025-04-24 11:49:10,936 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:49:11,092 - INFO - --- Starting Evaluation ---
2025-04-24 11:49:11,092 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:49:11,092 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:49:11,092 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-114911_metrics.json
2025-04-24 11:49:11,092 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-114911_outputs.json
2025-04-24 11:49:11,092 - INFO - NVML Reporting Active: False
2025-04-24 11:49:11,092 - INFO - Initial RAM usage: 520.75 MB
2025-04-24 11:49:11,092 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:49:11,115 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:49:11,246 - INFO - Tokenizer loaded.
2025-04-24 11:49:21,130 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:49:21,131 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 11:49:22,190 - WARNING - CUDA extension not installed.
2025-04-24 11:49:22,196 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 11:49:25,662 - INFO - CausalLM Model loaded.
2025-04-24 11:49:25,663 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 11:49:25,664 - INFO - Model loaded in 14.57 seconds.
2025-04-24 11:49:25,664 - INFO - Loading prompts from SEvenLLM saved dataset directory: /workspace/calibration_data/sevenllm_instruct_subset_manual/
2025-04-24 11:49:25,734 - INFO - Loaded dataset: Dataset({
    features: ['output', 'category', 'thought', 'input', 'id', 'instruction'],
    num_rows: 200
})
2025-04-24 11:49:25,736 - INFO - Selected first 100 samples for evaluation.

Parsing SEvenLLM prompts:   0%|          | 0/100 [00:00<?, ?it/s]
Parsing SEvenLLM prompts: 100%|██████████| 100/100 [00:00<00:00, 11293.53it/s]
2025-04-24 11:49:25,746 - INFO - Successfully loaded 100 prompts from sevenllm_bench.
2025-04-24 11:49:25,746 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 11:49:25,747 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
2025-04-24 11:49:25,828 - ERROR - Generation failed for 1: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,840 - ERROR - Generation failed for 2: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,845 - ERROR - Generation failed for 3: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,850 - ERROR - Generation failed for 4: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:   4%|▍         | 4/100 [00:00<00:02, 38.27it/s]2025-04-24 11:49:25,854 - ERROR - Generation failed for 5: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,859 - ERROR - Generation failed for 6: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,864 - ERROR - Generation failed for 7: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,869 - ERROR - Generation failed for 8: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,874 - ERROR - Generation failed for 9: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,878 - ERROR - Generation failed for 10: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,883 - ERROR - Generation failed for 11: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,887 - ERROR - Generation failed for 12: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,892 - ERROR - Generation failed for 13: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,896 - ERROR - Generation failed for 14: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,900 - ERROR - Generation failed for 15: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,914 - ERROR - Generation failed for 16: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,920 - ERROR - Generation failed for 17: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,924 - ERROR - Generation failed for 18: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,930 - ERROR - Generation failed for 19: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,935 - ERROR - Generation failed for 20: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,941 - ERROR - Generation failed for 21: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,945 - ERROR - Generation failed for 22: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,951 - ERROR - Generation failed for 23: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  23%|██▎       | 23/100 [00:00<00:00, 125.20it/s]2025-04-24 11:49:25,957 - ERROR - Generation failed for 24: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,964 - ERROR - Generation failed for 25: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,968 - ERROR - Generation failed for 26: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,973 - ERROR - Generation failed for 27: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,978 - ERROR - Generation failed for 28: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,982 - ERROR - Generation failed for 29: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,986 - ERROR - Generation failed for 30: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,990 - ERROR - Generation failed for 31: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,994 - ERROR - Generation failed for 32: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,998 - ERROR - Generation failed for 33: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,002 - ERROR - Generation failed for 34: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,014 - ERROR - Generation failed for 35: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,019 - ERROR - Generation failed for 36: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,024 - ERROR - Generation failed for 37: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,028 - ERROR - Generation failed for 38: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,033 - ERROR - Generation failed for 39: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,037 - ERROR - Generation failed for 40: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,041 - ERROR - Generation failed for 41: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,045 - ERROR - Generation failed for 42: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,049 - ERROR - Generation failed for 43: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,054 - ERROR - Generation failed for 44: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  44%|████▍     | 44/100 [00:00<00:00, 161.20it/s]2025-04-24 11:49:26,059 - ERROR - Generation failed for 45: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,064 - ERROR - Generation failed for 46: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,070 - ERROR - Generation failed for 47: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,075 - ERROR - Generation failed for 48: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,081 - ERROR - Generation failed for 49: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,085 - ERROR - Generation failed for 50: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,089 - ERROR - Generation failed for 51: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,093 - ERROR - Generation failed for 52: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,098 - ERROR - Generation failed for 53: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,102 - ERROR - Generation failed for 54: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,116 - ERROR - Generation failed for 55: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,122 - ERROR - Generation failed for 56: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,126 - ERROR - Generation failed for 57: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,131 - ERROR - Generation failed for 58: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,135 - ERROR - Generation failed for 59: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,140 - ERROR - Generation failed for 60: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,145 - ERROR - Generation failed for 61: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,150 - ERROR - Generation failed for 62: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,155 - ERROR - Generation failed for 63: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  63%|██████▎   | 63/100 [00:00<00:00, 171.63it/s]2025-04-24 11:49:26,160 - ERROR - Generation failed for 64: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,166 - ERROR - Generation failed for 65: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,170 - ERROR - Generation failed for 66: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,174 - ERROR - Generation failed for 67: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,178 - ERROR - Generation failed for 68: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,183 - ERROR - Generation failed for 69: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,188 - ERROR - Generation failed for 70: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,192 - ERROR - Generation failed for 71: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,197 - ERROR - Generation failed for 72: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,200 - ERROR - Generation failed for 73: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,214 - ERROR - Generation failed for 74: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,219 - ERROR - Generation failed for 75: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,226 - ERROR - Generation failed for 76: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,232 - ERROR - Generation failed for 77: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,237 - ERROR - Generation failed for 78: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,240 - ERROR - Generation failed for 79: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,244 - ERROR - Generation failed for 80: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,251 - ERROR - Generation failed for 81: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,257 - ERROR - Generation failed for 82: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  82%|████████▏ | 82/100 [00:00<00:00, 177.13it/s]2025-04-24 11:49:26,261 - ERROR - Generation failed for 83: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,267 - ERROR - Generation failed for 84: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,272 - ERROR - Generation failed for 85: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,276 - ERROR - Generation failed for 86: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,281 - ERROR - Generation failed for 87: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,284 - ERROR - Generation failed for 88: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,289 - ERROR - Generation failed for 89: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,293 - ERROR - Generation failed for 90: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,297 - ERROR - Generation failed for 91: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,301 - ERROR - Generation failed for 92: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,312 - ERROR - Generation failed for 93: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,316 - ERROR - Generation failed for 94: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,320 - ERROR - Generation failed for 95: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,325 - ERROR - Generation failed for 96: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,331 - ERROR - Generation failed for 97: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,336 - ERROR - Generation failed for 98: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,343 - ERROR - Generation failed for 99: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,349 - ERROR - Generation failed for 100: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress: 100%|██████████| 100/100 [00:00<00:00, 165.56it/s]
2025-04-24 11:49:26,352 - INFO - 
--- Inference Summary ---
2025-04-24 11:49:26,352 - INFO - Processed 100 samples.
2025-04-24 11:49:26,352 - INFO - Total 'generate' time (sum): 0.00 sec
2025-04-24 11:49:26,352 - INFO - Overall inference loop duration: 0.60 sec
2025-04-24 11:49:26,352 - INFO - Average 'generate' time per sample: 0.0000 sec
2025-04-24 11:49:26,352 - INFO - Total effective tokens generated: 0
2025-04-24 11:49:26,352 - INFO - Overall effective tokens per second: 0.00
2025-04-24 11:49:26,352 - INFO - RAM Delta during inference: 39.00 MB
2025-04-24 11:49:26,352 - INFO - PyTorch VRAM Peak Delta during inference: 0.03 MB
2025-04-24 11:49:26,359 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-114911_outputs.json
2025-04-24 11:49:26,362 - INFO - Saved metrics to: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-114911_metrics.json
2025-04-24 11:49:26,362 - INFO - Cleaning up resources...
2025-04-24 11:49:26,363 - INFO - CUDA cache cleared.
2025-04-24 11:49:26,363 - INFO - --- Evaluation Complete ---

2025-04-24 11:49:27,139 - INFO - SUCCESS: Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 11:49:27,140 - INFO - 
Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 11:49:27,145 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 11:49:59,897 - WARNING - Stderr:
2025-04-24 11:49:43,470 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:49:43,624 - INFO - --- Starting Evaluation ---
2025-04-24 11:49:43,625 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq'}
2025-04-24 11:49:43,625 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:49:43,625 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-114943_metrics.json
2025-04-24 11:49:43,625 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-114943_outputs.json
2025-04-24 11:49:43,625 - INFO - NVML Reporting Active: False
2025-04-24 11:49:43,625 - INFO - Initial RAM usage: 517.29 MB
2025-04-24 11:49:43,625 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:49:43,648 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:49:43,773 - INFO - Tokenizer loaded.
2025-04-24 11:49:53,909 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:49:53,909 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 11:49:54,981 - WARNING - CUDA extension not installed.
2025-04-24 11:49:54,987 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 11:49:57,832 - INFO - CausalLM Model loaded.
2025-04-24 11:49:57,833 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 11:49:57,833 - INFO - Model loaded in 14.21 seconds.
2025-04-24 11:49:57,833 - INFO - Loading prompts from CTIBench subset: cti-mcq
2025-04-24 11:49:58,343 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 11:49:58,348 - INFO - Selected first 100 samples.
2025-04-24 11:49:58,348 - INFO - Using columns for cti-mcq: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-mcq):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-mcq): 100%|██████████| 100/100 [00:00<00:00, 4545.34it/s]
2025-04-24 11:49:58,371 - INFO - Successfully loaded 100 prompts from ctibench subset cti-mcq.
2025-04-24 11:49:58,372 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 11:49:58,373 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
2025-04-24 11:49:58,466 - ERROR - Generation failed for index_0: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:   1%|          | 1/100 [00:00<00:09, 10.00it/s]2025-04-24 11:49:58,478 - ERROR - Generation failed for index_1: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,483 - ERROR - Generation failed for index_2: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,488 - ERROR - Generation failed for index_3: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,494 - ERROR - Generation failed for index_4: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,500 - ERROR - Generation failed for index_5: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,515 - ERROR - Generation failed for index_6: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,521 - ERROR - Generation failed for index_7: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,527 - ERROR - Generation failed for index_8: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,533 - ERROR - Generation failed for index_9: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,538 - ERROR - Generation failed for index_10: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,544 - ERROR - Generation failed for index_11: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,550 - ERROR - Generation failed for index_12: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,555 - ERROR - Generation failed for index_13: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,560 - ERROR - Generation failed for index_14: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,565 - ERROR - Generation failed for index_15: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,570 - ERROR - Generation failed for index_16: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,575 - ERROR - Generation failed for index_17: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  18%|█▊        | 18/100 [00:00<00:00, 102.31it/s]2025-04-24 11:49:58,580 - ERROR - Generation failed for index_18: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,585 - ERROR - Generation failed for index_19: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,589 - ERROR - Generation failed for index_20: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,594 - ERROR - Generation failed for index_21: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,599 - ERROR - Generation failed for index_22: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,618 - ERROR - Generation failed for index_23: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,623 - ERROR - Generation failed for index_24: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,629 - ERROR - Generation failed for index_25: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,635 - ERROR - Generation failed for index_26: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,640 - ERROR - Generation failed for index_27: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,646 - ERROR - Generation failed for index_28: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,651 - ERROR - Generation failed for index_29: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,656 - ERROR - Generation failed for index_30: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,661 - ERROR - Generation failed for index_31: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,666 - ERROR - Generation failed for index_32: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,671 - ERROR - Generation failed for index_33: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,677 - ERROR - Generation failed for index_34: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  35%|███▌      | 35/100 [00:00<00:00, 131.95it/s]2025-04-24 11:49:58,682 - ERROR - Generation failed for index_35: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,687 - ERROR - Generation failed for index_36: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,692 - ERROR - Generation failed for index_37: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,697 - ERROR - Generation failed for index_38: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,702 - ERROR - Generation failed for index_39: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,716 - ERROR - Generation failed for index_40: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,721 - ERROR - Generation failed for index_41: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,728 - ERROR - Generation failed for index_42: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,734 - ERROR - Generation failed for index_43: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,740 - ERROR - Generation failed for index_44: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,745 - ERROR - Generation failed for index_45: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,750 - ERROR - Generation failed for index_46: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,756 - ERROR - Generation failed for index_47: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,761 - ERROR - Generation failed for index_48: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,766 - ERROR - Generation failed for index_49: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,771 - ERROR - Generation failed for index_50: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,775 - ERROR - Generation failed for index_51: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,780 - ERROR - Generation failed for index_52: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  53%|█████▎    | 53/100 [00:00<00:00, 148.55it/s]2025-04-24 11:49:58,785 - ERROR - Generation failed for index_53: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,790 - ERROR - Generation failed for index_54: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,795 - ERROR - Generation failed for index_55: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,799 - ERROR - Generation failed for index_56: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,818 - ERROR - Generation failed for index_57: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,823 - ERROR - Generation failed for index_58: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,830 - ERROR - Generation failed for index_59: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,837 - ERROR - Generation failed for index_60: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,842 - ERROR - Generation failed for index_61: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,848 - ERROR - Generation failed for index_62: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,853 - ERROR - Generation failed for index_63: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,859 - ERROR - Generation failed for index_64: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,865 - ERROR - Generation failed for index_65: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,870 - ERROR - Generation failed for index_66: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,876 - ERROR - Generation failed for index_67: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,881 - ERROR - Generation failed for index_68: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  69%|██████▉   | 69/100 [00:00<00:00, 152.41it/s]2025-04-24 11:49:58,886 - ERROR - Generation failed for index_69: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,891 - ERROR - Generation failed for index_70: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,897 - ERROR - Generation failed for index_71: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,902 - ERROR - Generation failed for index_72: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,916 - ERROR - Generation failed for index_73: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,922 - ERROR - Generation failed for index_74: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,928 - ERROR - Generation failed for index_75: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,934 - ERROR - Generation failed for index_76: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,940 - ERROR - Generation failed for index_77: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,945 - ERROR - Generation failed for index_78: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,951 - ERROR - Generation failed for index_79: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,956 - ERROR - Generation failed for index_80: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,961 - ERROR - Generation failed for index_81: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,966 - ERROR - Generation failed for index_82: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,972 - ERROR - Generation failed for index_83: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,978 - ERROR - Generation failed for index_84: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,983 - ERROR - Generation failed for index_85: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  86%|████████▌ | 86/100 [00:00<00:00, 157.25it/s]2025-04-24 11:49:58,988 - ERROR - Generation failed for index_86: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,993 - ERROR - Generation failed for index_87: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,998 - ERROR - Generation failed for index_88: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,003 - ERROR - Generation failed for index_89: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,018 - ERROR - Generation failed for index_90: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,024 - ERROR - Generation failed for index_91: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,030 - ERROR - Generation failed for index_92: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,036 - ERROR - Generation failed for index_93: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,042 - ERROR - Generation failed for index_94: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,048 - ERROR - Generation failed for index_95: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,053 - ERROR - Generation failed for index_96: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,058 - ERROR - Generation failed for index_97: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,064 - ERROR - Generation failed for index_98: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,069 - ERROR - Generation failed for index_99: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress: 100%|██████████| 100/100 [00:00<00:00, 143.28it/s]
2025-04-24 11:49:59,073 - INFO - 
--- Inference Summary ---
2025-04-24 11:49:59,073 - INFO - Processed 100 samples.
2025-04-24 11:49:59,073 - INFO - Total 'generate' time (sum): 0.00 sec
2025-04-24 11:49:59,073 - INFO - Overall inference loop duration: 0.70 sec
2025-04-24 11:49:59,073 - INFO - Average 'generate' time per sample: 0.0000 sec
2025-04-24 11:49:59,073 - INFO - Total effective tokens generated: 0
2025-04-24 11:49:59,073 - INFO - Overall effective tokens per second: 0.00
2025-04-24 11:49:59,073 - INFO - RAM Delta during inference: 68.25 MB
2025-04-24 11:49:59,073 - INFO - PyTorch VRAM Peak Delta during inference: 0.01 MB
2025-04-24 11:49:59,080 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-114943_outputs.json
2025-04-24 11:49:59,089 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-114943_metrics.json
2025-04-24 11:49:59,089 - INFO - Cleaning up resources...
2025-04-24 11:49:59,090 - INFO - CUDA cache cleared.
2025-04-24 11:49:59,090 - INFO - --- Evaluation Complete ---

2025-04-24 11:49:59,918 - INFO - SUCCESS: Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 11:49:59,919 - INFO - 
Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 11:49:59,920 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
