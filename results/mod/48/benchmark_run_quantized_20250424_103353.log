2025-04-24 10:33:53,785 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 10:33:53,786 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 10:33:53,787 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 10:33:53,787 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 10:33:53,788 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 10:33:53,788 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_103353.log
2025-04-24 10:33:53,789 - INFO - Models to run: ['TinyLlama-1.1B-Chat-v1.0-GPTQ', 'Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 10:33:53,789 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 10:33:53,790 - INFO - Running 100 samples per benchmark.
2025-04-24 10:33:53,790 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 10:33:53,791 - INFO - 
Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 10:33:53,793 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 10:34:52,378 - WARNING - Stderr:
2025-04-24 10:34:38,889 - INFO - Initialized NVML for device 0.
2025-04-24 10:34:39,043 - INFO - --- Starting Evaluation ---
2025-04-24 10:34:39,043 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 10:34:39,043 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:34:39,043 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103439_metrics.json
2025-04-24 10:34:39,043 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103439_outputs.json
2025-04-24 10:34:39,043 - INFO - NVML Reporting Active: True
2025-04-24 10:34:39,043 - INFO - Initial RAM usage: 597.75 MB
2025-04-24 10:34:39,043 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:34:39,043 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:34:39,051 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:34:39,339 - INFO - Tokenizer loaded.
2025-04-24 10:34:50,941 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:34:50,967 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:34:50,968 - INFO - Cleaning up resources...
2025-04-24 10:34:50,968 - INFO - CUDA cache cleared.
2025-04-24 10:34:50,968 - INFO - NVML shut down.
2025-04-24 10:34:50,968 - INFO - --- Evaluation Complete ---

2025-04-24 10:34:52,380 - INFO - SUCCESS: Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 10:34:52,381 - INFO - 
Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 10:34:52,382 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 10:35:16,400 - WARNING - Stderr:
2025-04-24 10:35:12,412 - INFO - Initialized NVML for device 0.
2025-04-24 10:35:12,562 - INFO - --- Starting Evaluation ---
2025-04-24 10:35:12,562 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 10:35:12,562 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:35:12,562 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103512_metrics.json
2025-04-24 10:35:12,562 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103512_outputs.json
2025-04-24 10:35:12,562 - INFO - NVML Reporting Active: True
2025-04-24 10:35:12,562 - INFO - Initial RAM usage: 603.41 MB
2025-04-24 10:35:12,562 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:35:12,562 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:35:12,569 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:35:12,705 - INFO - Tokenizer loaded.
2025-04-24 10:35:15,766 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:35:15,783 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:35:15,783 - INFO - Cleaning up resources...
2025-04-24 10:35:15,784 - INFO - CUDA cache cleared.
2025-04-24 10:35:15,784 - INFO - NVML shut down.
2025-04-24 10:35:15,784 - INFO - --- Evaluation Complete ---

2025-04-24 10:35:16,413 - INFO - SUCCESS: Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 10:35:16,414 - INFO - 
Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 10:35:16,415 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 10:35:40,916 - WARNING - Stderr:
2025-04-24 10:35:37,295 - INFO - Initialized NVML for device 0.
2025-04-24 10:35:37,443 - INFO - --- Starting Evaluation ---
2025-04-24 10:35:37,443 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq'}
2025-04-24 10:35:37,443 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:35:37,443 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103537_metrics.json
2025-04-24 10:35:37,443 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103537_outputs.json
2025-04-24 10:35:37,443 - INFO - NVML Reporting Active: True
2025-04-24 10:35:37,443 - INFO - Initial RAM usage: 604.53 MB
2025-04-24 10:35:37,443 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:35:37,443 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:35:37,449 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:35:37,582 - INFO - Tokenizer loaded.
2025-04-24 10:35:40,275 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:35:40,294 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:35:40,295 - INFO - Cleaning up resources...
2025-04-24 10:35:40,295 - INFO - CUDA cache cleared.
2025-04-24 10:35:40,295 - INFO - NVML shut down.
2025-04-24 10:35:40,295 - INFO - --- Evaluation Complete ---

2025-04-24 10:35:40,919 - INFO - SUCCESS: Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 10:35:40,920 - INFO - 
Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 10:35:40,921 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 10:36:04,565 - WARNING - Stderr:
2025-04-24 10:36:00,877 - INFO - Initialized NVML for device 0.
2025-04-24 10:36:01,020 - INFO - --- Starting Evaluation ---
2025-04-24 10:36:01,020 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-vsp'}
2025-04-24 10:36:01,021 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:36:01,021 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103601_metrics.json
2025-04-24 10:36:01,021 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103601_outputs.json
2025-04-24 10:36:01,021 - INFO - NVML Reporting Active: True
2025-04-24 10:36:01,021 - INFO - Initial RAM usage: 602.79 MB
2025-04-24 10:36:01,021 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:36:01,021 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:36:01,027 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:36:01,160 - INFO - Tokenizer loaded.
2025-04-24 10:36:03,926 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:36:03,946 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:36:03,947 - INFO - Cleaning up resources...
2025-04-24 10:36:03,947 - INFO - CUDA cache cleared.
2025-04-24 10:36:03,948 - INFO - NVML shut down.
2025-04-24 10:36:03,948 - INFO - --- Evaluation Complete ---

2025-04-24 10:36:04,567 - INFO - SUCCESS: Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'. Check output files in /workspace/results/mod/48/
2025-04-24 10:36:04,568 - INFO - 
Run 5/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 10:36:04,569 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 10:36:29,033 - WARNING - Stderr:
2025-04-24 10:36:25,368 - INFO - Initialized NVML for device 0.
2025-04-24 10:36:25,514 - INFO - --- Starting Evaluation ---
2025-04-24 10:36:25,514 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-rcm'}
2025-04-24 10:36:25,514 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:36:25,514 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-rcm_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103625_metrics.json
2025-04-24 10:36:25,514 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-rcm_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103625_outputs.json
2025-04-24 10:36:25,514 - INFO - NVML Reporting Active: True
2025-04-24 10:36:25,515 - INFO - Initial RAM usage: 604.64 MB
2025-04-24 10:36:25,515 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:36:25,515 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:36:25,520 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:36:25,655 - INFO - Tokenizer loaded.
2025-04-24 10:36:28,383 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:36:28,402 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:36:28,412 - INFO - Cleaning up resources...
2025-04-24 10:36:28,412 - INFO - CUDA cache cleared.
2025-04-24 10:36:28,412 - INFO - NVML shut down.
2025-04-24 10:36:28,412 - INFO - --- Evaluation Complete ---

2025-04-24 10:36:29,036 - INFO - SUCCESS: Run 5/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-rcm'. Check output files in /workspace/results/mod/48/
2025-04-24 10:36:29,037 - INFO - 
Run 6/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 10:36:29,039 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
