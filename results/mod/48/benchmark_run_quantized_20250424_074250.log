2025-04-24 07:42:50,020 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 07:42:50,021 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 07:42:50,022 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 07:42:50,023 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 07:42:50,023 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 07:42:50,024 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_074250.log
2025-04-24 07:42:50,024 - INFO - Models to run: ['TinyLlama-1.1B-Chat-v1.0-GPTQ', 'Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 07:42:50,025 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 07:42:50,026 - INFO - Running 100 samples per benchmark.
2025-04-24 07:42:50,027 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 07:42:50,027 - INFO - 
Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 07:42:50,029 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 08:12:09,665 - WARNING - Stderr:
2025-04-24 07:43:17,006 - INFO - Initialized NVML for device 0.
2025-04-24 07:43:17,155 - INFO - --- Starting Evaluation ---
2025-04-24 07:43:17,155 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 07:43:17,155 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 07:43:17,155 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-074317_metrics.json
2025-04-24 07:43:17,155 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-074317_outputs.json
2025-04-24 07:43:17,155 - INFO - NVML Reporting Active: True
2025-04-24 07:43:17,155 - INFO - Initial RAM usage: 671.55 MB
2025-04-24 07:43:17,155 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 07:43:17,155 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 07:43:17,159 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 07:43:17,311 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 07:43:21,101 - WARNING - CUDA extension not installed.
2025-04-24 07:43:21,116 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 07:43:23,403 - INFO - CausalLM Model loaded.
2025-04-24 07:43:23,404 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 07:43:23,404 - INFO - Model loaded in 6.25 seconds.
2025-04-24 07:43:23,404 - INFO - Loading prompts from CyberSecEval3 MITRE file: /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json
2025-04-24 07:43:23,422 - INFO - Processing the first 100 items.

Parsing MITRE prompts:   0%|          | 0/100 [00:00<?, ?it/s]2025-04-24 07:43:23,424 - WARNING - Item 5: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,424 - WARNING - Item 8: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,425 - WARNING - Item 14: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,425 - WARNING - Item 21: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,426 - WARNING - Item 25: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,426 - WARNING - Item 27: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,426 - WARNING - Item 30: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,426 - WARNING - Item 32: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,427 - WARNING - Item 35: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,427 - WARNING - Item 36: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,427 - WARNING - Item 48: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,427 - WARNING - Item 49: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,428 - WARNING - Item 52: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,428 - WARNING - Item 58: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,428 - WARNING - Item 63: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,429 - WARNING - Item 72: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,429 - WARNING - Item 81: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,430 - WARNING - Item 86: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,430 - WARNING - Item 88: Regex failed to find/extract 'prompt' value from mutated_prompt_str.

Parsing MITRE prompts: 100%|██████████| 100/100 [00:00<00:00, 14194.40it/s]
2025-04-24 07:43:23,431 - INFO - Successfully loaded 81 prompts from cyberseceval3_mitre.
2025-04-24 07:43:23,431 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 07:43:23,432 - INFO - Starting inference on 81 samples...

Inference Progress:   0%|          | 0/81 [00:00<?, ?it/s]
Inference Progress:   1%|          | 1/81 [00:22<30:21, 22.77s/it]
Inference Progress:   2%|▏         | 2/81 [00:45<29:36, 22.49s/it]
Inference Progress:   4%|▎         | 3/81 [01:07<29:11, 22.46s/it]
Inference Progress:   5%|▍         | 4/81 [01:23<25:26, 19.82s/it]
Inference Progress:   6%|▌         | 5/81 [01:39<23:18, 18.40s/it]
Inference Progress:   7%|▋         | 6/81 [01:54<21:46, 17.42s/it]
Inference Progress:   9%|▊         | 7/81 [02:16<23:27, 19.02s/it]
Inference Progress:  10%|▉         | 8/81 [02:39<24:25, 20.07s/it]
Inference Progress:  11%|█         | 9/81 [03:01<24:54, 20.76s/it]
Inference Progress:  12%|█▏        | 10/81 [03:23<25:07, 21.24s/it]
Inference Progress:  14%|█▎        | 11/81 [03:41<23:31, 20.17s/it]
Inference Progress:  15%|█▍        | 12/81 [04:03<23:57, 20.83s/it]
Inference Progress:  16%|█▌        | 13/81 [04:26<24:05, 21.25s/it]
Inference Progress:  17%|█▋        | 14/81 [04:48<24:03, 21.54s/it]
Inference Progress:  19%|█▊        | 15/81 [05:10<23:54, 21.74s/it]
Inference Progress:  20%|█▉        | 16/81 [05:30<23:03, 21.29s/it]
Inference Progress:  21%|██        | 17/81 [05:53<23:03, 21.62s/it]
Inference Progress:  22%|██▏       | 18/81 [06:15<22:52, 21.79s/it]
Inference Progress:  23%|██▎       | 19/81 [06:37<22:43, 21.99s/it]
Inference Progress:  25%|██▍       | 20/81 [07:00<22:30, 22.14s/it]
Inference Progress:  26%|██▌       | 21/81 [07:22<22:10, 22.18s/it]
Inference Progress:  27%|██▋       | 22/81 [07:44<21:49, 22.19s/it]
Inference Progress:  28%|██▊       | 23/81 [08:07<21:26, 22.19s/it]
Inference Progress:  30%|██▉       | 24/81 [08:29<21:04, 22.19s/it]
Inference Progress:  31%|███       | 25/81 [08:51<20:43, 22.20s/it]
Inference Progress:  32%|███▏      | 26/81 [09:13<20:21, 22.20s/it]
Inference Progress:  33%|███▎      | 27/81 [09:35<19:57, 22.18s/it]
Inference Progress:  35%|███▍      | 28/81 [09:57<19:34, 22.16s/it]
Inference Progress:  36%|███▌      | 29/81 [10:20<19:12, 22.16s/it]
Inference Progress:  37%|███▋      | 30/81 [10:42<18:50, 22.16s/it]
Inference Progress:  38%|███▊      | 31/81 [11:04<18:29, 22.19s/it]
Inference Progress:  40%|███▉      | 32/81 [11:18<16:11, 19.83s/it]
Inference Progress:  41%|████      | 33/81 [11:40<16:26, 20.55s/it]
Inference Progress:  42%|████▏     | 34/81 [12:03<16:29, 21.05s/it]
Inference Progress:  43%|████▎     | 35/81 [12:25<16:24, 21.41s/it]
Inference Progress:  44%|████▍     | 36/81 [12:47<16:14, 21.65s/it]
Inference Progress:  46%|████▌     | 37/81 [13:09<16:00, 21.82s/it]
Inference Progress:  47%|████▋     | 38/81 [13:27<14:40, 20.48s/it]
Inference Progress:  48%|████▊     | 39/81 [13:49<14:41, 21.00s/it]
Inference Progress:  49%|████▉     | 40/81 [14:11<14:36, 21.38s/it]
Inference Progress:  51%|█████     | 41/81 [14:33<14:25, 21.64s/it]
Inference Progress:  52%|█████▏    | 42/81 [14:56<14:11, 21.83s/it]
Inference Progress:  53%|█████▎    | 43/81 [15:18<13:53, 21.95s/it]
Inference Progress:  54%|█████▍    | 44/81 [15:41<13:38, 22.13s/it]
Inference Progress:  56%|█████▌    | 45/81 [16:03<13:18, 22.19s/it]
Inference Progress:  57%|█████▋    | 46/81 [16:25<12:57, 22.21s/it]
Inference Progress:  58%|█████▊    | 47/81 [16:47<12:34, 22.19s/it]
Inference Progress:  59%|█████▉    | 48/81 [17:10<12:13, 22.21s/it]
Inference Progress:  60%|██████    | 49/81 [17:18<09:43, 18.22s/it]
Inference Progress:  62%|██████▏   | 50/81 [17:41<10:02, 19.45s/it]
Inference Progress:  63%|██████▎   | 51/81 [18:03<10:08, 20.27s/it]
Inference Progress:  64%|██████▍   | 52/81 [18:25<10:04, 20.85s/it]
Inference Progress:  65%|██████▌   | 53/81 [18:47<09:55, 21.27s/it]
Inference Progress:  67%|██████▋   | 54/81 [19:10<09:41, 21.54s/it]
Inference Progress:  68%|██████▊   | 55/81 [19:32<09:25, 21.74s/it]
Inference Progress:  69%|██████▉   | 56/81 [19:54<09:06, 21.86s/it]
Inference Progress:  70%|███████   | 57/81 [20:16<08:47, 21.97s/it]
Inference Progress:  72%|███████▏  | 58/81 [20:33<07:52, 20.55s/it]
Inference Progress:  73%|███████▎  | 59/81 [20:56<07:43, 21.05s/it]
Inference Progress:  74%|███████▍  | 60/81 [21:18<07:29, 21.39s/it]
Inference Progress:  75%|███████▌  | 61/81 [21:40<07:13, 21.66s/it]
Inference Progress:  77%|███████▋  | 62/81 [22:02<06:54, 21.81s/it]
Inference Progress:  78%|███████▊  | 63/81 [22:24<06:34, 21.92s/it]
Inference Progress:  79%|███████▉  | 64/81 [22:47<06:14, 22.04s/it]
Inference Progress:  80%|████████  | 65/81 [23:09<05:55, 22.25s/it]
Inference Progress:  81%|████████▏ | 66/81 [23:24<04:59, 19.95s/it]
Inference Progress:  83%|████████▎ | 67/81 [23:41<04:25, 18.93s/it]
Inference Progress:  84%|████████▍ | 68/81 [24:03<04:19, 19.94s/it]
Inference Progress:  85%|████████▌ | 69/81 [24:25<04:07, 20.61s/it]
Inference Progress:  86%|████████▋ | 70/81 [24:47<03:52, 21.10s/it]
Inference Progress:  88%|████████▊ | 71/81 [25:10<03:34, 21.46s/it]
Inference Progress:  89%|████████▉ | 72/81 [25:32<03:15, 21.71s/it]
Inference Progress:  90%|█████████ | 73/81 [25:54<02:54, 21.86s/it]
Inference Progress:  91%|█████████▏| 74/81 [26:16<02:33, 21.95s/it]
Inference Progress:  93%|█████████▎| 75/81 [26:39<02:12, 22.06s/it]
Inference Progress:  94%|█████████▍| 76/81 [27:01<01:50, 22.12s/it]
Inference Progress:  95%|█████████▌| 77/81 [27:23<01:28, 22.17s/it]
Inference Progress:  96%|█████████▋| 78/81 [27:45<01:06, 22.19s/it]
Inference Progress:  98%|█████████▊| 79/81 [28:08<00:44, 22.22s/it]
Inference Progress:  99%|█████████▉| 80/81 [28:30<00:22, 22.21s/it]
Inference Progress: 100%|██████████| 81/81 [28:45<00:00, 20.05s/it]
Inference Progress: 100%|██████████| 81/81 [28:45<00:00, 21.30s/it]
2025-04-24 08:12:08,762 - INFO - 
--- Inference Summary ---
2025-04-24 08:12:08,762 - INFO - Processed 81 samples.
2025-04-24 08:12:08,762 - INFO - Total 'generate' time (sum): 1725.15 sec
2025-04-24 08:12:08,762 - INFO - Overall inference loop duration: 1725.33 sec
2025-04-24 08:12:08,762 - INFO - Average 'generate' time per sample: 21.2982 sec
2025-04-24 08:12:08,762 - INFO - Total effective tokens generated: 39670
2025-04-24 08:12:08,762 - INFO - Overall effective tokens per second: 23.00
2025-04-24 08:12:08,762 - INFO - RAM Delta during inference: 498.75 MB
2025-04-24 08:12:08,762 - INFO - PyTorch VRAM Peak Delta during inference: 87.98 MB
2025-04-24 08:12:08,762 - INFO - System VRAM Peak Approx. Delta during inference: -1605.38 MB
2025-04-24 08:12:08,770 - INFO - Saved outputs (81 items) to: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-074317_outputs.json
2025-04-24 08:12:08,773 - INFO - Saved metrics to: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-074317_metrics.json
2025-04-24 08:12:08,774 - INFO - Cleaning up resources...
2025-04-24 08:12:08,777 - INFO - CUDA cache cleared.
2025-04-24 08:12:08,777 - INFO - NVML shut down.
2025-04-24 08:12:08,777 - INFO - --- Evaluation Complete ---

2025-04-24 08:12:09,667 - INFO - SUCCESS: Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 08:12:09,668 - INFO - 
Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 08:12:09,669 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 08:37:26,153 - WARNING - Stderr:
2025-04-24 08:12:54,601 - INFO - Initialized NVML for device 0.
2025-04-24 08:12:54,742 - INFO - --- Starting Evaluation ---
2025-04-24 08:12:54,742 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 08:12:54,742 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 08:12:54,742 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-081254_metrics.json
2025-04-24 08:12:54,742 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-081254_outputs.json
2025-04-24 08:12:54,742 - INFO - NVML Reporting Active: True
2025-04-24 08:12:54,742 - INFO - Initial RAM usage: 659.71 MB
2025-04-24 08:12:54,742 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 08:12:54,742 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 08:12:54,750 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 08:12:55,064 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 08:12:59,820 - WARNING - CUDA extension not installed.
2025-04-24 08:12:59,827 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 08:13:08,522 - INFO - CausalLM Model loaded.
2025-04-24 08:13:08,523 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 08:13:08,523 - INFO - Model loaded in 13.78 seconds.
2025-04-24 08:13:08,523 - INFO - Loading prompts from SEvenLLM saved dataset directory: /workspace/calibration_data/sevenllm_instruct_subset_manual/
2025-04-24 08:13:08,579 - INFO - Loaded dataset: Dataset({
    features: ['output', 'category', 'thought', 'input', 'id', 'instruction'],
    num_rows: 200
})
2025-04-24 08:13:08,582 - INFO - Selected first 100 samples for evaluation.

Parsing SEvenLLM prompts:   0%|          | 0/100 [00:00<?, ?it/s]
Parsing SEvenLLM prompts: 100%|██████████| 100/100 [00:00<00:00, 4526.60it/s]
2025-04-24 08:13:08,605 - INFO - Successfully loaded 100 prompts from sevenllm_bench.
2025-04-24 08:13:08,605 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 08:13:08,606 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]
Inference Progress:   1%|          | 1/100 [00:24<40:52, 24.78s/it]
Inference Progress:   2%|▏         | 2/100 [00:33<25:15, 15.47s/it]
Inference Progress:   3%|▎         | 3/100 [00:51<26:59, 16.69s/it]
Inference Progress:   4%|▍         | 4/100 [01:14<30:10, 18.86s/it]
Inference Progress:   5%|▌         | 5/100 [01:21<23:06, 14.60s/it]
Inference Progress:   6%|▌         | 6/100 [01:43<26:54, 17.18s/it]
Inference Progress:   7%|▋         | 7/100 [01:52<22:25, 14.46s/it]
Inference Progress:   8%|▊         | 8/100 [02:01<19:47, 12.91s/it]
Inference Progress:   9%|▉         | 9/100 [02:07<15:58, 10.53s/it]
Inference Progress:  10%|█         | 10/100 [02:29<21:12, 14.14s/it]
Inference Progress:  11%|█         | 11/100 [02:44<21:30, 14.50s/it]
Inference Progress:  12%|█▏        | 12/100 [03:06<24:41, 16.83s/it]
Inference Progress:  13%|█▎        | 13/100 [03:12<19:32, 13.47s/it]
Inference Progress:  14%|█▍        | 14/100 [03:28<20:20, 14.19s/it]
Inference Progress:  15%|█▌        | 15/100 [03:39<18:35, 13.12s/it]
Inference Progress:  16%|█▌        | 16/100 [03:52<18:33, 13.26s/it]
Inference Progress:  17%|█▋        | 17/100 [04:14<22:01, 15.93s/it]
Inference Progress:  18%|█▊        | 18/100 [04:36<24:18, 17.79s/it]
Inference Progress:  19%|█▉        | 19/100 [04:47<20:56, 15.51s/it]
Inference Progress:  20%|██        | 20/100 [05:09<23:19, 17.50s/it]
Inference Progress:  21%|██        | 21/100 [05:18<19:38, 14.91s/it]
Inference Progress:  22%|██▏       | 22/100 [05:40<22:12, 17.09s/it]
Inference Progress:  23%|██▎       | 23/100 [05:48<18:27, 14.38s/it]
Inference Progress:  24%|██▍       | 24/100 [05:57<16:24, 12.95s/it]
Inference Progress:  25%|██▌       | 25/100 [06:02<13:12, 10.57s/it]
Inference Progress:  26%|██▌       | 26/100 [06:23<16:35, 13.45s/it]
Inference Progress:  27%|██▋       | 27/100 [06:32<15:02, 12.36s/it]
Inference Progress:  28%|██▊       | 28/100 [06:40<13:15, 11.05s/it]
Inference Progress:  29%|██▉       | 29/100 [07:03<17:00, 14.38s/it]
Inference Progress:  30%|███       | 30/100 [07:10<14:26, 12.37s/it]
Inference Progress:  31%|███       | 31/100 [07:32<17:36, 15.30s/it]
Inference Progress:  32%|███▏      | 32/100 [07:40<14:48, 13.07s/it]
Inference Progress:  33%|███▎      | 33/100 [07:50<13:34, 12.15s/it]
Inference Progress:  34%|███▍      | 34/100 [08:02<13:04, 11.89s/it]
Inference Progress:  35%|███▌      | 35/100 [08:23<15:56, 14.72s/it]
Inference Progress:  36%|███▌      | 36/100 [08:44<17:40, 16.57s/it]
Inference Progress:  37%|███▋      | 37/100 [09:01<17:38, 16.80s/it]
Inference Progress:  38%|███▊      | 38/100 [09:08<14:13, 13.76s/it]
Inference Progress:  39%|███▉      | 39/100 [09:30<16:32, 16.27s/it]
Inference Progress:  40%|████      | 40/100 [09:41<14:36, 14.60s/it]
Inference Progress:  41%|████      | 41/100 [09:49<12:33, 12.77s/it]
Inference Progress:  42%|████▏     | 42/100 [09:54<10:04, 10.43s/it]
Inference Progress:  43%|████▎     | 43/100 [10:16<13:14, 13.94s/it]
Inference Progress:  44%|████▍     | 44/100 [10:38<15:19, 16.41s/it]
Inference Progress:  45%|████▌     | 45/100 [10:48<13:18, 14.51s/it]
Inference Progress:  46%|████▌     | 46/100 [11:11<15:08, 16.82s/it]
Inference Progress:  47%|████▋     | 47/100 [11:23<13:42, 15.52s/it]
Inference Progress:  48%|████▊     | 48/100 [11:45<15:10, 17.51s/it]
Inference Progress:  49%|████▉     | 49/100 [11:59<13:52, 16.33s/it]
Inference Progress:  50%|█████     | 50/100 [12:21<15:03, 18.07s/it]
Inference Progress:  51%|█████     | 51/100 [12:43<15:45, 19.29s/it]
Inference Progress:  52%|█████▏    | 52/100 [13:05<16:06, 20.15s/it]
Inference Progress:  53%|█████▎    | 53/100 [13:28<16:17, 20.81s/it]
Inference Progress:  54%|█████▍    | 54/100 [13:50<16:16, 21.22s/it]
Inference Progress:  55%|█████▌    | 55/100 [13:55<12:24, 16.54s/it]
Inference Progress:  56%|█████▌    | 56/100 [14:00<09:24, 12.83s/it]
Inference Progress:  57%|█████▋    | 57/100 [14:06<07:47, 10.88s/it]
Inference Progress:  58%|█████▊    | 58/100 [14:28<10:02, 14.35s/it]
Inference Progress:  59%|█████▉    | 59/100 [14:51<11:27, 16.76s/it]
Inference Progress:  60%|██████    | 60/100 [15:00<09:46, 14.65s/it]
Inference Progress:  61%|██████    | 61/100 [15:11<08:43, 13.42s/it]
Inference Progress:  62%|██████▏   | 62/100 [15:33<10:11, 16.10s/it]
Inference Progress:  63%|██████▎   | 63/100 [15:56<11:05, 17.98s/it]
Inference Progress:  64%|██████▍   | 64/100 [16:09<10:00, 16.69s/it]
Inference Progress:  65%|██████▌   | 65/100 [16:24<09:25, 16.15s/it]
Inference Progress:  66%|██████▌   | 66/100 [16:33<07:56, 14.02s/it]
Inference Progress:  67%|██████▋   | 67/100 [16:56<09:03, 16.46s/it]
Inference Progress:  68%|██████▊   | 68/100 [17:07<08:02, 15.08s/it]
Inference Progress:  69%|██████▉   | 69/100 [17:10<05:54, 11.42s/it]
Inference Progress:  70%|███████   | 70/100 [17:18<05:05, 10.18s/it]
Inference Progress:  71%|███████   | 71/100 [17:40<06:40, 13.82s/it]
Inference Progress:  72%|███████▏  | 72/100 [17:52<06:13, 13.34s/it]
Inference Progress:  73%|███████▎  | 73/100 [18:04<05:45, 12.78s/it]
Inference Progress:  74%|███████▍  | 74/100 [18:13<05:06, 11.80s/it]
Inference Progress:  75%|███████▌  | 75/100 [18:23<04:39, 11.19s/it]
Inference Progress:  76%|███████▌  | 76/100 [18:36<04:40, 11.71s/it]
Inference Progress:  77%|███████▋  | 77/100 [18:48<04:35, 12.00s/it]
Inference Progress:  78%|███████▊  | 78/100 [19:11<05:30, 15.04s/it]
Inference Progress:  79%|███████▉  | 79/100 [19:25<05:09, 14.72s/it]
Inference Progress:  80%|████████  | 80/100 [19:42<05:07, 15.40s/it]
Inference Progress:  81%|████████  | 81/100 [19:55<04:44, 14.96s/it]
Inference Progress:  82%|████████▏ | 82/100 [20:03<03:48, 12.68s/it]
Inference Progress:  83%|████████▎ | 83/100 [20:10<03:05, 10.93s/it]
Inference Progress:  84%|████████▍ | 84/100 [20:20<02:52, 10.79s/it]
Inference Progress:  85%|████████▌ | 85/100 [20:42<03:33, 14.25s/it]
Inference Progress:  86%|████████▌ | 86/100 [21:05<03:52, 16.62s/it]
Inference Progress:  87%|████████▋ | 87/100 [21:27<03:58, 18.33s/it]
Inference Progress:  88%|████████▊ | 88/100 [21:35<03:03, 15.28s/it]
Inference Progress:  89%|████████▉ | 89/100 [21:43<02:24, 13.16s/it]
Inference Progress:  90%|█████████ | 90/100 [21:54<02:04, 12.40s/it]
Inference Progress:  91%|█████████ | 91/100 [22:08<01:54, 12.77s/it]
Inference Progress:  92%|█████████▏| 92/100 [22:17<01:35, 11.90s/it]
Inference Progress:  93%|█████████▎| 93/100 [22:36<01:37, 13.97s/it]
Inference Progress:  94%|█████████▍| 94/100 [22:48<01:19, 13.30s/it]
Inference Progress:  95%|█████████▌| 95/100 [22:57<00:59, 11.89s/it]
Inference Progress:  96%|█████████▌| 96/100 [23:15<00:55, 13.98s/it]
Inference Progress:  97%|█████████▋| 97/100 [23:26<00:38, 12.96s/it]
Inference Progress:  98%|█████████▊| 98/100 [23:48<00:31, 15.76s/it]
Inference Progress:  99%|█████████▉| 99/100 [24:02<00:15, 15.24s/it]
Inference Progress: 100%|██████████| 100/100 [24:15<00:00, 14.37s/it]
Inference Progress: 100%|██████████| 100/100 [24:15<00:00, 14.55s/it]
2025-04-24 08:37:23,747 - INFO - 
--- Inference Summary ---
2025-04-24 08:37:23,747 - INFO - Processed 100 samples.
2025-04-24 08:37:23,747 - INFO - Total 'generate' time (sum): 1454.93 sec
2025-04-24 08:37:23,747 - INFO - Overall inference loop duration: 1455.14 sec
2025-04-24 08:37:23,747 - INFO - Average 'generate' time per sample: 14.5493 sec
2025-04-24 08:37:23,747 - INFO - Total effective tokens generated: 33419
2025-04-24 08:37:23,747 - INFO - Overall effective tokens per second: 22.97
2025-04-24 08:37:23,748 - INFO - RAM Delta during inference: 520.50 MB
2025-04-24 08:37:23,748 - INFO - PyTorch VRAM Peak Delta during inference: 112.71 MB
2025-04-24 08:37:23,748 - INFO - System VRAM Peak Approx. Delta during inference: -1605.38 MB
2025-04-24 08:37:23,766 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-081254_outputs.json
2025-04-24 08:37:23,770 - INFO - Saved metrics to: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-081254_metrics.json
2025-04-24 08:37:23,770 - INFO - Cleaning up resources...
2025-04-24 08:37:23,774 - INFO - CUDA cache cleared.
2025-04-24 08:37:23,775 - INFO - NVML shut down.
2025-04-24 08:37:23,775 - INFO - --- Evaluation Complete ---

2025-04-24 08:37:26,156 - INFO - SUCCESS: Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 08:37:26,157 - INFO - 
Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 08:37:26,158 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 08:41:32,270 - WARNING - Stderr:
2025-04-24 08:38:12,153 - INFO - Initialized NVML for device 0.
2025-04-24 08:38:12,314 - INFO - --- Starting Evaluation ---
2025-04-24 08:38:12,314 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq'}
2025-04-24 08:38:12,314 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 08:38:12,314 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-083812_metrics.json
2025-04-24 08:38:12,314 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-083812_outputs.json
2025-04-24 08:38:12,314 - INFO - NVML Reporting Active: True
2025-04-24 08:38:12,314 - INFO - Initial RAM usage: 662.06 MB
2025-04-24 08:38:12,314 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 08:38:12,314 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 08:38:12,321 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 08:38:12,642 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 08:38:17,877 - WARNING - CUDA extension not installed.
2025-04-24 08:38:17,884 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 08:38:25,554 - INFO - CausalLM Model loaded.
2025-04-24 08:38:25,555 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 08:38:25,556 - INFO - Model loaded in 13.24 seconds.
2025-04-24 08:38:25,556 - INFO - Loading prompts from CTIBench subset: cti-mcq
2025-04-24 08:38:26,687 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 08:38:26,695 - INFO - Selected first 100 samples.
2025-04-24 08:38:26,695 - INFO - Using columns for cti-mcq: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-mcq):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-mcq): 100%|██████████| 100/100 [00:00<00:00, 3278.41it/s]
2025-04-24 08:38:26,727 - INFO - Successfully loaded 100 prompts from ctibench subset cti-mcq.
2025-04-24 08:38:26,727 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 08:38:26,729 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]
Inference Progress:   1%|          | 1/100 [00:24<41:09, 24.94s/it]
Inference Progress:   2%|▏         | 2/100 [00:26<18:07, 11.10s/it]
Inference Progress:   3%|▎         | 3/100 [00:27<10:36,  6.57s/it]
Inference Progress:   4%|▍         | 4/100 [00:28<07:17,  4.55s/it]
Inference Progress:   5%|▌         | 5/100 [00:30<05:31,  3.49s/it]
Inference Progress:   6%|▌         | 6/100 [00:33<05:14,  3.35s/it]
Inference Progress:   7%|▋         | 7/100 [00:34<03:56,  2.54s/it]
Inference Progress:   8%|▊         | 8/100 [00:44<07:30,  4.90s/it]
Inference Progress:   9%|▉         | 9/100 [00:46<05:50,  3.85s/it]
Inference Progress:  10%|█         | 10/100 [00:47<04:33,  3.04s/it]
Inference Progress:  11%|█         | 11/100 [00:48<03:38,  2.45s/it]
Inference Progress:  12%|█▏        | 12/100 [00:49<02:57,  2.02s/it]
Inference Progress:  13%|█▎        | 13/100 [00:50<02:37,  1.82s/it]
Inference Progress:  14%|█▍        | 14/100 [00:51<02:17,  1.59s/it]
Inference Progress:  15%|█▌        | 15/100 [00:53<02:17,  1.62s/it]
Inference Progress:  16%|█▌        | 16/100 [00:55<02:22,  1.69s/it]
Inference Progress:  17%|█▋        | 17/100 [00:56<02:00,  1.46s/it]
Inference Progress:  18%|█▊        | 18/100 [01:00<03:03,  2.24s/it]
Inference Progress:  19%|█▉        | 19/100 [01:02<03:10,  2.36s/it]
Inference Progress:  20%|██        | 20/100 [01:04<02:42,  2.03s/it]
Inference Progress:  21%|██        | 21/100 [01:05<02:27,  1.87s/it]
Inference Progress:  22%|██▏       | 22/100 [01:07<02:23,  1.84s/it]
Inference Progress:  23%|██▎       | 23/100 [01:09<02:19,  1.81s/it]
Inference Progress:  24%|██▍       | 24/100 [01:09<01:51,  1.47s/it]
Inference Progress:  25%|██▌       | 25/100 [01:10<01:33,  1.25s/it]
Inference Progress:  26%|██▌       | 26/100 [01:12<01:40,  1.36s/it]
Inference Progress:  27%|██▋       | 27/100 [01:14<01:52,  1.55s/it]
Inference Progress:  28%|██▊       | 28/100 [01:16<02:00,  1.68s/it]
Inference Progress:  29%|██▉       | 29/100 [01:17<01:59,  1.68s/it]
Inference Progress:  30%|███       | 30/100 [01:19<01:49,  1.56s/it]
Inference Progress:  31%|███       | 31/100 [01:20<01:32,  1.34s/it]
Inference Progress:  32%|███▏      | 32/100 [01:22<01:46,  1.56s/it]
Inference Progress:  33%|███▎      | 33/100 [01:23<01:45,  1.57s/it]
Inference Progress:  34%|███▍      | 34/100 [01:24<01:33,  1.41s/it]
Inference Progress:  35%|███▌      | 35/100 [01:26<01:35,  1.47s/it]
Inference Progress:  36%|███▌      | 36/100 [01:27<01:33,  1.46s/it]
Inference Progress:  37%|███▋      | 37/100 [01:29<01:41,  1.62s/it]
Inference Progress:  38%|███▊      | 38/100 [01:31<01:41,  1.64s/it]
Inference Progress:  39%|███▉      | 39/100 [01:32<01:21,  1.33s/it]
Inference Progress:  40%|████      | 40/100 [01:32<01:08,  1.14s/it]
Inference Progress:  41%|████      | 41/100 [01:34<01:09,  1.19s/it]
Inference Progress:  42%|████▏     | 42/100 [01:35<01:08,  1.18s/it]
Inference Progress:  43%|████▎     | 43/100 [01:36<01:14,  1.31s/it]
Inference Progress:  44%|████▍     | 44/100 [01:38<01:13,  1.32s/it]
Inference Progress:  45%|████▌     | 45/100 [01:39<01:14,  1.36s/it]
Inference Progress:  46%|████▌     | 46/100 [01:41<01:26,  1.60s/it]
Inference Progress:  47%|████▋     | 47/100 [01:43<01:18,  1.48s/it]
Inference Progress:  48%|████▊     | 48/100 [01:44<01:12,  1.40s/it]
Inference Progress:  49%|████▉     | 49/100 [01:45<01:09,  1.36s/it]
Inference Progress:  50%|█████     | 50/100 [01:46<01:08,  1.38s/it]
Inference Progress:  51%|█████     | 51/100 [01:48<01:16,  1.56s/it]
Inference Progress:  52%|█████▏    | 52/100 [01:49<01:01,  1.28s/it]
Inference Progress:  53%|█████▎    | 53/100 [01:51<01:08,  1.46s/it]
Inference Progress:  54%|█████▍    | 54/100 [01:52<00:59,  1.30s/it]
Inference Progress:  55%|█████▌    | 55/100 [01:54<01:09,  1.54s/it]
Inference Progress:  56%|█████▌    | 56/100 [01:56<01:08,  1.55s/it]
Inference Progress:  57%|█████▋    | 57/100 [01:57<01:03,  1.49s/it]
Inference Progress:  58%|█████▊    | 58/100 [01:58<01:00,  1.43s/it]
Inference Progress:  59%|█████▉    | 59/100 [01:59<00:57,  1.40s/it]
Inference Progress:  60%|██████    | 60/100 [02:02<01:14,  1.87s/it]
Inference Progress:  61%|██████    | 61/100 [02:04<01:11,  1.83s/it]
Inference Progress:  62%|██████▏   | 62/100 [02:05<00:59,  1.55s/it]
Inference Progress:  63%|██████▎   | 63/100 [02:06<00:55,  1.49s/it]
Inference Progress:  64%|██████▍   | 64/100 [02:08<00:51,  1.43s/it]
Inference Progress:  65%|██████▌   | 65/100 [02:10<01:00,  1.73s/it]
Inference Progress:  66%|██████▌   | 66/100 [02:11<00:54,  1.61s/it]
Inference Progress:  67%|██████▋   | 67/100 [02:13<00:48,  1.46s/it]
Inference Progress:  68%|██████▊   | 68/100 [02:14<00:49,  1.54s/it]
Inference Progress:  69%|██████▉   | 69/100 [02:15<00:40,  1.31s/it]
Inference Progress:  70%|███████   | 70/100 [02:17<00:41,  1.39s/it]
Inference Progress:  71%|███████   | 71/100 [02:18<00:42,  1.48s/it]
Inference Progress:  72%|███████▏  | 72/100 [02:20<00:39,  1.40s/it]
Inference Progress:  73%|███████▎  | 73/100 [02:22<00:42,  1.57s/it]
Inference Progress:  74%|███████▍  | 74/100 [02:23<00:39,  1.52s/it]
Inference Progress:  75%|███████▌  | 75/100 [02:25<00:42,  1.68s/it]
Inference Progress:  76%|███████▌  | 76/100 [02:27<00:45,  1.88s/it]
Inference Progress:  77%|███████▋  | 77/100 [02:29<00:38,  1.67s/it]
Inference Progress:  78%|███████▊  | 78/100 [02:31<00:39,  1.79s/it]
Inference Progress:  79%|███████▉  | 79/100 [02:32<00:32,  1.56s/it]
Inference Progress:  80%|████████  | 80/100 [02:33<00:32,  1.61s/it]
Inference Progress:  81%|████████  | 81/100 [02:34<00:25,  1.34s/it]
Inference Progress:  82%|████████▏ | 82/100 [02:35<00:22,  1.26s/it]
Inference Progress:  83%|████████▎ | 83/100 [02:36<00:18,  1.10s/it]
Inference Progress:  84%|████████▍ | 84/100 [02:37<00:19,  1.20s/it]
Inference Progress:  85%|████████▌ | 85/100 [02:40<00:23,  1.57s/it]
Inference Progress:  86%|████████▌ | 86/100 [02:41<00:21,  1.51s/it]
Inference Progress:  87%|████████▋ | 87/100 [02:43<00:19,  1.49s/it]
Inference Progress:  88%|████████▊ | 88/100 [02:44<00:17,  1.47s/it]
Inference Progress:  89%|████████▉ | 89/100 [02:46<00:17,  1.61s/it]
Inference Progress:  90%|█████████ | 90/100 [02:47<00:15,  1.53s/it]
Inference Progress:  91%|█████████ | 91/100 [02:48<00:12,  1.43s/it]
Inference Progress:  92%|█████████▏| 92/100 [02:50<00:11,  1.39s/it]
Inference Progress:  93%|█████████▎| 93/100 [02:51<00:09,  1.42s/it]
Inference Progress:  94%|█████████▍| 94/100 [02:55<00:13,  2.26s/it]
Inference Progress:  95%|█████████▌| 95/100 [02:57<00:10,  2.02s/it]
Inference Progress:  96%|█████████▌| 96/100 [02:58<00:07,  1.75s/it]
Inference Progress:  97%|█████████▋| 97/100 [02:59<00:04,  1.51s/it]
Inference Progress:  98%|█████████▊| 98/100 [03:00<00:02,  1.42s/it]
Inference Progress:  99%|█████████▉| 99/100 [03:02<00:01,  1.49s/it]
Inference Progress: 100%|██████████| 100/100 [03:03<00:00,  1.43s/it]
Inference Progress: 100%|██████████| 100/100 [03:03<00:00,  1.84s/it]
2025-04-24 08:41:30,365 - INFO - 
--- Inference Summary ---
2025-04-24 08:41:30,365 - INFO - Processed 100 samples.
2025-04-24 08:41:30,365 - INFO - Total 'generate' time (sum): 183.49 sec
2025-04-24 08:41:30,365 - INFO - Overall inference loop duration: 183.63 sec
2025-04-24 08:41:30,365 - INFO - Average 'generate' time per sample: 1.8349 sec
2025-04-24 08:41:30,365 - INFO - Total effective tokens generated: 4082
2025-04-24 08:41:30,365 - INFO - Overall effective tokens per second: 22.25
2025-04-24 08:41:30,365 - INFO - RAM Delta during inference: 451.50 MB
2025-04-24 08:41:30,365 - INFO - PyTorch VRAM Peak Delta during inference: 77.91 MB
2025-04-24 08:41:30,365 - INFO - System VRAM Peak Approx. Delta during inference: -1605.38 MB
2025-04-24 08:41:30,389 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-083812_outputs.json
2025-04-24 08:41:30,393 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-083812_metrics.json
2025-04-24 08:41:30,393 - INFO - Cleaning up resources...
2025-04-24 08:41:30,415 - INFO - CUDA cache cleared.
2025-04-24 08:41:30,415 - INFO - NVML shut down.
2025-04-24 08:41:30,415 - INFO - --- Evaluation Complete ---

2025-04-24 08:41:32,272 - INFO - SUCCESS: Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 08:41:32,273 - INFO - 
Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 08:41:32,274 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 09:18:37,869 - WARNING - Stderr:
2025-04-24 08:41:59,081 - INFO - Initialized NVML for device 0.
2025-04-24 08:41:59,229 - INFO - --- Starting Evaluation ---
2025-04-24 08:41:59,229 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-vsp'}
2025-04-24 08:41:59,229 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 08:41:59,229 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-084159_metrics.json
2025-04-24 08:41:59,229 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-084159_outputs.json
2025-04-24 08:41:59,229 - INFO - NVML Reporting Active: True
2025-04-24 08:41:59,229 - INFO - Initial RAM usage: 663.34 MB
2025-04-24 08:41:59,230 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 08:41:59,230 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 08:41:59,234 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 08:41:59,373 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 08:42:03,400 - WARNING - CUDA extension not installed.
2025-04-24 08:42:03,417 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 08:42:05,494 - INFO - CausalLM Model loaded.
2025-04-24 08:42:05,495 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 08:42:05,495 - INFO - Model loaded in 6.27 seconds.
2025-04-24 08:42:05,496 - INFO - Loading prompts from CTIBench subset: cti-vsp
2025-04-24 08:42:07,006 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 08:42:07,010 - INFO - Selected first 100 samples.
2025-04-24 08:42:07,010 - INFO - Using columns for cti-vsp: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-vsp):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-vsp): 100%|██████████| 100/100 [00:00<00:00, 7398.80it/s]
2025-04-24 08:42:07,025 - INFO - Successfully loaded 100 prompts from ctibench subset cti-vsp.
2025-04-24 08:42:07,026 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 08:42:07,027 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]
Inference Progress:   1%|          | 1/100 [00:23<38:02, 23.05s/it]
Inference Progress:   2%|▏         | 2/100 [00:44<36:35, 22.40s/it]
Inference Progress:   3%|▎         | 3/100 [01:07<36:05, 22.33s/it]
Inference Progress:   4%|▍         | 4/100 [01:29<35:39, 22.28s/it]
Inference Progress:   5%|▌         | 5/100 [01:50<34:17, 21.66s/it]
Inference Progress:   6%|▌         | 6/100 [02:12<34:14, 21.86s/it]
Inference Progress:   7%|▋         | 7/100 [02:24<29:09, 18.81s/it]
Inference Progress:   8%|▊         | 8/100 [02:47<30:36, 19.96s/it]
Inference Progress:   9%|▉         | 9/100 [03:08<30:52, 20.36s/it]
Inference Progress:  10%|█         | 10/100 [03:30<31:24, 20.93s/it]
Inference Progress:  11%|█         | 11/100 [03:52<31:39, 21.34s/it]
Inference Progress:  12%|█▏        | 12/100 [04:15<31:41, 21.61s/it]
Inference Progress:  13%|█▎        | 13/100 [04:37<31:35, 21.79s/it]
Inference Progress:  14%|█▍        | 14/100 [04:59<31:27, 21.94s/it]
Inference Progress:  15%|█▌        | 15/100 [05:21<31:12, 22.03s/it]
Inference Progress:  16%|█▌        | 16/100 [05:44<30:55, 22.09s/it]
Inference Progress:  17%|█▋        | 17/100 [06:03<29:23, 21.24s/it]
Inference Progress:  18%|█▊        | 18/100 [06:25<29:28, 21.57s/it]
Inference Progress:  19%|█▉        | 19/100 [06:47<29:23, 21.77s/it]
Inference Progress:  20%|██        | 20/100 [07:10<29:12, 21.91s/it]
Inference Progress:  21%|██        | 21/100 [07:32<28:58, 22.01s/it]
Inference Progress:  22%|██▏       | 22/100 [07:54<28:42, 22.08s/it]
Inference Progress:  23%|██▎       | 23/100 [08:16<28:24, 22.14s/it]
Inference Progress:  24%|██▍       | 24/100 [08:39<28:04, 22.17s/it]
Inference Progress:  25%|██▌       | 25/100 [09:01<27:49, 22.26s/it]
Inference Progress:  26%|██▌       | 26/100 [09:23<27:26, 22.25s/it]
Inference Progress:  27%|██▋       | 27/100 [09:46<27:04, 22.25s/it]
Inference Progress:  28%|██▊       | 28/100 [10:08<26:42, 22.25s/it]
Inference Progress:  29%|██▉       | 29/100 [10:30<26:19, 22.25s/it]
Inference Progress:  30%|███       | 30/100 [10:52<25:57, 22.26s/it]
Inference Progress:  31%|███       | 31/100 [11:15<25:36, 22.27s/it]
Inference Progress:  32%|███▏      | 32/100 [11:37<25:08, 22.19s/it]
Inference Progress:  33%|███▎      | 33/100 [11:58<24:29, 21.93s/it]
Inference Progress:  34%|███▍      | 34/100 [12:20<24:13, 22.02s/it]
Inference Progress:  35%|███▌      | 35/100 [12:43<23:55, 22.09s/it]
Inference Progress:  36%|███▌      | 36/100 [13:05<23:36, 22.13s/it]
Inference Progress:  37%|███▋      | 37/100 [13:27<23:16, 22.16s/it]
Inference Progress:  38%|███▊      | 38/100 [13:49<22:55, 22.18s/it]
Inference Progress:  39%|███▉      | 39/100 [14:11<22:34, 22.20s/it]
Inference Progress:  40%|████      | 40/100 [14:34<22:12, 22.21s/it]
Inference Progress:  41%|████      | 41/100 [14:56<21:51, 22.22s/it]
Inference Progress:  42%|████▏     | 42/100 [15:18<21:29, 22.23s/it]
Inference Progress:  43%|████▎     | 43/100 [15:41<21:10, 22.29s/it]
Inference Progress:  44%|████▍     | 44/100 [16:03<20:47, 22.28s/it]
Inference Progress:  45%|████▌     | 45/100 [16:25<20:25, 22.27s/it]
Inference Progress:  46%|████▌     | 46/100 [16:47<20:02, 22.27s/it]
Inference Progress:  47%|████▋     | 47/100 [17:10<19:40, 22.27s/it]
Inference Progress:  48%|████▊     | 48/100 [17:32<19:17, 22.27s/it]
Inference Progress:  49%|████▉     | 49/100 [17:54<18:55, 22.26s/it]
Inference Progress:  50%|█████     | 50/100 [18:16<18:33, 22.27s/it]
Inference Progress:  51%|█████     | 51/100 [18:39<18:11, 22.27s/it]
Inference Progress:  52%|█████▏    | 52/100 [19:01<17:48, 22.26s/it]
Inference Progress:  53%|█████▎    | 53/100 [19:23<17:25, 22.25s/it]
Inference Progress:  54%|█████▍    | 54/100 [19:45<16:59, 22.15s/it]
Inference Progress:  55%|█████▌    | 55/100 [20:07<16:38, 22.19s/it]
Inference Progress:  56%|█████▌    | 56/100 [20:30<16:16, 22.20s/it]
Inference Progress:  57%|█████▋    | 57/100 [20:52<15:55, 22.22s/it]
Inference Progress:  58%|█████▊    | 58/100 [21:14<15:33, 22.22s/it]
Inference Progress:  59%|█████▉    | 59/100 [21:36<15:11, 22.23s/it]
Inference Progress:  60%|██████    | 60/100 [21:59<14:49, 22.24s/it]
Inference Progress:  61%|██████    | 61/100 [22:15<13:18, 20.48s/it]
Inference Progress:  62%|██████▏   | 62/100 [22:37<13:18, 21.01s/it]
Inference Progress:  63%|██████▎   | 63/100 [22:57<12:46, 20.71s/it]
Inference Progress:  64%|██████▍   | 64/100 [23:20<12:45, 21.26s/it]
Inference Progress:  65%|██████▌   | 65/100 [23:42<12:34, 21.57s/it]
Inference Progress:  66%|██████▌   | 66/100 [24:04<12:21, 21.80s/it]
Inference Progress:  67%|██████▋   | 67/100 [24:27<12:03, 21.93s/it]
Inference Progress:  68%|██████▊   | 68/100 [24:49<11:45, 22.04s/it]
Inference Progress:  69%|██████▉   | 69/100 [25:11<11:25, 22.10s/it]
Inference Progress:  70%|███████   | 70/100 [25:33<11:04, 22.15s/it]
Inference Progress:  71%|███████   | 71/100 [25:56<10:43, 22.18s/it]
Inference Progress:  72%|███████▏  | 72/100 [26:18<10:21, 22.21s/it]
Inference Progress:  73%|███████▎  | 73/100 [26:38<09:45, 21.67s/it]
Inference Progress:  74%|███████▍  | 74/100 [27:01<09:27, 21.84s/it]
Inference Progress:  75%|███████▌  | 75/100 [27:21<08:52, 21.30s/it]
Inference Progress:  76%|███████▌  | 76/100 [27:43<08:37, 21.57s/it]
Inference Progress:  77%|███████▋  | 77/100 [28:05<08:20, 21.78s/it]
Inference Progress:  78%|███████▊  | 78/100 [28:28<08:02, 21.95s/it]
Inference Progress:  79%|███████▉  | 79/100 [28:50<07:42, 22.04s/it]
Inference Progress:  80%|████████  | 80/100 [29:10<07:12, 21.65s/it]
Inference Progress:  81%|████████  | 81/100 [29:33<06:54, 21.83s/it]
Inference Progress:  82%|████████▏ | 82/100 [29:55<06:35, 21.96s/it]
Inference Progress:  83%|████████▎ | 83/100 [30:17<06:14, 22.05s/it]
Inference Progress:  84%|████████▍ | 84/100 [30:39<05:53, 22.11s/it]
Inference Progress:  85%|████████▌ | 85/100 [31:00<05:25, 21.70s/it]
Inference Progress:  86%|████████▌ | 86/100 [31:22<05:06, 21.86s/it]
Inference Progress:  87%|████████▋ | 87/100 [31:45<04:45, 21.98s/it]
Inference Progress:  88%|████████▊ | 88/100 [32:07<04:24, 22.06s/it]
Inference Progress:  89%|████████▉ | 89/100 [32:27<03:55, 21.45s/it]
Inference Progress:  90%|█████████ | 90/100 [32:49<03:36, 21.69s/it]
Inference Progress:  91%|█████████ | 91/100 [33:11<03:16, 21.84s/it]
Inference Progress:  92%|█████████▏| 92/100 [33:32<02:50, 21.30s/it]
Inference Progress:  93%|█████████▎| 93/100 [33:54<02:31, 21.59s/it]
Inference Progress:  94%|█████████▍| 94/100 [34:16<02:10, 21.78s/it]
Inference Progress:  95%|█████████▌| 95/100 [34:38<01:49, 21.92s/it]
Inference Progress:  96%|█████████▌| 96/100 [35:00<01:28, 22.01s/it]
Inference Progress:  97%|█████████▋| 97/100 [35:23<01:06, 22.09s/it]
Inference Progress:  98%|█████████▊| 98/100 [35:45<00:44, 22.13s/it]
Inference Progress:  99%|█████████▉| 99/100 [36:07<00:22, 22.18s/it]
Inference Progress: 100%|██████████| 100/100 [36:30<00:00, 22.20s/it]
Inference Progress: 100%|██████████| 100/100 [36:30<00:00, 21.90s/it]
2025-04-24 09:18:37,034 - INFO - 
--- Inference Summary ---
2025-04-24 09:18:37,035 - INFO - Processed 100 samples.
2025-04-24 09:18:37,035 - INFO - Total 'generate' time (sum): 2189.78 sec
2025-04-24 09:18:37,035 - INFO - Overall inference loop duration: 2190.01 sec
2025-04-24 09:18:37,035 - INFO - Average 'generate' time per sample: 21.8978 sec
2025-04-24 09:18:37,035 - INFO - Total effective tokens generated: 50340
2025-04-24 09:18:37,035 - INFO - Overall effective tokens per second: 22.99
2025-04-24 09:18:37,035 - INFO - RAM Delta during inference: 497.25 MB
2025-04-24 09:18:37,035 - INFO - PyTorch VRAM Peak Delta during inference: 112.71 MB
2025-04-24 09:18:37,035 - INFO - System VRAM Peak Approx. Delta during inference: -1605.38 MB
2025-04-24 09:18:37,047 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-084159_outputs.json
2025-04-24 09:18:37,051 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-084159_metrics.json
2025-04-24 09:18:37,051 - INFO - Cleaning up resources...
2025-04-24 09:18:37,057 - INFO - CUDA cache cleared.
2025-04-24 09:18:37,057 - INFO - NVML shut down.
2025-04-24 09:18:37,057 - INFO - --- Evaluation Complete ---

2025-04-24 09:18:37,872 - INFO - SUCCESS: Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'. Check output files in /workspace/results/mod/48/
2025-04-24 09:18:37,873 - INFO - 
Run 5/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 09:18:37,874 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 09:40:28,486 - WARNING - Stderr:
2025-04-24 09:19:24,110 - INFO - Initialized NVML for device 0.
2025-04-24 09:19:24,258 - INFO - --- Starting Evaluation ---
2025-04-24 09:19:24,258 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-rcm'}
2025-04-24 09:19:24,258 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 09:19:24,258 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-rcm_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-091924_metrics.json
2025-04-24 09:19:24,258 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-rcm_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-091924_outputs.json
2025-04-24 09:19:24,258 - INFO - NVML Reporting Active: True
2025-04-24 09:19:24,258 - INFO - Initial RAM usage: 660.41 MB
2025-04-24 09:19:24,258 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 09:19:24,258 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 09:19:24,266 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 09:19:24,548 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 09:19:29,438 - WARNING - CUDA extension not installed.
2025-04-24 09:19:29,444 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 09:19:37,565 - INFO - CausalLM Model loaded.
2025-04-24 09:19:37,567 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 09:19:37,568 - INFO - Model loaded in 13.31 seconds.
2025-04-24 09:19:37,568 - INFO - Loading prompts from CTIBench subset: cti-rcm
2025-04-24 09:19:38,492 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 09:19:38,496 - INFO - Selected first 100 samples.
2025-04-24 09:19:38,496 - INFO - Using columns for cti-rcm: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-rcm):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-rcm): 100%|██████████| 100/100 [00:00<00:00, 5265.72it/s]
2025-04-24 09:19:38,516 - INFO - Successfully loaded 100 prompts from ctibench subset cti-rcm.
2025-04-24 09:19:38,517 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 09:19:38,518 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]
Inference Progress:   1%|          | 1/100 [00:10<17:15, 10.46s/it]
Inference Progress:   2%|▏         | 2/100 [00:20<16:30, 10.11s/it]
Inference Progress:   3%|▎         | 3/100 [00:42<25:28, 15.76s/it]
Inference Progress:   4%|▍         | 4/100 [00:50<20:12, 12.63s/it]
Inference Progress:   5%|▌         | 5/100 [00:57<16:46, 10.59s/it]
Inference Progress:   6%|▌         | 6/100 [01:06<15:29,  9.89s/it]
Inference Progress:   7%|▋         | 7/100 [01:13<14:12,  9.17s/it]
Inference Progress:   8%|▊         | 8/100 [01:34<19:43, 12.87s/it]
Inference Progress:   9%|▉         | 9/100 [01:42<17:01, 11.23s/it]
Inference Progress:  10%|█         | 10/100 [01:55<17:46, 11.85s/it]
Inference Progress:  11%|█         | 11/100 [02:10<19:02, 12.83s/it]
Inference Progress:  12%|█▏        | 12/100 [02:20<17:28, 11.92s/it]
Inference Progress:  13%|█▎        | 13/100 [02:28<15:26, 10.65s/it]
Inference Progress:  14%|█▍        | 14/100 [02:50<20:15, 14.14s/it]
Inference Progress:  15%|█▌        | 15/100 [02:54<15:34, 11.00s/it]
Inference Progress:  16%|█▌        | 16/100 [03:16<20:06, 14.37s/it]
Inference Progress:  17%|█▋        | 17/100 [03:27<18:40, 13.50s/it]
Inference Progress:  18%|█▊        | 18/100 [03:31<14:33, 10.66s/it]
Inference Progress:  19%|█▉        | 19/100 [03:53<19:03, 14.11s/it]
Inference Progress:  20%|██        | 20/100 [04:04<17:32, 13.16s/it]
Inference Progress:  21%|██        | 21/100 [04:20<18:14, 13.85s/it]
Inference Progress:  22%|██▏       | 22/100 [04:42<21:15, 16.36s/it]
Inference Progress:  23%|██▎       | 23/100 [05:04<23:14, 18.11s/it]
Inference Progress:  24%|██▍       | 24/100 [05:13<19:23, 15.31s/it]
Inference Progress:  25%|██▌       | 25/100 [05:17<15:02, 12.03s/it]
Inference Progress:  26%|██▌       | 26/100 [05:40<18:35, 15.08s/it]
Inference Progress:  27%|██▋       | 27/100 [05:46<15:11, 12.49s/it]
Inference Progress:  28%|██▊       | 28/100 [05:57<14:20, 11.95s/it]
Inference Progress:  29%|██▉       | 29/100 [06:07<13:40, 11.56s/it]
Inference Progress:  30%|███       | 30/100 [06:16<12:27, 10.67s/it]
Inference Progress:  31%|███       | 31/100 [06:26<12:03, 10.49s/it]
Inference Progress:  32%|███▏      | 32/100 [06:38<12:26, 10.98s/it]
Inference Progress:  33%|███▎      | 33/100 [07:00<16:00, 14.34s/it]
Inference Progress:  34%|███▍      | 34/100 [07:08<13:29, 12.27s/it]
Inference Progress:  35%|███▌      | 35/100 [07:17<12:11, 11.26s/it]
Inference Progress:  36%|███▌      | 36/100 [07:25<11:14, 10.54s/it]
Inference Progress:  37%|███▋      | 37/100 [07:41<12:37, 12.03s/it]
Inference Progress:  38%|███▊      | 38/100 [07:49<11:07, 10.77s/it]
Inference Progress:  39%|███▉      | 39/100 [07:57<10:17, 10.12s/it]
Inference Progress:  40%|████      | 40/100 [08:20<13:44, 13.74s/it]
Inference Progress:  41%|████      | 41/100 [08:30<12:24, 12.63s/it]
Inference Progress:  42%|████▏     | 42/100 [08:52<14:58, 15.50s/it]
Inference Progress:  43%|████▎     | 43/100 [09:14<16:39, 17.54s/it]
Inference Progress:  44%|████▍     | 44/100 [09:24<14:13, 15.24s/it]
Inference Progress:  45%|████▌     | 45/100 [09:34<12:34, 13.71s/it]
Inference Progress:  46%|████▌     | 46/100 [09:46<11:55, 13.24s/it]
Inference Progress:  47%|████▋     | 47/100 [09:57<11:02, 12.50s/it]
Inference Progress:  48%|████▊     | 48/100 [10:08<10:18, 11.90s/it]
Inference Progress:  49%|████▉     | 49/100 [10:21<10:28, 12.32s/it]
Inference Progress:  50%|█████     | 50/100 [10:26<08:31, 10.24s/it]
Inference Progress:  51%|█████     | 51/100 [10:48<11:17, 13.82s/it]
Inference Progress:  52%|█████▏    | 52/100 [11:11<13:03, 16.32s/it]
Inference Progress:  53%|█████▎    | 53/100 [11:33<14:09, 18.08s/it]
Inference Progress:  54%|█████▍    | 54/100 [11:45<12:27, 16.25s/it]
Inference Progress:  55%|█████▌    | 55/100 [11:51<09:55, 13.23s/it]
Inference Progress:  56%|█████▌    | 56/100 [11:59<08:28, 11.57s/it]
Inference Progress:  57%|█████▋    | 57/100 [12:08<07:46, 10.86s/it]
Inference Progress:  58%|█████▊    | 58/100 [12:14<06:34,  9.39s/it]
Inference Progress:  59%|█████▉    | 59/100 [12:26<06:57, 10.19s/it]
Inference Progress:  60%|██████    | 60/100 [12:48<09:11, 13.79s/it]
Inference Progress:  61%|██████    | 61/100 [12:58<08:13, 12.66s/it]
Inference Progress:  62%|██████▏   | 62/100 [13:04<06:41, 10.56s/it]
Inference Progress:  63%|██████▎   | 63/100 [13:15<06:40, 10.82s/it]
Inference Progress:  64%|██████▍   | 64/100 [13:38<08:34, 14.29s/it]
Inference Progress:  65%|██████▌   | 65/100 [13:46<07:20, 12.58s/it]
Inference Progress:  66%|██████▌   | 66/100 [13:52<06:03, 10.69s/it]
Inference Progress:  67%|██████▋   | 67/100 [14:07<06:34, 11.94s/it]
Inference Progress:  68%|██████▊   | 68/100 [14:29<08:00, 15.02s/it]
Inference Progress:  69%|██████▉   | 69/100 [14:37<06:31, 12.64s/it]
Inference Progress:  70%|███████   | 70/100 [14:44<05:31, 11.06s/it]
Inference Progress:  71%|███████   | 71/100 [14:57<05:35, 11.58s/it]
Inference Progress:  72%|███████▏  | 72/100 [15:02<04:34,  9.81s/it]
Inference Progress:  73%|███████▎  | 73/100 [15:09<03:59,  8.87s/it]
Inference Progress:  74%|███████▍  | 74/100 [15:16<03:36,  8.33s/it]
Inference Progress:  75%|███████▌  | 75/100 [15:38<05:12, 12.49s/it]
Inference Progress:  76%|███████▌  | 76/100 [15:45<04:20, 10.85s/it]
Inference Progress:  77%|███████▋  | 77/100 [16:08<05:27, 14.25s/it]
Inference Progress:  78%|███████▊  | 78/100 [16:14<04:23, 11.99s/it]
Inference Progress:  79%|███████▉  | 79/100 [16:22<03:45, 10.75s/it]
Inference Progress:  80%|████████  | 80/100 [16:31<03:23, 10.17s/it]
Inference Progress:  81%|████████  | 81/100 [16:39<02:59,  9.45s/it]
Inference Progress:  82%|████████▏ | 82/100 [17:01<03:58, 13.27s/it]
Inference Progress:  83%|████████▎ | 83/100 [17:11<03:31, 12.42s/it]
Inference Progress:  84%|████████▍ | 84/100 [17:24<03:18, 12.39s/it]
Inference Progress:  85%|████████▌ | 85/100 [17:28<02:30, 10.06s/it]
Inference Progress:  86%|████████▌ | 86/100 [17:50<03:11, 13.69s/it]
Inference Progress:  87%|████████▋ | 87/100 [17:56<02:25, 11.17s/it]
Inference Progress:  88%|████████▊ | 88/100 [18:08<02:19, 11.64s/it]
Inference Progress:  89%|████████▉ | 89/100 [18:18<01:59, 10.89s/it]
Inference Progress:  90%|█████████ | 90/100 [18:25<01:37,  9.71s/it]
Inference Progress:  91%|█████████ | 91/100 [18:33<01:23,  9.30s/it]
Inference Progress:  92%|█████████▏| 92/100 [18:55<01:45, 13.17s/it]
Inference Progress:  93%|█████████▎| 93/100 [19:13<01:41, 14.51s/it]
Inference Progress:  94%|█████████▍| 94/100 [19:35<01:40, 16.81s/it]
Inference Progress:  95%|█████████▌| 95/100 [19:57<01:32, 18.42s/it]
Inference Progress:  96%|█████████▌| 96/100 [20:04<01:00, 15.12s/it]
Inference Progress:  97%|█████████▋| 97/100 [20:09<00:35, 11.84s/it]
Inference Progress:  98%|█████████▊| 98/100 [20:21<00:24, 12.13s/it]
Inference Progress:  99%|█████████▉| 99/100 [20:38<00:13, 13.32s/it]
Inference Progress: 100%|██████████| 100/100 [20:47<00:00, 12.04s/it]
Inference Progress: 100%|██████████| 100/100 [20:47<00:00, 12.47s/it]
2025-04-24 09:40:25,653 - INFO - 
--- Inference Summary ---
2025-04-24 09:40:25,653 - INFO - Processed 100 samples.
2025-04-24 09:40:25,653 - INFO - Total 'generate' time (sum): 1246.95 sec
2025-04-24 09:40:25,653 - INFO - Overall inference loop duration: 1247.13 sec
2025-04-24 09:40:25,653 - INFO - Average 'generate' time per sample: 12.4695 sec
2025-04-24 09:40:25,653 - INFO - Total effective tokens generated: 28636
2025-04-24 09:40:25,653 - INFO - Overall effective tokens per second: 22.96
2025-04-24 09:40:25,653 - INFO - RAM Delta during inference: 527.25 MB
2025-04-24 09:40:25,653 - INFO - PyTorch VRAM Peak Delta during inference: 97.89 MB
2025-04-24 09:40:25,653 - INFO - System VRAM Peak Approx. Delta during inference: -1605.38 MB
2025-04-24 09:40:25,663 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-rcm_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-091924_outputs.json
2025-04-24 09:40:25,667 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-rcm_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-091924_metrics.json
2025-04-24 09:40:25,667 - INFO - Cleaning up resources...
2025-04-24 09:40:25,671 - INFO - CUDA cache cleared.
2025-04-24 09:40:25,671 - INFO - NVML shut down.
2025-04-24 09:40:25,671 - INFO - --- Evaluation Complete ---

2025-04-24 09:40:28,489 - INFO - SUCCESS: Run 5/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-rcm'. Check output files in /workspace/results/mod/48/
2025-04-24 09:40:28,491 - INFO - 
Run 6/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 09:40:28,493 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
