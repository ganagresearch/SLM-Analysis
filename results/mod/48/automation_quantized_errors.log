2025-04-24 04:24:50,682 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 04:24:50,682 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 04:24:50,682 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 04:24:50,682 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 04:24:50,682 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 04:24:50,682 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_042450.log
2025-04-24 04:24:50,682 - INFO - Models to run: ['TinyLlama-1.1B-Chat-v1.0-GPTQ', 'Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 04:24:50,682 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['attack', 'exploit_target', 'threat_actor', 'ttps'])]
2025-04-24 04:24:50,682 - INFO - Running 100 samples per benchmark.
2025-04-24 04:24:50,682 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 04:24:50,682 - INFO - 
Run 1/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 04:24:50,682 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 04:31:29,826 - WARNING - Stderr:
2025-04-24 04:24:52,813 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:24:53,121 - INFO - --- Starting Evaluation ---
2025-04-24 04:24:53,121 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 04:24:53,121 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:24:53,121 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre__20250424-042453_metrics.json
2025-04-24 04:24:53,121 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre__20250424-042453_outputs.json
2025-04-24 04:24:53,121 - INFO - NVML Reporting Active: False
2025-04-24 04:24:53,121 - INFO - Initial RAM usage: 555.29 MB
2025-04-24 04:24:53,121 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/...
2025-04-24 04:24:53,142 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:24:53,237 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:24:55,125 - INFO - CausalLM Model loaded.
2025-04-24 04:24:55,125 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:24:55,126 - INFO - Model loaded in 2.00 seconds.
2025-04-24 04:24:55,126 - INFO - Loading prompts from CyberSecEval3 MITRE file: /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json
2025-04-24 04:24:55,135 - INFO - Processing the first 100 items.

Parsing MITRE prompts:   0%|          | 0/100 [00:00<?, ?it/s]2025-04-24 04:24:55,137 - WARNING - Item 5: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,137 - WARNING - Item 8: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,138 - WARNING - Item 14: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,138 - WARNING - Item 21: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,139 - WARNING - Item 25: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,139 - WARNING - Item 27: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,139 - WARNING - Item 30: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,139 - WARNING - Item 32: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,140 - WARNING - Item 35: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,140 - WARNING - Item 36: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,140 - WARNING - Item 48: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,141 - WARNING - Item 49: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,141 - WARNING - Item 52: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,141 - WARNING - Item 58: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,142 - WARNING - Item 63: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,142 - WARNING - Item 72: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,143 - WARNING - Item 81: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,143 - WARNING - Item 86: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:24:55,143 - WARNING - Item 88: Regex failed to find/extract 'prompt' value from mutated_prompt_str.

Parsing MITRE prompts: 100%|██████████| 100/100 [00:00<00:00, 13249.63it/s]
2025-04-24 04:24:55,144 - INFO - Successfully loaded 81 prompts from cyberseceval3_mitre.
2025-04-24 04:24:55,144 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 04:24:55,145 - INFO - Starting inference on 81 samples...

Inference Progress:   0%|          | 0/81 [00:00<?, ?it/s]
Inference Progress:   1%|          | 1/81 [00:05<07:22,  5.53s/it]
Inference Progress:   2%|▏         | 2/81 [00:10<06:57,  5.28s/it]
Inference Progress:   4%|▎         | 3/81 [00:15<06:46,  5.22s/it]
Inference Progress:   5%|▍         | 4/81 [00:19<05:53,  4.59s/it]
Inference Progress:   6%|▌         | 5/81 [00:22<05:12,  4.11s/it]
Inference Progress:   7%|▋         | 6/81 [00:26<04:54,  3.93s/it]
Inference Progress:   9%|▊         | 7/81 [00:31<05:19,  4.31s/it]
Inference Progress:  10%|▉         | 8/81 [00:36<05:33,  4.57s/it]
Inference Progress:  11%|█         | 9/81 [00:41<05:40,  4.73s/it]
Inference Progress:  12%|█▏        | 10/81 [00:46<05:44,  4.85s/it]
Inference Progress:  14%|█▎        | 11/81 [00:50<05:22,  4.61s/it]
Inference Progress:  15%|█▍        | 12/81 [00:55<05:28,  4.76s/it]
Inference Progress:  16%|█▌        | 13/81 [01:00<05:20,  4.71s/it]
Inference Progress:  17%|█▋        | 14/81 [01:05<05:23,  4.83s/it]
Inference Progress:  19%|█▊        | 15/81 [01:10<05:23,  4.91s/it]
Inference Progress:  20%|█▉        | 16/81 [01:15<05:13,  4.82s/it]
Inference Progress:  21%|██        | 17/81 [01:20<05:13,  4.91s/it]
Inference Progress:  22%|██▏       | 18/81 [01:25<05:12,  4.96s/it]
Inference Progress:  23%|██▎       | 19/81 [01:30<05:09,  4.99s/it]
Inference Progress:  25%|██▍       | 20/81 [01:35<05:06,  5.02s/it]
Inference Progress:  26%|██▌       | 21/81 [01:40<05:03,  5.05s/it]
Inference Progress:  27%|██▋       | 22/81 [01:45<04:58,  5.06s/it]
Inference Progress:  28%|██▊       | 23/81 [01:50<04:54,  5.07s/it]
Inference Progress:  30%|██▉       | 24/81 [01:55<04:49,  5.08s/it]
Inference Progress:  31%|███       | 25/81 [02:01<04:44,  5.09s/it]
Inference Progress:  32%|███▏      | 26/81 [02:06<04:39,  5.09s/it]
Inference Progress:  33%|███▎      | 27/81 [02:11<04:34,  5.09s/it]
Inference Progress:  35%|███▍      | 28/81 [02:16<04:29,  5.08s/it]
Inference Progress:  36%|███▌      | 29/81 [02:21<04:24,  5.09s/it]
Inference Progress:  37%|███▋      | 30/81 [02:26<04:19,  5.09s/it]
Inference Progress:  38%|███▊      | 31/81 [02:31<04:14,  5.09s/it]
Inference Progress:  40%|███▉      | 32/81 [02:34<03:42,  4.55s/it]
Inference Progress:  41%|████      | 33/81 [02:39<03:46,  4.71s/it]
Inference Progress:  42%|████▏     | 34/81 [02:45<03:46,  4.83s/it]
Inference Progress:  43%|████▎     | 35/81 [02:50<03:45,  4.91s/it]
Inference Progress:  44%|████▍     | 36/81 [02:55<03:43,  4.97s/it]
Inference Progress:  46%|████▌     | 37/81 [03:00<03:40,  5.01s/it]
Inference Progress:  47%|████▋     | 38/81 [03:04<03:22,  4.70s/it]
Inference Progress:  48%|████▊     | 39/81 [03:09<03:22,  4.82s/it]
Inference Progress:  49%|████▉     | 40/81 [03:14<03:21,  4.91s/it]
Inference Progress:  51%|█████     | 41/81 [03:19<03:18,  4.97s/it]
Inference Progress:  52%|█████▏    | 42/81 [03:24<03:15,  5.01s/it]
Inference Progress:  53%|█████▎    | 43/81 [03:29<03:11,  5.03s/it]
Inference Progress:  54%|█████▍    | 44/81 [03:34<03:06,  5.05s/it]
Inference Progress:  56%|█████▌    | 45/81 [03:40<03:02,  5.07s/it]
Inference Progress:  57%|█████▋    | 46/81 [03:45<02:57,  5.08s/it]
Inference Progress:  58%|█████▊    | 47/81 [03:50<02:52,  5.08s/it]
Inference Progress:  59%|█████▉    | 48/81 [03:55<02:47,  5.09s/it]
Inference Progress:  60%|██████    | 49/81 [03:57<02:13,  4.17s/it]
Inference Progress:  62%|██████▏   | 50/81 [04:02<02:18,  4.45s/it]
Inference Progress:  63%|██████▎   | 51/81 [04:07<02:19,  4.65s/it]
Inference Progress:  64%|██████▍   | 52/81 [04:12<02:18,  4.78s/it]
Inference Progress:  65%|██████▌   | 53/81 [04:17<02:16,  4.88s/it]
Inference Progress:  67%|██████▋   | 54/81 [04:22<02:13,  4.94s/it]
Inference Progress:  68%|██████▊   | 55/81 [04:28<02:09,  4.99s/it]
Inference Progress:  69%|██████▉   | 56/81 [04:33<02:05,  5.02s/it]
Inference Progress:  70%|███████   | 57/81 [04:38<02:01,  5.04s/it]
Inference Progress:  72%|███████▏  | 58/81 [04:42<01:48,  4.72s/it]
Inference Progress:  73%|███████▎  | 59/81 [04:47<01:46,  4.84s/it]
Inference Progress:  74%|███████▍  | 60/81 [04:52<01:43,  4.91s/it]
Inference Progress:  75%|███████▌  | 61/81 [04:57<01:39,  4.97s/it]
Inference Progress:  77%|███████▋  | 62/81 [05:02<01:35,  5.00s/it]
Inference Progress:  78%|███████▊  | 63/81 [05:07<01:30,  5.03s/it]
Inference Progress:  79%|███████▉  | 64/81 [05:12<01:25,  5.05s/it]
Inference Progress:  80%|████████  | 65/81 [05:17<01:21,  5.06s/it]
Inference Progress:  81%|████████▏ | 66/81 [05:21<01:08,  4.55s/it]
Inference Progress:  83%|████████▎ | 67/81 [05:24<01:00,  4.32s/it]
Inference Progress:  84%|████████▍ | 68/81 [05:30<00:59,  4.55s/it]
Inference Progress:  85%|████████▌ | 69/81 [05:35<00:56,  4.71s/it]
Inference Progress:  86%|████████▋ | 70/81 [05:40<00:53,  4.82s/it]
Inference Progress:  88%|████████▊ | 71/81 [05:45<00:48,  4.86s/it]
Inference Progress:  89%|████████▉ | 72/81 [05:50<00:44,  4.92s/it]
Inference Progress:  90%|█████████ | 73/81 [05:55<00:39,  4.96s/it]
Inference Progress:  91%|█████████▏| 74/81 [06:00<00:34,  5.00s/it]
Inference Progress:  93%|█████████▎| 75/81 [06:05<00:30,  5.02s/it]
Inference Progress:  94%|█████████▍| 76/81 [06:10<00:25,  5.05s/it]
Inference Progress:  95%|█████████▌| 77/81 [06:15<00:19,  4.93s/it]
Inference Progress:  96%|█████████▋| 78/81 [06:20<00:14,  4.98s/it]
Inference Progress:  98%|█████████▊| 79/81 [06:25<00:10,  5.01s/it]
Inference Progress:  99%|█████████▉| 80/81 [06:30<00:05,  5.03s/it]
Inference Progress: 100%|██████████| 81/81 [06:33<00:00,  4.56s/it]
Inference Progress: 100%|██████████| 81/81 [06:33<00:00,  4.86s/it]
2025-04-24 04:31:29,068 - INFO - 
--- Inference Summary ---
2025-04-24 04:31:29,068 - INFO - Processed 81 samples.
2025-04-24 04:31:29,068 - INFO - Total 'generate' time (sum): 393.78 sec
2025-04-24 04:31:29,068 - INFO - Overall inference loop duration: 393.92 sec
2025-04-24 04:31:29,068 - INFO - Average 'generate' time per sample: 4.8615 sec
2025-04-24 04:31:29,068 - INFO - Total effective tokens generated: 39520
2025-04-24 04:31:29,068 - INFO - Overall effective tokens per second: 100.36
2025-04-24 04:31:29,068 - INFO - RAM Delta during inference: 550.56 MB
2025-04-24 04:31:29,068 - INFO - PyTorch VRAM Peak Delta during inference: 67.51 MB
2025-04-24 04:31:29,070 - INFO - Saved outputs (81 items) to: /workspace/results/mod/48/cyberseceval3_mitre__20250424-042453_outputs.json
2025-04-24 04:31:29,070 - INFO - Saved metrics to: /workspace/results/mod/48/cyberseceval3_mitre__20250424-042453_metrics.json
2025-04-24 04:31:29,070 - INFO - Cleaning up resources...
2025-04-24 04:31:29,072 - INFO - CUDA cache cleared.
2025-04-24 04:31:29,072 - INFO - --- Evaluation Complete ---

2025-04-24 04:31:29,826 - INFO - SUCCESS: Run 1/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 04:31:29,826 - INFO - 
Run 2/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 04:31:29,826 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 04:37:12,970 - WARNING - Stderr:
2025-04-24 04:31:32,006 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:31:32,340 - INFO - --- Starting Evaluation ---
2025-04-24 04:31:32,340 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 04:31:32,340 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:31:32,340 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench__20250424-043132_metrics.json
2025-04-24 04:31:32,340 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench__20250424-043132_outputs.json
2025-04-24 04:31:32,340 - INFO - NVML Reporting Active: False
2025-04-24 04:31:32,340 - INFO - Initial RAM usage: 553.75 MB
2025-04-24 04:31:32,340 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/...
2025-04-24 04:31:32,361 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:31:32,456 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:31:34,333 - INFO - CausalLM Model loaded.
2025-04-24 04:31:34,334 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:31:34,334 - INFO - Model loaded in 1.99 seconds.
2025-04-24 04:31:34,334 - INFO - Loading prompts from SEvenLLM saved dataset directory: /workspace/calibration_data/sevenllm_instruct_subset_manual/
2025-04-24 04:31:34,337 - INFO - Loaded dataset: Dataset({
    features: ['id', 'output', 'instruction', 'input', 'category', 'thought'],
    num_rows: 200
})
2025-04-24 04:31:34,338 - INFO - Selected first 100 samples for evaluation.

Parsing SEvenLLM prompts:   0%|          | 0/100 [00:00<?, ?it/s]
Parsing SEvenLLM prompts: 100%|██████████| 100/100 [00:00<00:00, 26049.96it/s]
2025-04-24 04:31:34,343 - INFO - Successfully loaded 100 prompts from sevenllm_bench.
2025-04-24 04:31:34,343 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 04:31:34,343 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]
Inference Progress:   1%|          | 1/100 [00:05<09:09,  5.55s/it]
Inference Progress:   2%|▏         | 2/100 [00:07<05:43,  3.51s/it]
Inference Progress:   3%|▎         | 3/100 [00:11<06:09,  3.81s/it]
Inference Progress:   4%|▍         | 4/100 [00:16<06:55,  4.33s/it]
Inference Progress:   5%|▌         | 5/100 [00:18<05:18,  3.35s/it]
Inference Progress:   6%|▌         | 6/100 [00:23<06:11,  3.96s/it]
Inference Progress:   7%|▋         | 7/100 [00:25<05:10,  3.33s/it]
Inference Progress:   8%|▊         | 8/100 [00:30<05:58,  3.90s/it]
Inference Progress:   9%|▉         | 9/100 [00:33<05:23,  3.56s/it]
Inference Progress:  10%|█         | 10/100 [00:38<06:03,  4.03s/it]
Inference Progress:  11%|█         | 11/100 [00:42<05:45,  3.88s/it]
Inference Progress:  12%|█▏        | 12/100 [00:47<06:14,  4.25s/it]
Inference Progress:  13%|█▎        | 13/100 [00:50<05:30,  3.80s/it]
Inference Progress:  14%|█▍        | 14/100 [00:54<05:48,  4.06s/it]
Inference Progress:  15%|█▌        | 15/100 [00:57<05:04,  3.58s/it]
Inference Progress:  16%|█▌        | 16/100 [01:00<04:49,  3.45s/it]
Inference Progress:  17%|█▋        | 17/100 [01:03<04:33,  3.30s/it]
Inference Progress:  18%|█▊        | 18/100 [01:08<05:14,  3.84s/it]
Inference Progress:  19%|█▉        | 19/100 [01:10<04:34,  3.39s/it]
Inference Progress:  20%|██        | 20/100 [01:15<05:11,  3.90s/it]
Inference Progress:  21%|██        | 21/100 [01:17<04:23,  3.34s/it]
Inference Progress:  22%|██▏       | 22/100 [01:23<05:02,  3.88s/it]
Inference Progress:  23%|██▎       | 23/100 [01:24<04:11,  3.27s/it]
Inference Progress:  24%|██▍       | 24/100 [01:27<03:44,  2.95s/it]
Inference Progress:  25%|██▌       | 25/100 [01:28<03:00,  2.41s/it]
Inference Progress:  26%|██▌       | 26/100 [01:32<03:48,  3.09s/it]
Inference Progress:  27%|██▋       | 27/100 [01:35<03:28,  2.85s/it]
Inference Progress:  28%|██▊       | 28/100 [01:37<03:03,  2.55s/it]
Inference Progress:  29%|██▉       | 29/100 [01:42<03:55,  3.32s/it]
Inference Progress:  30%|███       | 30/100 [01:43<03:20,  2.86s/it]
Inference Progress:  31%|███       | 31/100 [01:49<04:04,  3.54s/it]
Inference Progress:  32%|███▏      | 32/100 [01:51<03:46,  3.32s/it]
Inference Progress:  33%|███▎      | 33/100 [01:57<04:18,  3.86s/it]
Inference Progress:  34%|███▍      | 34/100 [01:59<03:50,  3.49s/it]
Inference Progress:  35%|███▌      | 35/100 [02:03<03:47,  3.50s/it]
Inference Progress:  36%|███▌      | 36/100 [02:08<04:09,  3.90s/it]
Inference Progress:  37%|███▋      | 37/100 [02:12<04:07,  3.93s/it]
Inference Progress:  38%|███▊      | 38/100 [02:13<03:19,  3.21s/it]
Inference Progress:  39%|███▉      | 39/100 [02:18<03:50,  3.78s/it]
Inference Progress:  40%|████      | 40/100 [02:21<03:23,  3.39s/it]
Inference Progress:  41%|████      | 41/100 [02:23<02:54,  2.97s/it]
Inference Progress:  42%|████▏     | 42/100 [02:24<02:20,  2.42s/it]
Inference Progress:  43%|████▎     | 43/100 [02:29<03:04,  3.23s/it]
Inference Progress:  44%|████▍     | 44/100 [02:34<03:32,  3.80s/it]
Inference Progress:  45%|████▌     | 45/100 [02:36<03:04,  3.36s/it]
Inference Progress:  46%|████▌     | 46/100 [02:38<02:37,  2.92s/it]
Inference Progress:  47%|████▋     | 47/100 [02:41<02:31,  2.86s/it]
Inference Progress:  48%|████▊     | 48/100 [02:44<02:37,  3.02s/it]
Inference Progress:  49%|████▉     | 49/100 [02:47<02:35,  3.05s/it]
Inference Progress:  50%|█████     | 50/100 [02:53<03:03,  3.67s/it]
Inference Progress:  51%|█████     | 51/100 [02:58<03:20,  4.10s/it]
Inference Progress:  52%|█████▏    | 52/100 [03:03<03:31,  4.40s/it]
Inference Progress:  53%|█████▎    | 53/100 [03:08<03:36,  4.61s/it]
Inference Progress:  54%|█████▍    | 54/100 [03:13<03:39,  4.76s/it]
Inference Progress:  55%|█████▌    | 55/100 [03:14<02:47,  3.72s/it]
Inference Progress:  56%|█████▌    | 56/100 [03:15<02:07,  2.89s/it]
Inference Progress:  57%|█████▋    | 57/100 [03:17<01:45,  2.46s/it]
Inference Progress:  58%|█████▊    | 58/100 [03:22<02:16,  3.26s/it]
Inference Progress:  59%|█████▉    | 59/100 [03:27<02:36,  3.81s/it]
Inference Progress:  60%|██████    | 60/100 [03:30<02:21,  3.54s/it]
Inference Progress:  61%|██████    | 61/100 [03:32<02:04,  3.20s/it]
Inference Progress:  62%|██████▏   | 62/100 [03:35<02:00,  3.18s/it]
Inference Progress:  63%|██████▎   | 63/100 [03:40<02:19,  3.76s/it]
Inference Progress:  64%|██████▍   | 64/100 [03:44<02:08,  3.57s/it]
Inference Progress:  65%|██████▌   | 65/100 [03:49<02:21,  4.04s/it]
Inference Progress:  66%|██████▌   | 66/100 [03:51<01:57,  3.45s/it]
Inference Progress:  67%|██████▋   | 67/100 [03:56<02:10,  3.95s/it]
Inference Progress:  68%|██████▊   | 68/100 [03:59<01:54,  3.59s/it]
Inference Progress:  69%|██████▉   | 69/100 [03:59<01:25,  2.75s/it]
Inference Progress:  70%|███████   | 70/100 [04:02<01:20,  2.68s/it]
Inference Progress:  71%|███████   | 71/100 [04:07<01:39,  3.41s/it]
Inference Progress:  72%|███████▏  | 72/100 [04:10<01:30,  3.23s/it]
Inference Progress:  73%|███████▎  | 73/100 [04:13<01:22,  3.06s/it]
Inference Progress:  74%|███████▍  | 74/100 [04:15<01:12,  2.80s/it]
Inference Progress:  75%|███████▌  | 75/100 [04:17<01:05,  2.64s/it]
Inference Progress:  76%|███████▌  | 76/100 [04:20<01:05,  2.73s/it]
Inference Progress:  77%|███████▋  | 77/100 [04:23<01:03,  2.78s/it]
Inference Progress:  78%|███████▊  | 78/100 [04:28<01:16,  3.48s/it]
Inference Progress:  79%|███████▉  | 79/100 [04:31<01:11,  3.40s/it]
Inference Progress:  80%|████████  | 80/100 [04:35<01:11,  3.56s/it]
Inference Progress:  81%|████████  | 81/100 [04:38<01:02,  3.31s/it]
Inference Progress:  82%|████████▏ | 82/100 [04:40<00:50,  2.82s/it]
Inference Progress:  83%|████████▎ | 83/100 [04:41<00:41,  2.45s/it]
Inference Progress:  84%|████████▍ | 84/100 [04:44<00:38,  2.43s/it]
Inference Progress:  85%|████████▌ | 85/100 [04:49<00:48,  3.24s/it]
Inference Progress:  86%|████████▌ | 86/100 [04:54<00:53,  3.80s/it]
Inference Progress:  87%|████████▋ | 87/100 [04:59<00:54,  4.20s/it]
Inference Progress:  88%|████████▊ | 88/100 [05:01<00:42,  3.50s/it]
Inference Progress:  89%|████████▉ | 89/100 [05:03<00:33,  3.02s/it]
Inference Progress:  90%|█████████ | 90/100 [05:05<00:28,  2.85s/it]
Inference Progress:  91%|█████████ | 91/100 [05:08<00:26,  2.89s/it]
Inference Progress:  92%|█████████▏| 92/100 [05:12<00:25,  3.20s/it]
Inference Progress:  93%|█████████▎| 93/100 [05:16<00:24,  3.53s/it]
Inference Progress:  94%|█████████▍| 94/100 [05:19<00:19,  3.28s/it]
Inference Progress:  95%|█████████▌| 95/100 [05:21<00:14,  2.89s/it]
Inference Progress:  96%|█████████▌| 96/100 [05:24<00:11,  2.96s/it]
Inference Progress:  97%|█████████▋| 97/100 [05:26<00:08,  2.72s/it]
Inference Progress:  98%|█████████▊| 98/100 [05:31<00:06,  3.44s/it]
Inference Progress:  99%|█████████▉| 99/100 [05:35<00:03,  3.37s/it]
Inference Progress: 100%|██████████| 100/100 [05:37<00:00,  3.20s/it]
Inference Progress: 100%|██████████| 100/100 [05:37<00:00,  3.38s/it]
2025-04-24 04:37:12,227 - INFO - 
--- Inference Summary ---
2025-04-24 04:37:12,227 - INFO - Processed 100 samples.
2025-04-24 04:37:12,227 - INFO - Total 'generate' time (sum): 337.73 sec
2025-04-24 04:37:12,227 - INFO - Overall inference loop duration: 337.88 sec
2025-04-24 04:37:12,227 - INFO - Average 'generate' time per sample: 3.3773 sec
2025-04-24 04:37:12,227 - INFO - Total effective tokens generated: 33702
2025-04-24 04:37:12,227 - INFO - Overall effective tokens per second: 99.79
2025-04-24 04:37:12,227 - INFO - RAM Delta during inference: 559.53 MB
2025-04-24 04:37:12,227 - INFO - PyTorch VRAM Peak Delta during inference: 102.09 MB
2025-04-24 04:37:12,229 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/sevenllm_bench__20250424-043132_outputs.json
2025-04-24 04:37:12,229 - INFO - Saved metrics to: /workspace/results/mod/48/sevenllm_bench__20250424-043132_metrics.json
2025-04-24 04:37:12,229 - INFO - Cleaning up resources...
2025-04-24 04:37:12,232 - INFO - CUDA cache cleared.
2025-04-24 04:37:12,232 - INFO - --- Evaluation Complete ---

2025-04-24 04:37:12,970 - INFO - SUCCESS: Run 2/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 04:37:12,970 - INFO - 
Run 3/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='attack'
2025-04-24 04:37:12,970 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset attack --max-new-tokens 512
2025-04-24 04:37:19,258 - WARNING - Stderr:
2025-04-24 04:37:15,104 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:37:15,510 - INFO - --- Starting Evaluation ---
2025-04-24 04:37:15,510 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'attack'}
2025-04-24 04:37:15,510 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:37:15,510 - INFO - Metrics file: /workspace/results/mod/48/ctibench_attack__20250424-043715_metrics.json
2025-04-24 04:37:15,510 - INFO - Outputs file: /workspace/results/mod/48/ctibench_attack__20250424-043715_outputs.json
2025-04-24 04:37:15,510 - INFO - NVML Reporting Active: False
2025-04-24 04:37:15,510 - INFO - Initial RAM usage: 556.65 MB
2025-04-24 04:37:15,510 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/...
2025-04-24 04:37:15,531 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:37:15,625 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:37:17,525 - INFO - CausalLM Model loaded.
2025-04-24 04:37:17,525 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:37:17,526 - INFO - Model loaded in 2.01 seconds.
2025-04-24 04:37:17,526 - INFO - Loading prompts from CTIBench subset: attack
2025-04-24 04:37:18,663 - ERROR - Error loading/processing ctibench (attack): BuilderConfig 'attack' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'attack' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:37:18,664 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:37:18,664 - INFO - Cleaning up resources...
2025-04-24 04:37:18,664 - INFO - CUDA cache cleared.
2025-04-24 04:37:18,664 - INFO - --- Evaluation Complete ---

2025-04-24 04:37:19,258 - INFO - SUCCESS: Run 3/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='attack'. Check output files in /workspace/results/mod/48/
2025-04-24 04:37:19,258 - INFO - 
Run 4/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='exploit_target'
2025-04-24 04:37:19,258 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset exploit_target --max-new-tokens 512
2025-04-24 04:37:25,050 - WARNING - Stderr:
2025-04-24 04:37:21,403 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:37:21,706 - INFO - --- Starting Evaluation ---
2025-04-24 04:37:21,706 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'exploit_target'}
2025-04-24 04:37:21,706 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:37:21,706 - INFO - Metrics file: /workspace/results/mod/48/ctibench_exploit_target__20250424-043721_metrics.json
2025-04-24 04:37:21,706 - INFO - Outputs file: /workspace/results/mod/48/ctibench_exploit_target__20250424-043721_outputs.json
2025-04-24 04:37:21,706 - INFO - NVML Reporting Active: False
2025-04-24 04:37:21,706 - INFO - Initial RAM usage: 555.27 MB
2025-04-24 04:37:21,706 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/...
2025-04-24 04:37:21,726 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:37:21,819 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:37:23,782 - INFO - CausalLM Model loaded.
2025-04-24 04:37:23,783 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:37:23,783 - INFO - Model loaded in 2.08 seconds.
2025-04-24 04:37:23,783 - INFO - Loading prompts from CTIBench subset: exploit_target
2025-04-24 04:37:24,437 - ERROR - Error loading/processing ctibench (exploit_target): BuilderConfig 'exploit_target' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'exploit_target' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:37:24,438 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:37:24,438 - INFO - Cleaning up resources...
2025-04-24 04:37:24,438 - INFO - CUDA cache cleared.
2025-04-24 04:37:24,438 - INFO - --- Evaluation Complete ---

2025-04-24 04:37:25,050 - INFO - SUCCESS: Run 4/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='exploit_target'. Check output files in /workspace/results/mod/48/
2025-04-24 04:37:25,050 - INFO - 
Run 5/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='threat_actor'
2025-04-24 04:37:25,050 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset threat_actor --max-new-tokens 512
2025-04-24 04:37:30,828 - WARNING - Stderr:
2025-04-24 04:37:27,203 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:37:27,505 - INFO - --- Starting Evaluation ---
2025-04-24 04:37:27,505 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'threat_actor'}
2025-04-24 04:37:27,505 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:37:27,505 - INFO - Metrics file: /workspace/results/mod/48/ctibench_threat_actor__20250424-043727_metrics.json
2025-04-24 04:37:27,505 - INFO - Outputs file: /workspace/results/mod/48/ctibench_threat_actor__20250424-043727_outputs.json
2025-04-24 04:37:27,505 - INFO - NVML Reporting Active: False
2025-04-24 04:37:27,505 - INFO - Initial RAM usage: 556.86 MB
2025-04-24 04:37:27,505 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/...
2025-04-24 04:37:27,525 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:37:27,619 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:37:29,527 - INFO - CausalLM Model loaded.
2025-04-24 04:37:29,528 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:37:29,528 - INFO - Model loaded in 2.02 seconds.
2025-04-24 04:37:29,528 - INFO - Loading prompts from CTIBench subset: threat_actor
2025-04-24 04:37:30,232 - ERROR - Error loading/processing ctibench (threat_actor): BuilderConfig 'threat_actor' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'threat_actor' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:37:30,233 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:37:30,234 - INFO - Cleaning up resources...
2025-04-24 04:37:30,234 - INFO - CUDA cache cleared.
2025-04-24 04:37:30,234 - INFO - --- Evaluation Complete ---

2025-04-24 04:37:30,828 - INFO - SUCCESS: Run 5/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='threat_actor'. Check output files in /workspace/results/mod/48/
2025-04-24 04:37:30,828 - INFO - 
Run 6/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='ttps'
2025-04-24 04:37:30,828 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset ttps --max-new-tokens 512
2025-04-24 04:37:36,644 - WARNING - Stderr:
2025-04-24 04:37:33,006 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:37:33,310 - INFO - --- Starting Evaluation ---
2025-04-24 04:37:33,311 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'ttps'}
2025-04-24 04:37:33,311 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:37:33,311 - INFO - Metrics file: /workspace/results/mod/48/ctibench_ttps__20250424-043733_metrics.json
2025-04-24 04:37:33,311 - INFO - Outputs file: /workspace/results/mod/48/ctibench_ttps__20250424-043733_outputs.json
2025-04-24 04:37:33,311 - INFO - NVML Reporting Active: False
2025-04-24 04:37:33,311 - INFO - Initial RAM usage: 556.69 MB
2025-04-24 04:37:33,311 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/...
2025-04-24 04:37:33,331 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:37:33,424 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:37:35,330 - INFO - CausalLM Model loaded.
2025-04-24 04:37:35,331 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:37:35,331 - INFO - Model loaded in 2.02 seconds.
2025-04-24 04:37:35,331 - INFO - Loading prompts from CTIBench subset: ttps
2025-04-24 04:37:36,016 - ERROR - Error loading/processing ctibench (ttps): BuilderConfig 'ttps' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'ttps' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:37:36,017 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:37:36,017 - INFO - Cleaning up resources...
2025-04-24 04:37:36,017 - INFO - CUDA cache cleared.
2025-04-24 04:37:36,017 - INFO - --- Evaluation Complete ---

2025-04-24 04:37:36,644 - INFO - SUCCESS: Run 6/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='ttps'. Check output files in /workspace/results/mod/48/
2025-04-24 04:37:36,644 - INFO - 
Run 7/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 04:37:36,644 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 04:37:42,525 - WARNING - Stderr:
2025-04-24 04:37:38,804 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:37:39,108 - INFO - --- Starting Evaluation ---
2025-04-24 04:37:39,108 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 04:37:39,108 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:37:39,108 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre__20250424-043739_metrics.json
2025-04-24 04:37:39,108 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre__20250424-043739_outputs.json
2025-04-24 04:37:39,108 - INFO - NVML Reporting Active: False
2025-04-24 04:37:39,109 - INFO - Initial RAM usage: 555.24 MB
2025-04-24 04:37:39,109 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/...
2025-04-24 04:37:39,129 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:37:39,223 - INFO - Tokenizer loaded.
2025-04-24 04:37:39,990 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 04:37:39,990 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:37:41,534 - INFO - CausalLM Model loaded.
2025-04-24 04:37:41,535 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:37:41,535 - INFO - Model loaded in 2.43 seconds.
2025-04-24 04:37:41,536 - INFO - Loading prompts from CyberSecEval3 MITRE file: /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json
2025-04-24 04:37:41,547 - INFO - Processing the first 100 items.

Parsing MITRE prompts:   0%|          | 0/100 [00:00<?, ?it/s]2025-04-24 04:37:41,549 - WARNING - Item 5: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,549 - WARNING - Item 8: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,550 - WARNING - Item 14: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,551 - WARNING - Item 21: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,551 - WARNING - Item 25: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,551 - WARNING - Item 27: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,551 - WARNING - Item 30: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,552 - WARNING - Item 32: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,552 - WARNING - Item 35: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,552 - WARNING - Item 36: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,552 - WARNING - Item 48: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,553 - WARNING - Item 49: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,553 - WARNING - Item 52: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,553 - WARNING - Item 58: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,553 - WARNING - Item 63: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,554 - WARNING - Item 72: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,554 - WARNING - Item 81: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,555 - WARNING - Item 86: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:37:41,555 - WARNING - Item 88: Regex failed to find/extract 'prompt' value from mutated_prompt_str.

Parsing MITRE prompts: 100%|██████████| 100/100 [00:00<00:00, 13107.61it/s]
2025-04-24 04:37:41,555 - INFO - Successfully loaded 81 prompts from cyberseceval3_mitre.
2025-04-24 04:37:41,556 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 04:37:41,556 - INFO - Starting inference on 81 samples...

Inference Progress:   0%|          | 0/81 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
2025-04-24 04:37:41,645 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,648 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,651 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,653 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,655 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,658 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:   7%|▋         | 6/81 [00:00<00:01, 58.94it/s]2025-04-24 04:37:41,660 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,662 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,665 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,667 - ERROR - Generation failed for T1024: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,669 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,672 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,674 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,676 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,677 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,680 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,682 - ERROR - Generation failed for T1090.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,684 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,686 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,688 - ERROR - Generation failed for T1090.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,691 - ERROR - Generation failed for T1090.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,692 - ERROR - Generation failed for T1483: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,694 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,696 - ERROR - Generation failed for T1090.003: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,699 - ERROR - Generation failed for T1571: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,701 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,702 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,704 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,706 - ERROR - Generation failed for T1095: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,708 - ERROR - Generation failed for T1105: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,710 - ERROR - Generation failed for T1094.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,712 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,714 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,716 - ERROR - Generation failed for T1102: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,718 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,721 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,723 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,725 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,727 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,729 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,731 - ERROR - Generation failed for T1074: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,733 - ERROR - Generation failed for T1097: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,736 - ERROR - Generation failed for T1483: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,738 - ERROR - Generation failed for T1021.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,740 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,742 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,744 - ERROR - Generation failed for T1090.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,746 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,747 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,750 - ERROR - Generation failed for T1090.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,751 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,753 - ERROR - Generation failed for T1071: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,755 - ERROR - Generation failed for T1074: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,757 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,759 - ERROR - Generation failed for T1090.003: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  68%|██████▊   | 55/81 [00:00<00:00, 307.97it/s]2025-04-24 04:37:41,761 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,763 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,765 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,767 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,769 - ERROR - Generation failed for T1090.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,771 - ERROR - Generation failed for T1090.003: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,773 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,774 - ERROR - Generation failed for T1094: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,776 - ERROR - Generation failed for T1132.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,779 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,780 - ERROR - Generation failed for T1059: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,783 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,785 - ERROR - Generation failed for T1071: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,787 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,789 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,791 - ERROR - Generation failed for T1094: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,794 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,795 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,797 - ERROR - Generation failed for T1071: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,799 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,801 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,804 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,806 - ERROR - Generation failed for T1095: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,808 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,810 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:41,811 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress: 100%|██████████| 81/81 [00:00<00:00, 316.76it/s]
2025-04-24 04:37:41,812 - INFO - 
--- Inference Summary ---
2025-04-24 04:37:41,812 - INFO - Processed 81 samples.
2025-04-24 04:37:41,812 - INFO - Total 'generate' time (sum): 0.00 sec
2025-04-24 04:37:41,812 - INFO - Overall inference loop duration: 0.26 sec
2025-04-24 04:37:41,812 - INFO - Average 'generate' time per sample: 0.0000 sec
2025-04-24 04:37:41,812 - INFO - Total effective tokens generated: 0
2025-04-24 04:37:41,812 - INFO - Overall effective tokens per second: 0.00
2025-04-24 04:37:41,812 - INFO - RAM Delta during inference: 28.53 MB
2025-04-24 04:37:41,812 - INFO - PyTorch VRAM Peak Delta during inference: 0.02 MB
2025-04-24 04:37:41,813 - INFO - Saved outputs (81 items) to: /workspace/results/mod/48/cyberseceval3_mitre__20250424-043739_outputs.json
2025-04-24 04:37:41,813 - INFO - Saved metrics to: /workspace/results/mod/48/cyberseceval3_mitre__20250424-043739_metrics.json
2025-04-24 04:37:41,813 - INFO - Cleaning up resources...
2025-04-24 04:37:41,813 - INFO - CUDA cache cleared.
2025-04-24 04:37:41,813 - INFO - --- Evaluation Complete ---

2025-04-24 04:37:42,527 - INFO - SUCCESS: Run 7/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 04:37:42,527 - INFO - 
Run 8/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 04:37:42,527 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 04:37:48,444 - WARNING - Stderr:
2025-04-24 04:37:44,701 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:37:45,006 - INFO - --- Starting Evaluation ---
2025-04-24 04:37:45,006 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 04:37:45,006 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:37:45,006 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench__20250424-043745_metrics.json
2025-04-24 04:37:45,006 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench__20250424-043745_outputs.json
2025-04-24 04:37:45,006 - INFO - NVML Reporting Active: False
2025-04-24 04:37:45,006 - INFO - Initial RAM usage: 557.00 MB
2025-04-24 04:37:45,006 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/...
2025-04-24 04:37:45,026 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:37:45,119 - INFO - Tokenizer loaded.
2025-04-24 04:37:45,887 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 04:37:45,887 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:37:47,453 - INFO - CausalLM Model loaded.
2025-04-24 04:37:47,454 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:37:47,454 - INFO - Model loaded in 2.45 seconds.
2025-04-24 04:37:47,455 - INFO - Loading prompts from SEvenLLM saved dataset directory: /workspace/calibration_data/sevenllm_instruct_subset_manual/
2025-04-24 04:37:47,458 - INFO - Loaded dataset: Dataset({
    features: ['id', 'output', 'instruction', 'input', 'category', 'thought'],
    num_rows: 200
})
2025-04-24 04:37:47,459 - INFO - Selected first 100 samples for evaluation.

Parsing SEvenLLM prompts:   0%|          | 0/100 [00:00<?, ?it/s]
Parsing SEvenLLM prompts: 100%|██████████| 100/100 [00:00<00:00, 25466.33it/s]
2025-04-24 04:37:47,464 - INFO - Successfully loaded 100 prompts from sevenllm_bench.
2025-04-24 04:37:47,464 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 04:37:47,464 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
2025-04-24 04:37:47,569 - ERROR - Generation failed for 1: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:   1%|          | 1/100 [00:00<00:10,  9.44it/s]2025-04-24 04:37:47,572 - ERROR - Generation failed for 2: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,574 - ERROR - Generation failed for 3: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,577 - ERROR - Generation failed for 4: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,578 - ERROR - Generation failed for 5: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,581 - ERROR - Generation failed for 6: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,583 - ERROR - Generation failed for 7: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,585 - ERROR - Generation failed for 8: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,587 - ERROR - Generation failed for 9: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,589 - ERROR - Generation failed for 10: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,591 - ERROR - Generation failed for 11: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,593 - ERROR - Generation failed for 12: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,595 - ERROR - Generation failed for 13: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,597 - ERROR - Generation failed for 14: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,599 - ERROR - Generation failed for 15: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,601 - ERROR - Generation failed for 16: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,603 - ERROR - Generation failed for 17: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,605 - ERROR - Generation failed for 18: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,607 - ERROR - Generation failed for 19: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,609 - ERROR - Generation failed for 20: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,611 - ERROR - Generation failed for 21: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,613 - ERROR - Generation failed for 22: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,616 - ERROR - Generation failed for 23: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,619 - ERROR - Generation failed for 24: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,622 - ERROR - Generation failed for 25: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,624 - ERROR - Generation failed for 26: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,626 - ERROR - Generation failed for 27: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,627 - ERROR - Generation failed for 28: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,629 - ERROR - Generation failed for 29: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,631 - ERROR - Generation failed for 30: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,633 - ERROR - Generation failed for 31: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,635 - ERROR - Generation failed for 32: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,637 - ERROR - Generation failed for 33: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,639 - ERROR - Generation failed for 34: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,641 - ERROR - Generation failed for 35: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,643 - ERROR - Generation failed for 36: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,645 - ERROR - Generation failed for 37: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,646 - ERROR - Generation failed for 38: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,648 - ERROR - Generation failed for 39: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,650 - ERROR - Generation failed for 40: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,652 - ERROR - Generation failed for 41: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,654 - ERROR - Generation failed for 42: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,656 - ERROR - Generation failed for 43: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,658 - ERROR - Generation failed for 44: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,660 - ERROR - Generation failed for 45: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,662 - ERROR - Generation failed for 46: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,665 - ERROR - Generation failed for 47: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,667 - ERROR - Generation failed for 48: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,669 - ERROR - Generation failed for 49: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,671 - ERROR - Generation failed for 50: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  50%|█████     | 50/100 [00:00<00:00, 283.48it/s]2025-04-24 04:37:47,673 - ERROR - Generation failed for 51: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,675 - ERROR - Generation failed for 52: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,677 - ERROR - Generation failed for 53: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,679 - ERROR - Generation failed for 54: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,682 - ERROR - Generation failed for 55: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,685 - ERROR - Generation failed for 56: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,687 - ERROR - Generation failed for 57: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,689 - ERROR - Generation failed for 58: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,691 - ERROR - Generation failed for 59: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,693 - ERROR - Generation failed for 60: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,695 - ERROR - Generation failed for 61: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,697 - ERROR - Generation failed for 62: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,700 - ERROR - Generation failed for 63: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,702 - ERROR - Generation failed for 64: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,705 - ERROR - Generation failed for 65: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,707 - ERROR - Generation failed for 66: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,709 - ERROR - Generation failed for 67: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,711 - ERROR - Generation failed for 68: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,713 - ERROR - Generation failed for 69: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,716 - ERROR - Generation failed for 70: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,718 - ERROR - Generation failed for 71: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,720 - ERROR - Generation failed for 72: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,722 - ERROR - Generation failed for 73: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,724 - ERROR - Generation failed for 74: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,726 - ERROR - Generation failed for 75: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,729 - ERROR - Generation failed for 76: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,732 - ERROR - Generation failed for 77: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,734 - ERROR - Generation failed for 78: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,736 - ERROR - Generation failed for 79: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,737 - ERROR - Generation failed for 80: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,740 - ERROR - Generation failed for 81: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,743 - ERROR - Generation failed for 82: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,745 - ERROR - Generation failed for 83: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,748 - ERROR - Generation failed for 84: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,750 - ERROR - Generation failed for 85: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,752 - ERROR - Generation failed for 86: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,755 - ERROR - Generation failed for 87: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,756 - ERROR - Generation failed for 88: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,758 - ERROR - Generation failed for 89: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,760 - ERROR - Generation failed for 90: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,762 - ERROR - Generation failed for 91: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,764 - ERROR - Generation failed for 92: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,766 - ERROR - Generation failed for 93: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,768 - ERROR - Generation failed for 94: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,770 - ERROR - Generation failed for 95: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,772 - ERROR - Generation failed for 96: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  96%|█████████▌| 96/100 [00:00<00:00, 361.94it/s]2025-04-24 04:37:47,775 - ERROR - Generation failed for 97: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,777 - ERROR - Generation failed for 98: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,780 - ERROR - Generation failed for 99: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 04:37:47,784 - ERROR - Generation failed for 100: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress: 100%|██████████| 100/100 [00:00<00:00, 312.77it/s]
2025-04-24 04:37:47,785 - INFO - 
--- Inference Summary ---
2025-04-24 04:37:47,785 - INFO - Processed 100 samples.
2025-04-24 04:37:47,785 - INFO - Total 'generate' time (sum): 0.00 sec
2025-04-24 04:37:47,785 - INFO - Overall inference loop duration: 0.32 sec
2025-04-24 04:37:47,785 - INFO - Average 'generate' time per sample: 0.0000 sec
2025-04-24 04:37:47,785 - INFO - Total effective tokens generated: 0
2025-04-24 04:37:47,785 - INFO - Overall effective tokens per second: 0.00
2025-04-24 04:37:47,785 - INFO - RAM Delta during inference: 52.50 MB
2025-04-24 04:37:47,785 - INFO - PyTorch VRAM Peak Delta during inference: 0.03 MB
2025-04-24 04:37:47,786 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/sevenllm_bench__20250424-043745_outputs.json
2025-04-24 04:37:47,786 - INFO - Saved metrics to: /workspace/results/mod/48/sevenllm_bench__20250424-043745_metrics.json
2025-04-24 04:37:47,786 - INFO - Cleaning up resources...
2025-04-24 04:37:47,786 - INFO - CUDA cache cleared.
2025-04-24 04:37:47,786 - INFO - --- Evaluation Complete ---

2025-04-24 04:37:48,447 - INFO - SUCCESS: Run 8/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 04:37:48,447 - INFO - 
Run 9/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='attack'
2025-04-24 04:37:48,447 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset attack --max-new-tokens 512
2025-04-24 04:37:54,673 - WARNING - Stderr:
2025-04-24 04:37:50,603 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:37:50,908 - INFO - --- Starting Evaluation ---
2025-04-24 04:37:50,908 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'attack'}
2025-04-24 04:37:50,908 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:37:50,908 - INFO - Metrics file: /workspace/results/mod/48/ctibench_attack__20250424-043750_metrics.json
2025-04-24 04:37:50,908 - INFO - Outputs file: /workspace/results/mod/48/ctibench_attack__20250424-043750_outputs.json
2025-04-24 04:37:50,908 - INFO - NVML Reporting Active: False
2025-04-24 04:37:50,908 - INFO - Initial RAM usage: 555.34 MB
2025-04-24 04:37:50,908 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/...
2025-04-24 04:37:50,928 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:37:51,021 - INFO - Tokenizer loaded.
2025-04-24 04:37:51,880 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 04:37:51,880 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:37:53,437 - INFO - CausalLM Model loaded.
2025-04-24 04:37:53,438 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:37:53,438 - INFO - Model loaded in 2.53 seconds.
2025-04-24 04:37:53,438 - INFO - Loading prompts from CTIBench subset: attack
2025-04-24 04:37:54,020 - ERROR - Error loading/processing ctibench (attack): BuilderConfig 'attack' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'attack' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:37:54,021 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:37:54,022 - INFO - Cleaning up resources...
2025-04-24 04:37:54,022 - INFO - CUDA cache cleared.
2025-04-24 04:37:54,022 - INFO - --- Evaluation Complete ---

2025-04-24 04:37:54,673 - INFO - SUCCESS: Run 9/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='attack'. Check output files in /workspace/results/mod/48/
2025-04-24 04:37:54,673 - INFO - 
Run 10/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='exploit_target'
2025-04-24 04:37:54,673 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset exploit_target --max-new-tokens 512
2025-04-24 04:38:00,999 - WARNING - Stderr:
2025-04-24 04:37:56,808 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:37:57,116 - INFO - --- Starting Evaluation ---
2025-04-24 04:37:57,116 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'exploit_target'}
2025-04-24 04:37:57,116 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:37:57,116 - INFO - Metrics file: /workspace/results/mod/48/ctibench_exploit_target__20250424-043757_metrics.json
2025-04-24 04:37:57,116 - INFO - Outputs file: /workspace/results/mod/48/ctibench_exploit_target__20250424-043757_outputs.json
2025-04-24 04:37:57,116 - INFO - NVML Reporting Active: False
2025-04-24 04:37:57,116 - INFO - Initial RAM usage: 556.60 MB
2025-04-24 04:37:57,116 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/...
2025-04-24 04:37:57,137 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:37:57,231 - INFO - Tokenizer loaded.
2025-04-24 04:37:58,013 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 04:37:58,013 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:37:59,695 - INFO - CausalLM Model loaded.
2025-04-24 04:37:59,695 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:37:59,696 - INFO - Model loaded in 2.58 seconds.
2025-04-24 04:37:59,696 - INFO - Loading prompts from CTIBench subset: exploit_target
2025-04-24 04:38:00,316 - ERROR - Error loading/processing ctibench (exploit_target): BuilderConfig 'exploit_target' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'exploit_target' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:38:00,317 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:38:00,317 - INFO - Cleaning up resources...
2025-04-24 04:38:00,317 - INFO - CUDA cache cleared.
2025-04-24 04:38:00,317 - INFO - --- Evaluation Complete ---

2025-04-24 04:38:00,999 - INFO - SUCCESS: Run 10/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='exploit_target'. Check output files in /workspace/results/mod/48/
2025-04-24 04:38:00,999 - INFO - 
Run 11/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='threat_actor'
2025-04-24 04:38:01,000 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset threat_actor --max-new-tokens 512
2025-04-24 04:38:07,257 - WARNING - Stderr:
2025-04-24 04:38:03,191 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:38:03,495 - INFO - --- Starting Evaluation ---
2025-04-24 04:38:03,495 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'threat_actor'}
2025-04-24 04:38:03,495 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:38:03,495 - INFO - Metrics file: /workspace/results/mod/48/ctibench_threat_actor__20250424-043803_metrics.json
2025-04-24 04:38:03,495 - INFO - Outputs file: /workspace/results/mod/48/ctibench_threat_actor__20250424-043803_outputs.json
2025-04-24 04:38:03,495 - INFO - NVML Reporting Active: False
2025-04-24 04:38:03,495 - INFO - Initial RAM usage: 556.89 MB
2025-04-24 04:38:03,495 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/...
2025-04-24 04:38:03,515 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:38:03,610 - INFO - Tokenizer loaded.
2025-04-24 04:38:04,395 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 04:38:04,395 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:38:05,979 - INFO - CausalLM Model loaded.
2025-04-24 04:38:05,979 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:38:05,980 - INFO - Model loaded in 2.48 seconds.
2025-04-24 04:38:05,980 - INFO - Loading prompts from CTIBench subset: threat_actor
2025-04-24 04:38:06,606 - ERROR - Error loading/processing ctibench (threat_actor): BuilderConfig 'threat_actor' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'threat_actor' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:38:06,607 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:38:06,607 - INFO - Cleaning up resources...
2025-04-24 04:38:06,607 - INFO - CUDA cache cleared.
2025-04-24 04:38:06,607 - INFO - --- Evaluation Complete ---

2025-04-24 04:38:07,257 - INFO - SUCCESS: Run 11/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='threat_actor'. Check output files in /workspace/results/mod/48/
2025-04-24 04:38:07,257 - INFO - 
Run 12/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='ttps'
2025-04-24 04:38:07,257 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset ttps --max-new-tokens 512
2025-04-24 04:38:13,338 - WARNING - Stderr:
2025-04-24 04:38:09,404 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:38:09,712 - INFO - --- Starting Evaluation ---
2025-04-24 04:38:09,712 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'ttps'}
2025-04-24 04:38:09,712 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:38:09,712 - INFO - Metrics file: /workspace/results/mod/48/ctibench_ttps__20250424-043809_metrics.json
2025-04-24 04:38:09,712 - INFO - Outputs file: /workspace/results/mod/48/ctibench_ttps__20250424-043809_outputs.json
2025-04-24 04:38:09,712 - INFO - NVML Reporting Active: False
2025-04-24 04:38:09,712 - INFO - Initial RAM usage: 556.59 MB
2025-04-24 04:38:09,712 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/...
2025-04-24 04:38:09,732 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:38:09,826 - INFO - Tokenizer loaded.
2025-04-24 04:38:10,597 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 04:38:10,597 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:38:12,133 - INFO - CausalLM Model loaded.
2025-04-24 04:38:12,134 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:38:12,134 - INFO - Model loaded in 2.42 seconds.
2025-04-24 04:38:12,134 - INFO - Loading prompts from CTIBench subset: ttps
2025-04-24 04:38:12,688 - ERROR - Error loading/processing ctibench (ttps): BuilderConfig 'ttps' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'ttps' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:38:12,689 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:38:12,689 - INFO - Cleaning up resources...
2025-04-24 04:38:12,689 - INFO - CUDA cache cleared.
2025-04-24 04:38:12,689 - INFO - --- Evaluation Complete ---

2025-04-24 04:38:13,339 - INFO - SUCCESS: Run 12/18: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='ttps'. Check output files in /workspace/results/mod/48/
2025-04-24 04:38:13,339 - INFO - 
Run 13/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 04:38:13,339 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 04:48:27,847 - WARNING - Stderr:
2025-04-24 04:38:15,498 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:38:15,798 - INFO - --- Starting Evaluation ---
2025-04-24 04:38:15,798 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 04:38:15,798 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:38:15,798 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre__20250424-043815_metrics.json
2025-04-24 04:38:15,799 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre__20250424-043815_outputs.json
2025-04-24 04:38:15,799 - INFO - NVML Reporting Active: False
2025-04-24 04:38:15,799 - INFO - Initial RAM usage: 556.84 MB
2025-04-24 04:38:15,799 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/...
2025-04-24 04:38:15,822 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:38:16,118 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:38:19,595 - INFO - CausalLM Model loaded.
2025-04-24 04:38:19,596 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:38:19,596 - INFO - Model loaded in 3.80 seconds.
2025-04-24 04:38:19,596 - INFO - Loading prompts from CyberSecEval3 MITRE file: /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json
2025-04-24 04:38:19,608 - INFO - Processing the first 100 items.

Parsing MITRE prompts:   0%|          | 0/100 [00:00<?, ?it/s]2025-04-24 04:38:19,610 - WARNING - Item 5: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,611 - WARNING - Item 8: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,611 - WARNING - Item 14: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,612 - WARNING - Item 21: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,613 - WARNING - Item 25: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,613 - WARNING - Item 27: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,613 - WARNING - Item 30: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,613 - WARNING - Item 32: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,614 - WARNING - Item 35: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,614 - WARNING - Item 36: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,615 - WARNING - Item 48: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,615 - WARNING - Item 49: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,615 - WARNING - Item 52: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,616 - WARNING - Item 58: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,616 - WARNING - Item 63: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,617 - WARNING - Item 72: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,617 - WARNING - Item 81: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,618 - WARNING - Item 86: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 04:38:19,618 - WARNING - Item 88: Regex failed to find/extract 'prompt' value from mutated_prompt_str.

Parsing MITRE prompts: 100%|██████████| 100/100 [00:00<00:00, 10602.92it/s]
2025-04-24 04:38:19,619 - INFO - Successfully loaded 81 prompts from cyberseceval3_mitre.
2025-04-24 04:38:19,619 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 04:38:19,620 - INFO - Starting inference on 81 samples...

Inference Progress:   0%|          | 0/81 [00:00<?, ?it/s]
Inference Progress:   1%|          | 1/81 [00:08<10:41,  8.02s/it]
Inference Progress:   2%|▏         | 2/81 [00:15<10:08,  7.70s/it]
Inference Progress:   4%|▎         | 3/81 [00:23<09:58,  7.67s/it]
Inference Progress:   5%|▍         | 4/81 [00:30<09:46,  7.62s/it]
Inference Progress:   6%|▌         | 5/81 [00:38<09:37,  7.60s/it]
Inference Progress:   7%|▋         | 6/81 [00:45<09:27,  7.57s/it]
Inference Progress:   9%|▊         | 7/81 [00:53<09:19,  7.56s/it]
Inference Progress:  10%|▉         | 8/81 [01:00<09:13,  7.58s/it]
Inference Progress:  11%|█         | 9/81 [01:08<09:06,  7.58s/it]
Inference Progress:  12%|█▏        | 10/81 [01:16<08:58,  7.58s/it]
Inference Progress:  14%|█▎        | 11/81 [01:23<08:50,  7.57s/it]
Inference Progress:  15%|█▍        | 12/81 [01:31<08:42,  7.58s/it]
Inference Progress:  16%|█▌        | 13/81 [01:38<08:33,  7.55s/it]
Inference Progress:  17%|█▋        | 14/81 [01:46<08:24,  7.53s/it]
Inference Progress:  19%|█▊        | 15/81 [01:53<08:14,  7.50s/it]
Inference Progress:  20%|█▉        | 16/81 [02:01<08:09,  7.53s/it]
Inference Progress:  21%|██        | 17/81 [02:08<08:02,  7.53s/it]
Inference Progress:  22%|██▏       | 18/81 [02:16<07:51,  7.48s/it]
Inference Progress:  23%|██▎       | 19/81 [02:23<07:46,  7.52s/it]
Inference Progress:  25%|██▍       | 20/81 [02:31<07:39,  7.54s/it]
Inference Progress:  26%|██▌       | 21/81 [02:38<07:33,  7.55s/it]
Inference Progress:  27%|██▋       | 22/81 [02:46<07:22,  7.50s/it]
Inference Progress:  28%|██▊       | 23/81 [02:53<07:16,  7.52s/it]
Inference Progress:  30%|██▉       | 24/81 [03:01<07:07,  7.50s/it]
Inference Progress:  31%|███       | 25/81 [03:08<07:00,  7.51s/it]
Inference Progress:  32%|███▏      | 26/81 [03:16<06:53,  7.52s/it]
Inference Progress:  33%|███▎      | 27/81 [03:23<06:43,  7.47s/it]
Inference Progress:  35%|███▍      | 28/81 [03:31<06:35,  7.46s/it]
Inference Progress:  36%|███▌      | 29/81 [03:38<06:27,  7.45s/it]
Inference Progress:  37%|███▋      | 30/81 [03:46<06:19,  7.44s/it]
Inference Progress:  38%|███▊      | 31/81 [03:53<06:14,  7.48s/it]
Inference Progress:  40%|███▉      | 32/81 [04:00<06:04,  7.44s/it]
Inference Progress:  41%|████      | 33/81 [04:08<05:58,  7.47s/it]
Inference Progress:  42%|████▏     | 34/81 [04:16<05:52,  7.51s/it]
Inference Progress:  43%|████▎     | 35/81 [04:23<05:46,  7.52s/it]
Inference Progress:  44%|████▍     | 36/81 [04:31<05:39,  7.55s/it]
Inference Progress:  46%|████▌     | 37/81 [04:38<05:31,  7.54s/it]
Inference Progress:  47%|████▋     | 38/81 [04:46<05:23,  7.53s/it]
Inference Progress:  48%|████▊     | 39/81 [04:53<05:16,  7.53s/it]
Inference Progress:  49%|████▉     | 40/81 [05:01<05:10,  7.57s/it]
Inference Progress:  51%|█████     | 41/81 [05:08<05:01,  7.54s/it]
Inference Progress:  52%|█████▏    | 42/81 [05:16<04:54,  7.56s/it]
Inference Progress:  53%|█████▎    | 43/81 [05:24<04:46,  7.54s/it]
Inference Progress:  54%|█████▍    | 44/81 [05:31<04:39,  7.54s/it]
Inference Progress:  56%|█████▌    | 45/81 [05:39<04:33,  7.58s/it]
Inference Progress:  57%|█████▋    | 46/81 [05:46<04:24,  7.55s/it]
Inference Progress:  58%|█████▊    | 47/81 [05:54<04:15,  7.51s/it]
Inference Progress:  59%|█████▉    | 48/81 [06:01<04:08,  7.52s/it]
Inference Progress:  60%|██████    | 49/81 [06:08<03:58,  7.46s/it]
Inference Progress:  62%|██████▏   | 50/81 [06:16<03:52,  7.50s/it]
Inference Progress:  63%|██████▎   | 51/81 [06:24<03:44,  7.49s/it]
Inference Progress:  64%|██████▍   | 52/81 [06:31<03:37,  7.50s/it]
Inference Progress:  65%|██████▌   | 53/81 [06:39<03:30,  7.51s/it]
Inference Progress:  67%|██████▋   | 54/81 [06:46<03:21,  7.46s/it]
Inference Progress:  68%|██████▊   | 55/81 [06:53<03:14,  7.46s/it]
Inference Progress:  69%|██████▉   | 56/81 [07:01<03:06,  7.45s/it]
Inference Progress:  70%|███████   | 57/81 [07:08<02:59,  7.47s/it]
Inference Progress:  72%|███████▏  | 58/81 [07:14<02:39,  6.95s/it]
Inference Progress:  73%|███████▎  | 59/81 [07:22<02:36,  7.11s/it]
Inference Progress:  74%|███████▍  | 60/81 [07:29<02:31,  7.21s/it]
Inference Progress:  75%|███████▌  | 61/81 [07:37<02:26,  7.32s/it]
Inference Progress:  77%|███████▋  | 62/81 [07:44<02:19,  7.34s/it]
Inference Progress:  78%|███████▊  | 63/81 [07:51<02:12,  7.34s/it]
Inference Progress:  79%|███████▉  | 64/81 [07:59<02:05,  7.41s/it]
Inference Progress:  80%|████████  | 65/81 [08:06<01:59,  7.47s/it]
Inference Progress:  81%|████████▏ | 66/81 [08:14<01:51,  7.46s/it]
Inference Progress:  83%|████████▎ | 67/81 [08:21<01:44,  7.48s/it]
Inference Progress:  84%|████████▍ | 68/81 [08:29<01:37,  7.52s/it]
Inference Progress:  85%|████████▌ | 69/81 [08:37<01:30,  7.51s/it]
Inference Progress:  86%|████████▋ | 70/81 [08:44<01:22,  7.51s/it]
Inference Progress:  88%|████████▊ | 71/81 [08:52<01:15,  7.54s/it]
Inference Progress:  89%|████████▉ | 72/81 [08:59<01:07,  7.55s/it]
Inference Progress:  90%|█████████ | 73/81 [09:07<00:59,  7.49s/it]
Inference Progress:  91%|█████████▏| 74/81 [09:14<00:52,  7.46s/it]
Inference Progress:  93%|█████████▎| 75/81 [09:22<00:45,  7.51s/it]
Inference Progress:  94%|█████████▍| 76/81 [09:29<00:37,  7.51s/it]
Inference Progress:  95%|█████████▌| 77/81 [09:37<00:30,  7.54s/it]
Inference Progress:  96%|█████████▋| 78/81 [09:44<00:22,  7.55s/it]
Inference Progress:  98%|█████████▊| 79/81 [09:52<00:15,  7.58s/it]
Inference Progress:  99%|█████████▉| 80/81 [09:59<00:07,  7.54s/it]
Inference Progress: 100%|██████████| 81/81 [10:07<00:00,  7.50s/it]
Inference Progress: 100%|██████████| 81/81 [10:07<00:00,  7.50s/it]
2025-04-24 04:48:26,969 - INFO - 
--- Inference Summary ---
2025-04-24 04:48:26,969 - INFO - Processed 81 samples.
2025-04-24 04:48:26,969 - INFO - Total 'generate' time (sum): 607.19 sec
2025-04-24 04:48:26,969 - INFO - Overall inference loop duration: 607.35 sec
2025-04-24 04:48:26,969 - INFO - Average 'generate' time per sample: 7.4962 sec
2025-04-24 04:48:26,969 - INFO - Total effective tokens generated: 41349
2025-04-24 04:48:26,969 - INFO - Overall effective tokens per second: 68.10
2025-04-24 04:48:26,969 - INFO - RAM Delta during inference: 679.57 MB
2025-04-24 04:48:26,969 - INFO - PyTorch VRAM Peak Delta during inference: 176.48 MB
2025-04-24 04:48:26,971 - INFO - Saved outputs (81 items) to: /workspace/results/mod/48/cyberseceval3_mitre__20250424-043815_outputs.json
2025-04-24 04:48:26,971 - INFO - Saved metrics to: /workspace/results/mod/48/cyberseceval3_mitre__20250424-043815_metrics.json
2025-04-24 04:48:26,971 - INFO - Cleaning up resources...
2025-04-24 04:48:26,977 - INFO - CUDA cache cleared.
2025-04-24 04:48:26,977 - INFO - --- Evaluation Complete ---

2025-04-24 04:48:27,847 - INFO - SUCCESS: Run 13/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 04:48:27,847 - INFO - 
Run 14/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 04:48:27,847 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 04:54:50,695 - WARNING - Stderr:
2025-04-24 04:48:30,004 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:48:30,336 - INFO - --- Starting Evaluation ---
2025-04-24 04:48:30,336 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 04:48:30,336 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:48:30,336 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench__20250424-044830_metrics.json
2025-04-24 04:48:30,336 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench__20250424-044830_outputs.json
2025-04-24 04:48:30,336 - INFO - NVML Reporting Active: False
2025-04-24 04:48:30,336 - INFO - Initial RAM usage: 555.07 MB
2025-04-24 04:48:30,336 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/...
2025-04-24 04:48:30,357 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:48:30,654 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:48:34,220 - INFO - CausalLM Model loaded.
2025-04-24 04:48:34,221 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:48:34,221 - INFO - Model loaded in 3.88 seconds.
2025-04-24 04:48:34,221 - INFO - Loading prompts from SEvenLLM saved dataset directory: /workspace/calibration_data/sevenllm_instruct_subset_manual/
2025-04-24 04:48:34,225 - INFO - Loaded dataset: Dataset({
    features: ['id', 'output', 'instruction', 'input', 'category', 'thought'],
    num_rows: 200
})
2025-04-24 04:48:34,226 - INFO - Selected first 100 samples for evaluation.

Parsing SEvenLLM prompts:   0%|          | 0/100 [00:00<?, ?it/s]
Parsing SEvenLLM prompts: 100%|██████████| 100/100 [00:00<00:00, 25990.23it/s]
2025-04-24 04:48:34,231 - INFO - Successfully loaded 100 prompts from sevenllm_bench.
2025-04-24 04:48:34,231 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 04:48:34,231 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]
Inference Progress:   1%|          | 1/100 [00:04<08:02,  4.88s/it]
Inference Progress:   2%|▏         | 2/100 [00:08<06:50,  4.19s/it]
Inference Progress:   3%|▎         | 3/100 [00:13<07:07,  4.41s/it]
Inference Progress:   4%|▍         | 4/100 [00:16<06:28,  4.05s/it]
Inference Progress:   5%|▌         | 5/100 [00:19<05:47,  3.66s/it]
Inference Progress:   6%|▌         | 6/100 [00:23<05:48,  3.71s/it]
Inference Progress:   7%|▋         | 7/100 [00:29<06:41,  4.32s/it]
Inference Progress:   8%|▊         | 8/100 [00:32<06:03,  3.95s/it]
Inference Progress:   9%|▉         | 9/100 [00:36<05:59,  3.95s/it]
Inference Progress:  10%|█         | 10/100 [00:39<05:26,  3.63s/it]
Inference Progress:  11%|█         | 11/100 [00:46<06:57,  4.69s/it]
Inference Progress:  12%|█▏        | 12/100 [00:50<06:52,  4.69s/it]
Inference Progress:  13%|█▎        | 13/100 [00:54<06:31,  4.50s/it]
Inference Progress:  14%|█▍        | 14/100 [00:57<05:40,  3.96s/it]
Inference Progress:  15%|█▌        | 15/100 [00:59<04:32,  3.21s/it]
Inference Progress:  16%|█▌        | 16/100 [01:01<03:55,  2.81s/it]
Inference Progress:  17%|█▋        | 17/100 [01:02<03:27,  2.50s/it]
Inference Progress:  18%|█▊        | 18/100 [01:06<03:49,  2.80s/it]
Inference Progress:  19%|█▉        | 19/100 [01:12<05:17,  3.92s/it]
Inference Progress:  20%|██        | 20/100 [01:15<04:51,  3.64s/it]
Inference Progress:  21%|██        | 21/100 [01:20<05:19,  4.05s/it]
Inference Progress:  22%|██▏       | 22/100 [01:21<04:07,  3.17s/it]
Inference Progress:  23%|██▎       | 23/100 [01:25<04:23,  3.42s/it]
Inference Progress:  24%|██▍       | 24/100 [01:30<04:39,  3.68s/it]
Inference Progress:  25%|██▌       | 25/100 [01:37<05:52,  4.70s/it]
Inference Progress:  26%|██▌       | 26/100 [01:41<05:34,  4.52s/it]
Inference Progress:  27%|██▋       | 27/100 [01:43<04:45,  3.90s/it]
Inference Progress:  28%|██▊       | 28/100 [01:46<04:22,  3.64s/it]
Inference Progress:  29%|██▉       | 29/100 [01:49<04:05,  3.46s/it]
Inference Progress:  30%|███       | 30/100 [01:53<04:04,  3.49s/it]
Inference Progress:  31%|███       | 31/100 [01:54<03:03,  2.66s/it]
Inference Progress:  32%|███▏      | 32/100 [01:55<02:33,  2.26s/it]
Inference Progress:  33%|███▎      | 33/100 [01:59<03:10,  2.84s/it]
Inference Progress:  34%|███▍      | 34/100 [02:02<03:08,  2.85s/it]
Inference Progress:  35%|███▌      | 35/100 [02:05<03:14,  2.99s/it]
Inference Progress:  36%|███▌      | 36/100 [02:10<03:38,  3.42s/it]
Inference Progress:  37%|███▋      | 37/100 [02:13<03:21,  3.20s/it]
Inference Progress:  38%|███▊      | 38/100 [02:17<03:33,  3.44s/it]
Inference Progress:  39%|███▉      | 39/100 [02:19<03:06,  3.06s/it]
Inference Progress:  40%|████      | 40/100 [02:22<03:06,  3.11s/it]
Inference Progress:  41%|████      | 41/100 [02:28<03:56,  4.01s/it]
Inference Progress:  42%|████▏     | 42/100 [02:30<03:18,  3.42s/it]
Inference Progress:  43%|████▎     | 43/100 [02:32<02:46,  2.92s/it]
Inference Progress:  44%|████▍     | 44/100 [02:35<02:52,  3.09s/it]
Inference Progress:  45%|████▌     | 45/100 [02:42<03:47,  4.14s/it]
Inference Progress:  46%|████▌     | 46/100 [02:48<04:16,  4.75s/it]
Inference Progress:  47%|████▋     | 47/100 [02:49<03:11,  3.62s/it]
Inference Progress:  48%|████▊     | 48/100 [02:50<02:20,  2.71s/it]
Inference Progress:  49%|████▉     | 49/100 [02:56<03:10,  3.73s/it]
Inference Progress:  50%|█████     | 50/100 [02:58<02:40,  3.20s/it]
Inference Progress:  51%|█████     | 51/100 [02:59<02:09,  2.65s/it]
Inference Progress:  52%|█████▏    | 52/100 [03:01<01:55,  2.40s/it]
Inference Progress:  53%|█████▎    | 53/100 [03:05<02:10,  2.79s/it]
Inference Progress:  54%|█████▍    | 54/100 [03:10<02:49,  3.69s/it]
Inference Progress:  55%|█████▌    | 55/100 [03:13<02:36,  3.47s/it]
Inference Progress:  56%|█████▌    | 56/100 [03:19<03:02,  4.14s/it]
Inference Progress:  57%|█████▋    | 57/100 [03:21<02:34,  3.59s/it]
Inference Progress:  58%|█████▊    | 58/100 [03:26<02:39,  3.79s/it]
Inference Progress:  59%|█████▉    | 59/100 [03:29<02:33,  3.75s/it]
Inference Progress:  60%|██████    | 60/100 [03:30<01:53,  2.83s/it]
Inference Progress:  61%|██████    | 61/100 [03:38<02:47,  4.29s/it]
Inference Progress:  62%|██████▏   | 62/100 [03:45<03:22,  5.32s/it]
Inference Progress:  63%|██████▎   | 63/100 [03:48<02:44,  4.44s/it]
Inference Progress:  64%|██████▍   | 64/100 [03:56<03:15,  5.42s/it]
Inference Progress:  65%|██████▌   | 65/100 [04:01<03:08,  5.39s/it]
Inference Progress:  66%|██████▌   | 66/100 [04:07<03:06,  5.48s/it]
Inference Progress:  67%|██████▋   | 67/100 [04:10<02:39,  4.83s/it]
Inference Progress:  68%|██████▊   | 68/100 [04:13<02:14,  4.21s/it]
Inference Progress:  69%|██████▉   | 69/100 [04:19<02:30,  4.86s/it]
Inference Progress:  70%|███████   | 70/100 [04:22<02:09,  4.32s/it]
Inference Progress:  71%|███████   | 71/100 [04:26<02:05,  4.33s/it]
Inference Progress:  72%|███████▏  | 72/100 [04:29<01:49,  3.90s/it]
Inference Progress:  73%|███████▎  | 73/100 [04:37<02:13,  4.96s/it]
Inference Progress:  74%|███████▍  | 74/100 [04:40<01:54,  4.42s/it]
Inference Progress:  75%|███████▌  | 75/100 [04:42<01:30,  3.61s/it]
Inference Progress:  76%|███████▌  | 76/100 [04:47<01:42,  4.29s/it]
Inference Progress:  77%|███████▋  | 77/100 [04:54<01:52,  4.89s/it]
Inference Progress:  78%|███████▊  | 78/100 [04:56<01:28,  4.03s/it]
Inference Progress:  79%|███████▉  | 79/100 [04:58<01:14,  3.57s/it]
Inference Progress:  80%|████████  | 80/100 [05:00<01:01,  3.08s/it]
Inference Progress:  81%|████████  | 81/100 [05:06<01:12,  3.83s/it]
Inference Progress:  82%|████████▏ | 82/100 [05:14<01:30,  5.02s/it]
Inference Progress:  83%|████████▎ | 83/100 [05:15<01:06,  3.92s/it]
Inference Progress:  84%|████████▍ | 84/100 [05:22<01:16,  4.79s/it]
Inference Progress:  85%|████████▌ | 85/100 [05:26<01:08,  4.57s/it]
Inference Progress:  86%|████████▌ | 86/100 [05:28<00:51,  3.69s/it]
Inference Progress:  87%|████████▋ | 87/100 [05:31<00:45,  3.52s/it]
Inference Progress:  88%|████████▊ | 88/100 [05:32<00:32,  2.74s/it]
Inference Progress:  89%|████████▉ | 89/100 [05:36<00:34,  3.11s/it]
Inference Progress:  90%|█████████ | 90/100 [05:38<00:30,  3.01s/it]
Inference Progress:  91%|█████████ | 91/100 [05:42<00:27,  3.11s/it]
Inference Progress:  92%|█████████▏| 92/100 [05:43<00:19,  2.50s/it]
Inference Progress:  93%|█████████▎| 93/100 [05:44<00:15,  2.22s/it]
Inference Progress:  94%|█████████▍| 94/100 [05:46<00:12,  2.11s/it]
Inference Progress:  95%|█████████▌| 95/100 [05:50<00:13,  2.60s/it]
Inference Progress:  96%|█████████▌| 96/100 [05:53<00:10,  2.73s/it]
Inference Progress:  97%|█████████▋| 97/100 [05:57<00:09,  3.09s/it]
Inference Progress:  98%|█████████▊| 98/100 [06:04<00:08,  4.30s/it]
Inference Progress:  99%|█████████▉| 99/100 [06:09<00:04,  4.66s/it]
Inference Progress: 100%|██████████| 100/100 [06:15<00:00,  4.94s/it]
Inference Progress: 100%|██████████| 100/100 [06:15<00:00,  3.76s/it]
2025-04-24 04:54:49,787 - INFO - 
--- Inference Summary ---
2025-04-24 04:54:49,787 - INFO - Processed 100 samples.
2025-04-24 04:54:49,787 - INFO - Total 'generate' time (sum): 375.39 sec
2025-04-24 04:54:49,787 - INFO - Overall inference loop duration: 375.55 sec
2025-04-24 04:54:49,787 - INFO - Average 'generate' time per sample: 3.7539 sec
2025-04-24 04:54:49,787 - INFO - Total effective tokens generated: 25293
2025-04-24 04:54:49,787 - INFO - Overall effective tokens per second: 67.38
2025-04-24 04:54:49,787 - INFO - RAM Delta during inference: 684.07 MB
2025-04-24 04:54:49,787 - INFO - PyTorch VRAM Peak Delta during inference: 259.83 MB
2025-04-24 04:54:49,789 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/sevenllm_bench__20250424-044830_outputs.json
2025-04-24 04:54:49,789 - INFO - Saved metrics to: /workspace/results/mod/48/sevenllm_bench__20250424-044830_metrics.json
2025-04-24 04:54:49,789 - INFO - Cleaning up resources...
2025-04-24 04:54:49,795 - INFO - CUDA cache cleared.
2025-04-24 04:54:49,795 - INFO - --- Evaluation Complete ---

2025-04-24 04:54:50,695 - INFO - SUCCESS: Run 14/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 04:54:50,695 - INFO - 
Run 15/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='attack'
2025-04-24 04:54:50,695 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset attack --max-new-tokens 512
2025-04-24 04:54:58,436 - WARNING - Stderr:
2025-04-24 04:54:52,820 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:54:53,133 - INFO - --- Starting Evaluation ---
2025-04-24 04:54:53,133 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'attack'}
2025-04-24 04:54:53,133 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:54:53,133 - INFO - Metrics file: /workspace/results/mod/48/ctibench_attack__20250424-045453_metrics.json
2025-04-24 04:54:53,133 - INFO - Outputs file: /workspace/results/mod/48/ctibench_attack__20250424-045453_outputs.json
2025-04-24 04:54:53,133 - INFO - NVML Reporting Active: False
2025-04-24 04:54:53,133 - INFO - Initial RAM usage: 553.80 MB
2025-04-24 04:54:53,133 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/...
2025-04-24 04:54:53,153 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:54:53,449 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:54:56,937 - INFO - CausalLM Model loaded.
2025-04-24 04:54:56,938 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:54:56,938 - INFO - Model loaded in 3.80 seconds.
2025-04-24 04:54:56,938 - INFO - Loading prompts from CTIBench subset: attack
2025-04-24 04:54:57,643 - ERROR - Error loading/processing ctibench (attack): BuilderConfig 'attack' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'attack' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:54:57,644 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:54:57,644 - INFO - Cleaning up resources...
2025-04-24 04:54:57,645 - INFO - CUDA cache cleared.
2025-04-24 04:54:57,645 - INFO - --- Evaluation Complete ---

2025-04-24 04:54:58,437 - INFO - SUCCESS: Run 15/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='attack'. Check output files in /workspace/results/mod/48/
2025-04-24 04:54:58,437 - INFO - 
Run 16/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='exploit_target'
2025-04-24 04:54:58,437 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset exploit_target --max-new-tokens 512
2025-04-24 04:55:06,492 - WARNING - Stderr:
2025-04-24 04:55:00,608 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:55:00,917 - INFO - --- Starting Evaluation ---
2025-04-24 04:55:00,917 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'exploit_target'}
2025-04-24 04:55:00,917 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:55:00,917 - INFO - Metrics file: /workspace/results/mod/48/ctibench_exploit_target__20250424-045500_metrics.json
2025-04-24 04:55:00,917 - INFO - Outputs file: /workspace/results/mod/48/ctibench_exploit_target__20250424-045500_outputs.json
2025-04-24 04:55:00,917 - INFO - NVML Reporting Active: False
2025-04-24 04:55:00,917 - INFO - Initial RAM usage: 557.02 MB
2025-04-24 04:55:00,917 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/...
2025-04-24 04:55:00,937 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:55:01,238 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:55:04,834 - INFO - CausalLM Model loaded.
2025-04-24 04:55:04,835 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:55:04,835 - INFO - Model loaded in 3.92 seconds.
2025-04-24 04:55:04,835 - INFO - Loading prompts from CTIBench subset: exploit_target
2025-04-24 04:55:05,652 - ERROR - Error loading/processing ctibench (exploit_target): BuilderConfig 'exploit_target' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'exploit_target' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:55:05,653 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:55:05,653 - INFO - Cleaning up resources...
2025-04-24 04:55:05,653 - INFO - CUDA cache cleared.
2025-04-24 04:55:05,653 - INFO - --- Evaluation Complete ---

2025-04-24 04:55:06,492 - INFO - SUCCESS: Run 16/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='exploit_target'. Check output files in /workspace/results/mod/48/
2025-04-24 04:55:06,492 - INFO - 
Run 17/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='threat_actor'
2025-04-24 04:55:06,492 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset threat_actor --max-new-tokens 512
2025-04-24 04:55:15,168 - WARNING - Stderr:
2025-04-24 04:55:08,621 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:55:08,933 - INFO - --- Starting Evaluation ---
2025-04-24 04:55:08,934 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'threat_actor'}
2025-04-24 04:55:08,934 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:55:08,934 - INFO - Metrics file: /workspace/results/mod/48/ctibench_threat_actor__20250424-045508_metrics.json
2025-04-24 04:55:08,934 - INFO - Outputs file: /workspace/results/mod/48/ctibench_threat_actor__20250424-045508_outputs.json
2025-04-24 04:55:08,934 - INFO - NVML Reporting Active: False
2025-04-24 04:55:08,934 - INFO - Initial RAM usage: 556.78 MB
2025-04-24 04:55:08,934 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/...
2025-04-24 04:55:08,954 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:55:09,254 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:55:13,103 - INFO - CausalLM Model loaded.
2025-04-24 04:55:13,104 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:55:13,104 - INFO - Model loaded in 4.17 seconds.
2025-04-24 04:55:13,104 - INFO - Loading prompts from CTIBench subset: threat_actor
2025-04-24 04:55:14,384 - ERROR - Error loading/processing ctibench (threat_actor): BuilderConfig 'threat_actor' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'threat_actor' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:55:14,385 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:55:14,386 - INFO - Cleaning up resources...
2025-04-24 04:55:14,386 - INFO - CUDA cache cleared.
2025-04-24 04:55:14,386 - INFO - --- Evaluation Complete ---

2025-04-24 04:55:15,168 - INFO - SUCCESS: Run 17/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='threat_actor'. Check output files in /workspace/results/mod/48/
2025-04-24 04:55:15,168 - INFO - 
Run 18/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='ttps'
2025-04-24 04:55:15,168 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset ttps --max-new-tokens 512
2025-04-24 04:55:22,827 - WARNING - Stderr:
2025-04-24 04:55:17,303 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 04:55:17,605 - INFO - --- Starting Evaluation ---
2025-04-24 04:55:17,606 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'ttps'}
2025-04-24 04:55:17,606 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 04:55:17,606 - INFO - Metrics file: /workspace/results/mod/48/ctibench_ttps__20250424-045517_metrics.json
2025-04-24 04:55:17,606 - INFO - Outputs file: /workspace/results/mod/48/ctibench_ttps__20250424-045517_outputs.json
2025-04-24 04:55:17,606 - INFO - NVML Reporting Active: False
2025-04-24 04:55:17,606 - INFO - Initial RAM usage: 556.89 MB
2025-04-24 04:55:17,606 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/...
2025-04-24 04:55:17,626 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 04:55:17,922 - INFO - Tokenizer loaded.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 04:55:21,329 - INFO - CausalLM Model loaded.
2025-04-24 04:55:21,330 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 04:55:21,330 - INFO - Model loaded in 3.72 seconds.
2025-04-24 04:55:21,330 - INFO - Loading prompts from CTIBench subset: ttps
2025-04-24 04:55:21,970 - ERROR - Error loading/processing ctibench (ttps): BuilderConfig 'ttps' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 102, in load_benchmark_prompts
    dataset = load_dataset('AI4Sec/cti-bench', name=cti_subset, cache_dir=dataset_cache_dir)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 343, in __init__
    self.config, self.config_id = self._create_builder_config(
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/datasets/builder.py", line 570, in _create_builder_config
    raise ValueError(
ValueError: BuilderConfig 'ttps' not found. Available: ['cti-mcq', 'cti-rcm', 'cti-vsp', 'cti-taa', 'cti-ate', 'cti-rcm-2021']
2025-04-24 04:55:21,971 - ERROR - An error occurred during evaluation pipeline: Prompt loading failed (returned None)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 79, in main
    if prompts_data is None: raise RuntimeError("Prompt loading failed (returned None)")
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Prompt loading failed (returned None)
2025-04-24 04:55:21,971 - INFO - Cleaning up resources...
2025-04-24 04:55:21,971 - INFO - CUDA cache cleared.
2025-04-24 04:55:21,971 - INFO - --- Evaluation Complete ---

2025-04-24 04:55:22,827 - INFO - SUCCESS: Run 18/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='ttps'. Check output files in /workspace/results/mod/48/
2025-04-24 04:55:22,827 - INFO - --- Security Benchmark Automation Finished (QUANTIZED MODELS) ---
2025-04-24 07:32:31,796 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 07:32:31,797 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 07:32:31,798 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 07:32:31,799 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 07:32:31,800 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 07:32:31,800 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_073231.log
2025-04-24 07:32:31,810 - INFO - Models to run: ['TinyLlama-1.1B-Chat-v1.0-GPTQ', 'Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 07:32:31,811 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 07:32:31,812 - INFO - Running 100 samples per benchmark.
2025-04-24 07:32:31,813 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 07:32:31,813 - INFO - 
Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 07:32:31,815 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 07:33:27,535 - WARNING - Stderr:
2025-04-24 07:33:21,110 - INFO - Initialized NVML for device 0.
2025-04-24 07:33:21,271 - INFO - --- Starting Evaluation ---
2025-04-24 07:33:21,271 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 07:33:21,271 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 07:33:21,271 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-073321_metrics.json
2025-04-24 07:33:21,271 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-073321_outputs.json
2025-04-24 07:33:21,271 - INFO - NVML Reporting Active: True
2025-04-24 07:33:21,271 - INFO - Initial RAM usage: 661.26 MB
2025-04-24 07:33:21,271 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 07:33:21,271 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 07:33:21,284 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 07:33:21,606 - INFO - Tokenizer loaded.
2025-04-24 07:33:25,124 - ERROR - Error loading model: Loading a GPTQ quantized model requires the auto-gptq library (`pip install auto-gptq`)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 3620, in from_pretrained
    hf_quantizer.validate_environment(
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/quantizer_gptq.py", line 59, in validate_environment
    raise ImportError(
ImportError: Loading a GPTQ quantized model requires the auto-gptq library (`pip install auto-gptq`)
2025-04-24 07:33:25,145 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 07:33:25,146 - INFO - Cleaning up resources...
2025-04-24 07:33:25,146 - INFO - CUDA cache cleared.
2025-04-24 07:33:25,147 - INFO - NVML shut down.
2025-04-24 07:33:25,147 - INFO - --- Evaluation Complete ---

2025-04-24 07:33:27,537 - INFO - SUCCESS: Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 07:33:27,538 - INFO - 
Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 07:33:27,539 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 07:33:56,778 - WARNING - Stderr:
2025-04-24 07:33:52,850 - INFO - Initialized NVML for device 0.
2025-04-24 07:33:52,998 - INFO - --- Starting Evaluation ---
2025-04-24 07:33:52,999 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 07:33:52,999 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 07:33:52,999 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-073352_metrics.json
2025-04-24 07:33:52,999 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-073352_outputs.json
2025-04-24 07:33:52,999 - INFO - NVML Reporting Active: True
2025-04-24 07:33:52,999 - INFO - Initial RAM usage: 665.44 MB
2025-04-24 07:33:52,999 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 07:33:52,999 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 07:33:53,003 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 07:33:53,160 - INFO - Tokenizer loaded.
2025-04-24 07:33:56,058 - ERROR - Error loading model: Loading a GPTQ quantized model requires the auto-gptq library (`pip install auto-gptq`)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 3620, in from_pretrained
    hf_quantizer.validate_environment(
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/quantizer_gptq.py", line 59, in validate_environment
    raise ImportError(
ImportError: Loading a GPTQ quantized model requires the auto-gptq library (`pip install auto-gptq`)
2025-04-24 07:33:56,068 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 07:33:56,069 - INFO - Cleaning up resources...
2025-04-24 07:33:56,070 - INFO - CUDA cache cleared.
2025-04-24 07:33:56,070 - INFO - NVML shut down.
2025-04-24 07:33:56,070 - INFO - --- Evaluation Complete ---

2025-04-24 07:33:56,780 - INFO - SUCCESS: Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 07:33:56,781 - INFO - 
Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 07:33:56,782 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 07:34:26,724 - WARNING - Stderr:
2025-04-24 07:34:22,735 - INFO - Initialized NVML for device 0.
2025-04-24 07:34:22,882 - INFO - --- Starting Evaluation ---
2025-04-24 07:34:22,882 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq'}
2025-04-24 07:34:22,883 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 07:34:22,883 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-073422_metrics.json
2025-04-24 07:34:22,883 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-073422_outputs.json
2025-04-24 07:34:22,883 - INFO - NVML Reporting Active: True
2025-04-24 07:34:22,883 - INFO - Initial RAM usage: 668.48 MB
2025-04-24 07:34:22,883 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 07:34:22,883 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 07:34:22,887 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 07:34:23,055 - INFO - Tokenizer loaded.
2025-04-24 07:34:25,977 - ERROR - Error loading model: Loading a GPTQ quantized model requires the auto-gptq library (`pip install auto-gptq`)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 3620, in from_pretrained
    hf_quantizer.validate_environment(
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/quantizer_gptq.py", line 59, in validate_environment
    raise ImportError(
ImportError: Loading a GPTQ quantized model requires the auto-gptq library (`pip install auto-gptq`)
2025-04-24 07:34:25,988 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 07:34:25,989 - INFO - Cleaning up resources...
2025-04-24 07:34:25,989 - INFO - CUDA cache cleared.
2025-04-24 07:34:25,989 - INFO - NVML shut down.
2025-04-24 07:34:25,989 - INFO - --- Evaluation Complete ---

2025-04-24 07:34:26,726 - INFO - SUCCESS: Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 07:34:26,727 - INFO - 
Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 07:34:26,728 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 07:34:56,313 - WARNING - Stderr:
2025-04-24 07:34:52,306 - INFO - Initialized NVML for device 0.
2025-04-24 07:34:52,452 - INFO - --- Starting Evaluation ---
2025-04-24 07:34:52,452 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-vsp'}
2025-04-24 07:34:52,452 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 07:34:52,453 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-073452_metrics.json
2025-04-24 07:34:52,453 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-073452_outputs.json
2025-04-24 07:34:52,453 - INFO - NVML Reporting Active: True
2025-04-24 07:34:52,453 - INFO - Initial RAM usage: 668.18 MB
2025-04-24 07:34:52,453 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 07:34:52,453 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 07:34:52,457 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 07:34:52,596 - INFO - Tokenizer loaded.
2025-04-24 07:34:55,577 - ERROR - Error loading model: Loading a GPTQ quantized model requires the auto-gptq library (`pip install auto-gptq`)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 3620, in from_pretrained
    hf_quantizer.validate_environment(
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/quantizer_gptq.py", line 59, in validate_environment
    raise ImportError(
ImportError: Loading a GPTQ quantized model requires the auto-gptq library (`pip install auto-gptq`)
2025-04-24 07:34:55,588 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 07:34:55,589 - INFO - Cleaning up resources...
2025-04-24 07:34:55,589 - INFO - CUDA cache cleared.
2025-04-24 07:34:55,589 - INFO - NVML shut down.
2025-04-24 07:34:55,589 - INFO - --- Evaluation Complete ---

2025-04-24 07:34:56,315 - INFO - SUCCESS: Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'. Check output files in /workspace/results/mod/48/
2025-04-24 07:34:56,315 - INFO - 
Run 5/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 07:34:56,317 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 07:36:08,554 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 07:36:08,555 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 07:36:08,557 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 07:36:08,557 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 07:36:08,558 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 07:36:08,558 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_073608.log
2025-04-24 07:36:08,559 - INFO - Models to run: ['TinyLlama-1.1B-Chat-v1.0-GPTQ', 'Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 07:36:08,560 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 07:36:08,560 - INFO - Running 100 samples per benchmark.
2025-04-24 07:36:08,561 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 07:36:08,562 - INFO - 
Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 07:36:08,564 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 07:42:50,020 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 07:42:50,021 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 07:42:50,022 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 07:42:50,023 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 07:42:50,023 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 07:42:50,024 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_074250.log
2025-04-24 07:42:50,024 - INFO - Models to run: ['TinyLlama-1.1B-Chat-v1.0-GPTQ', 'Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 07:42:50,025 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 07:42:50,026 - INFO - Running 100 samples per benchmark.
2025-04-24 07:42:50,027 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 07:42:50,027 - INFO - 
Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 07:42:50,029 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 08:12:09,665 - WARNING - Stderr:
2025-04-24 07:43:17,006 - INFO - Initialized NVML for device 0.
2025-04-24 07:43:17,155 - INFO - --- Starting Evaluation ---
2025-04-24 07:43:17,155 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 07:43:17,155 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 07:43:17,155 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-074317_metrics.json
2025-04-24 07:43:17,155 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-074317_outputs.json
2025-04-24 07:43:17,155 - INFO - NVML Reporting Active: True
2025-04-24 07:43:17,155 - INFO - Initial RAM usage: 671.55 MB
2025-04-24 07:43:17,155 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 07:43:17,155 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 07:43:17,159 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 07:43:17,311 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 07:43:21,101 - WARNING - CUDA extension not installed.
2025-04-24 07:43:21,116 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 07:43:23,403 - INFO - CausalLM Model loaded.
2025-04-24 07:43:23,404 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 07:43:23,404 - INFO - Model loaded in 6.25 seconds.
2025-04-24 07:43:23,404 - INFO - Loading prompts from CyberSecEval3 MITRE file: /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json
2025-04-24 07:43:23,422 - INFO - Processing the first 100 items.

Parsing MITRE prompts:   0%|          | 0/100 [00:00<?, ?it/s]2025-04-24 07:43:23,424 - WARNING - Item 5: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,424 - WARNING - Item 8: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,425 - WARNING - Item 14: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,425 - WARNING - Item 21: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,426 - WARNING - Item 25: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,426 - WARNING - Item 27: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,426 - WARNING - Item 30: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,426 - WARNING - Item 32: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,427 - WARNING - Item 35: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,427 - WARNING - Item 36: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,427 - WARNING - Item 48: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,427 - WARNING - Item 49: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,428 - WARNING - Item 52: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,428 - WARNING - Item 58: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,428 - WARNING - Item 63: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,429 - WARNING - Item 72: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,429 - WARNING - Item 81: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,430 - WARNING - Item 86: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 07:43:23,430 - WARNING - Item 88: Regex failed to find/extract 'prompt' value from mutated_prompt_str.

Parsing MITRE prompts: 100%|██████████| 100/100 [00:00<00:00, 14194.40it/s]
2025-04-24 07:43:23,431 - INFO - Successfully loaded 81 prompts from cyberseceval3_mitre.
2025-04-24 07:43:23,431 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 07:43:23,432 - INFO - Starting inference on 81 samples...

Inference Progress:   0%|          | 0/81 [00:00<?, ?it/s]
Inference Progress:   1%|          | 1/81 [00:22<30:21, 22.77s/it]
Inference Progress:   2%|▏         | 2/81 [00:45<29:36, 22.49s/it]
Inference Progress:   4%|▎         | 3/81 [01:07<29:11, 22.46s/it]
Inference Progress:   5%|▍         | 4/81 [01:23<25:26, 19.82s/it]
Inference Progress:   6%|▌         | 5/81 [01:39<23:18, 18.40s/it]
Inference Progress:   7%|▋         | 6/81 [01:54<21:46, 17.42s/it]
Inference Progress:   9%|▊         | 7/81 [02:16<23:27, 19.02s/it]
Inference Progress:  10%|▉         | 8/81 [02:39<24:25, 20.07s/it]
Inference Progress:  11%|█         | 9/81 [03:01<24:54, 20.76s/it]
Inference Progress:  12%|█▏        | 10/81 [03:23<25:07, 21.24s/it]
Inference Progress:  14%|█▎        | 11/81 [03:41<23:31, 20.17s/it]
Inference Progress:  15%|█▍        | 12/81 [04:03<23:57, 20.83s/it]
Inference Progress:  16%|█▌        | 13/81 [04:26<24:05, 21.25s/it]
Inference Progress:  17%|█▋        | 14/81 [04:48<24:03, 21.54s/it]
Inference Progress:  19%|█▊        | 15/81 [05:10<23:54, 21.74s/it]
Inference Progress:  20%|█▉        | 16/81 [05:30<23:03, 21.29s/it]
Inference Progress:  21%|██        | 17/81 [05:53<23:03, 21.62s/it]
Inference Progress:  22%|██▏       | 18/81 [06:15<22:52, 21.79s/it]
Inference Progress:  23%|██▎       | 19/81 [06:37<22:43, 21.99s/it]
Inference Progress:  25%|██▍       | 20/81 [07:00<22:30, 22.14s/it]
Inference Progress:  26%|██▌       | 21/81 [07:22<22:10, 22.18s/it]
Inference Progress:  27%|██▋       | 22/81 [07:44<21:49, 22.19s/it]
Inference Progress:  28%|██▊       | 23/81 [08:07<21:26, 22.19s/it]
Inference Progress:  30%|██▉       | 24/81 [08:29<21:04, 22.19s/it]
Inference Progress:  31%|███       | 25/81 [08:51<20:43, 22.20s/it]
Inference Progress:  32%|███▏      | 26/81 [09:13<20:21, 22.20s/it]
Inference Progress:  33%|███▎      | 27/81 [09:35<19:57, 22.18s/it]
Inference Progress:  35%|███▍      | 28/81 [09:57<19:34, 22.16s/it]
Inference Progress:  36%|███▌      | 29/81 [10:20<19:12, 22.16s/it]
Inference Progress:  37%|███▋      | 30/81 [10:42<18:50, 22.16s/it]
Inference Progress:  38%|███▊      | 31/81 [11:04<18:29, 22.19s/it]
Inference Progress:  40%|███▉      | 32/81 [11:18<16:11, 19.83s/it]
Inference Progress:  41%|████      | 33/81 [11:40<16:26, 20.55s/it]
Inference Progress:  42%|████▏     | 34/81 [12:03<16:29, 21.05s/it]
Inference Progress:  43%|████▎     | 35/81 [12:25<16:24, 21.41s/it]
Inference Progress:  44%|████▍     | 36/81 [12:47<16:14, 21.65s/it]
Inference Progress:  46%|████▌     | 37/81 [13:09<16:00, 21.82s/it]
Inference Progress:  47%|████▋     | 38/81 [13:27<14:40, 20.48s/it]
Inference Progress:  48%|████▊     | 39/81 [13:49<14:41, 21.00s/it]
Inference Progress:  49%|████▉     | 40/81 [14:11<14:36, 21.38s/it]
Inference Progress:  51%|█████     | 41/81 [14:33<14:25, 21.64s/it]
Inference Progress:  52%|█████▏    | 42/81 [14:56<14:11, 21.83s/it]
Inference Progress:  53%|█████▎    | 43/81 [15:18<13:53, 21.95s/it]
Inference Progress:  54%|█████▍    | 44/81 [15:41<13:38, 22.13s/it]
Inference Progress:  56%|█████▌    | 45/81 [16:03<13:18, 22.19s/it]
Inference Progress:  57%|█████▋    | 46/81 [16:25<12:57, 22.21s/it]
Inference Progress:  58%|█████▊    | 47/81 [16:47<12:34, 22.19s/it]
Inference Progress:  59%|█████▉    | 48/81 [17:10<12:13, 22.21s/it]
Inference Progress:  60%|██████    | 49/81 [17:18<09:43, 18.22s/it]
Inference Progress:  62%|██████▏   | 50/81 [17:41<10:02, 19.45s/it]
Inference Progress:  63%|██████▎   | 51/81 [18:03<10:08, 20.27s/it]
Inference Progress:  64%|██████▍   | 52/81 [18:25<10:04, 20.85s/it]
Inference Progress:  65%|██████▌   | 53/81 [18:47<09:55, 21.27s/it]
Inference Progress:  67%|██████▋   | 54/81 [19:10<09:41, 21.54s/it]
Inference Progress:  68%|██████▊   | 55/81 [19:32<09:25, 21.74s/it]
Inference Progress:  69%|██████▉   | 56/81 [19:54<09:06, 21.86s/it]
Inference Progress:  70%|███████   | 57/81 [20:16<08:47, 21.97s/it]
Inference Progress:  72%|███████▏  | 58/81 [20:33<07:52, 20.55s/it]
Inference Progress:  73%|███████▎  | 59/81 [20:56<07:43, 21.05s/it]
Inference Progress:  74%|███████▍  | 60/81 [21:18<07:29, 21.39s/it]
Inference Progress:  75%|███████▌  | 61/81 [21:40<07:13, 21.66s/it]
Inference Progress:  77%|███████▋  | 62/81 [22:02<06:54, 21.81s/it]
Inference Progress:  78%|███████▊  | 63/81 [22:24<06:34, 21.92s/it]
Inference Progress:  79%|███████▉  | 64/81 [22:47<06:14, 22.04s/it]
Inference Progress:  80%|████████  | 65/81 [23:09<05:55, 22.25s/it]
Inference Progress:  81%|████████▏ | 66/81 [23:24<04:59, 19.95s/it]
Inference Progress:  83%|████████▎ | 67/81 [23:41<04:25, 18.93s/it]
Inference Progress:  84%|████████▍ | 68/81 [24:03<04:19, 19.94s/it]
Inference Progress:  85%|████████▌ | 69/81 [24:25<04:07, 20.61s/it]
Inference Progress:  86%|████████▋ | 70/81 [24:47<03:52, 21.10s/it]
Inference Progress:  88%|████████▊ | 71/81 [25:10<03:34, 21.46s/it]
Inference Progress:  89%|████████▉ | 72/81 [25:32<03:15, 21.71s/it]
Inference Progress:  90%|█████████ | 73/81 [25:54<02:54, 21.86s/it]
Inference Progress:  91%|█████████▏| 74/81 [26:16<02:33, 21.95s/it]
Inference Progress:  93%|█████████▎| 75/81 [26:39<02:12, 22.06s/it]
Inference Progress:  94%|█████████▍| 76/81 [27:01<01:50, 22.12s/it]
Inference Progress:  95%|█████████▌| 77/81 [27:23<01:28, 22.17s/it]
Inference Progress:  96%|█████████▋| 78/81 [27:45<01:06, 22.19s/it]
Inference Progress:  98%|█████████▊| 79/81 [28:08<00:44, 22.22s/it]
Inference Progress:  99%|█████████▉| 80/81 [28:30<00:22, 22.21s/it]
Inference Progress: 100%|██████████| 81/81 [28:45<00:00, 20.05s/it]
Inference Progress: 100%|██████████| 81/81 [28:45<00:00, 21.30s/it]
2025-04-24 08:12:08,762 - INFO - 
--- Inference Summary ---
2025-04-24 08:12:08,762 - INFO - Processed 81 samples.
2025-04-24 08:12:08,762 - INFO - Total 'generate' time (sum): 1725.15 sec
2025-04-24 08:12:08,762 - INFO - Overall inference loop duration: 1725.33 sec
2025-04-24 08:12:08,762 - INFO - Average 'generate' time per sample: 21.2982 sec
2025-04-24 08:12:08,762 - INFO - Total effective tokens generated: 39670
2025-04-24 08:12:08,762 - INFO - Overall effective tokens per second: 23.00
2025-04-24 08:12:08,762 - INFO - RAM Delta during inference: 498.75 MB
2025-04-24 08:12:08,762 - INFO - PyTorch VRAM Peak Delta during inference: 87.98 MB
2025-04-24 08:12:08,762 - INFO - System VRAM Peak Approx. Delta during inference: -1605.38 MB
2025-04-24 08:12:08,770 - INFO - Saved outputs (81 items) to: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-074317_outputs.json
2025-04-24 08:12:08,773 - INFO - Saved metrics to: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-074317_metrics.json
2025-04-24 08:12:08,774 - INFO - Cleaning up resources...
2025-04-24 08:12:08,777 - INFO - CUDA cache cleared.
2025-04-24 08:12:08,777 - INFO - NVML shut down.
2025-04-24 08:12:08,777 - INFO - --- Evaluation Complete ---

2025-04-24 08:12:09,667 - INFO - SUCCESS: Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 08:12:09,668 - INFO - 
Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 08:12:09,669 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 08:37:26,153 - WARNING - Stderr:
2025-04-24 08:12:54,601 - INFO - Initialized NVML for device 0.
2025-04-24 08:12:54,742 - INFO - --- Starting Evaluation ---
2025-04-24 08:12:54,742 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 08:12:54,742 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 08:12:54,742 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-081254_metrics.json
2025-04-24 08:12:54,742 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-081254_outputs.json
2025-04-24 08:12:54,742 - INFO - NVML Reporting Active: True
2025-04-24 08:12:54,742 - INFO - Initial RAM usage: 659.71 MB
2025-04-24 08:12:54,742 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 08:12:54,742 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 08:12:54,750 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 08:12:55,064 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 08:12:59,820 - WARNING - CUDA extension not installed.
2025-04-24 08:12:59,827 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 08:13:08,522 - INFO - CausalLM Model loaded.
2025-04-24 08:13:08,523 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 08:13:08,523 - INFO - Model loaded in 13.78 seconds.
2025-04-24 08:13:08,523 - INFO - Loading prompts from SEvenLLM saved dataset directory: /workspace/calibration_data/sevenllm_instruct_subset_manual/
2025-04-24 08:13:08,579 - INFO - Loaded dataset: Dataset({
    features: ['output', 'category', 'thought', 'input', 'id', 'instruction'],
    num_rows: 200
})
2025-04-24 08:13:08,582 - INFO - Selected first 100 samples for evaluation.

Parsing SEvenLLM prompts:   0%|          | 0/100 [00:00<?, ?it/s]
Parsing SEvenLLM prompts: 100%|██████████| 100/100 [00:00<00:00, 4526.60it/s]
2025-04-24 08:13:08,605 - INFO - Successfully loaded 100 prompts from sevenllm_bench.
2025-04-24 08:13:08,605 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 08:13:08,606 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]
Inference Progress:   1%|          | 1/100 [00:24<40:52, 24.78s/it]
Inference Progress:   2%|▏         | 2/100 [00:33<25:15, 15.47s/it]
Inference Progress:   3%|▎         | 3/100 [00:51<26:59, 16.69s/it]
Inference Progress:   4%|▍         | 4/100 [01:14<30:10, 18.86s/it]
Inference Progress:   5%|▌         | 5/100 [01:21<23:06, 14.60s/it]
Inference Progress:   6%|▌         | 6/100 [01:43<26:54, 17.18s/it]
Inference Progress:   7%|▋         | 7/100 [01:52<22:25, 14.46s/it]
Inference Progress:   8%|▊         | 8/100 [02:01<19:47, 12.91s/it]
Inference Progress:   9%|▉         | 9/100 [02:07<15:58, 10.53s/it]
Inference Progress:  10%|█         | 10/100 [02:29<21:12, 14.14s/it]
Inference Progress:  11%|█         | 11/100 [02:44<21:30, 14.50s/it]
Inference Progress:  12%|█▏        | 12/100 [03:06<24:41, 16.83s/it]
Inference Progress:  13%|█▎        | 13/100 [03:12<19:32, 13.47s/it]
Inference Progress:  14%|█▍        | 14/100 [03:28<20:20, 14.19s/it]
Inference Progress:  15%|█▌        | 15/100 [03:39<18:35, 13.12s/it]
Inference Progress:  16%|█▌        | 16/100 [03:52<18:33, 13.26s/it]
Inference Progress:  17%|█▋        | 17/100 [04:14<22:01, 15.93s/it]
Inference Progress:  18%|█▊        | 18/100 [04:36<24:18, 17.79s/it]
Inference Progress:  19%|█▉        | 19/100 [04:47<20:56, 15.51s/it]
Inference Progress:  20%|██        | 20/100 [05:09<23:19, 17.50s/it]
Inference Progress:  21%|██        | 21/100 [05:18<19:38, 14.91s/it]
Inference Progress:  22%|██▏       | 22/100 [05:40<22:12, 17.09s/it]
Inference Progress:  23%|██▎       | 23/100 [05:48<18:27, 14.38s/it]
Inference Progress:  24%|██▍       | 24/100 [05:57<16:24, 12.95s/it]
Inference Progress:  25%|██▌       | 25/100 [06:02<13:12, 10.57s/it]
Inference Progress:  26%|██▌       | 26/100 [06:23<16:35, 13.45s/it]
Inference Progress:  27%|██▋       | 27/100 [06:32<15:02, 12.36s/it]
Inference Progress:  28%|██▊       | 28/100 [06:40<13:15, 11.05s/it]
Inference Progress:  29%|██▉       | 29/100 [07:03<17:00, 14.38s/it]
Inference Progress:  30%|███       | 30/100 [07:10<14:26, 12.37s/it]
Inference Progress:  31%|███       | 31/100 [07:32<17:36, 15.30s/it]
Inference Progress:  32%|███▏      | 32/100 [07:40<14:48, 13.07s/it]
Inference Progress:  33%|███▎      | 33/100 [07:50<13:34, 12.15s/it]
Inference Progress:  34%|███▍      | 34/100 [08:02<13:04, 11.89s/it]
Inference Progress:  35%|███▌      | 35/100 [08:23<15:56, 14.72s/it]
Inference Progress:  36%|███▌      | 36/100 [08:44<17:40, 16.57s/it]
Inference Progress:  37%|███▋      | 37/100 [09:01<17:38, 16.80s/it]
Inference Progress:  38%|███▊      | 38/100 [09:08<14:13, 13.76s/it]
Inference Progress:  39%|███▉      | 39/100 [09:30<16:32, 16.27s/it]
Inference Progress:  40%|████      | 40/100 [09:41<14:36, 14.60s/it]
Inference Progress:  41%|████      | 41/100 [09:49<12:33, 12.77s/it]
Inference Progress:  42%|████▏     | 42/100 [09:54<10:04, 10.43s/it]
Inference Progress:  43%|████▎     | 43/100 [10:16<13:14, 13.94s/it]
Inference Progress:  44%|████▍     | 44/100 [10:38<15:19, 16.41s/it]
Inference Progress:  45%|████▌     | 45/100 [10:48<13:18, 14.51s/it]
Inference Progress:  46%|████▌     | 46/100 [11:11<15:08, 16.82s/it]
Inference Progress:  47%|████▋     | 47/100 [11:23<13:42, 15.52s/it]
Inference Progress:  48%|████▊     | 48/100 [11:45<15:10, 17.51s/it]
Inference Progress:  49%|████▉     | 49/100 [11:59<13:52, 16.33s/it]
Inference Progress:  50%|█████     | 50/100 [12:21<15:03, 18.07s/it]
Inference Progress:  51%|█████     | 51/100 [12:43<15:45, 19.29s/it]
Inference Progress:  52%|█████▏    | 52/100 [13:05<16:06, 20.15s/it]
Inference Progress:  53%|█████▎    | 53/100 [13:28<16:17, 20.81s/it]
Inference Progress:  54%|█████▍    | 54/100 [13:50<16:16, 21.22s/it]
Inference Progress:  55%|█████▌    | 55/100 [13:55<12:24, 16.54s/it]
Inference Progress:  56%|█████▌    | 56/100 [14:00<09:24, 12.83s/it]
Inference Progress:  57%|█████▋    | 57/100 [14:06<07:47, 10.88s/it]
Inference Progress:  58%|█████▊    | 58/100 [14:28<10:02, 14.35s/it]
Inference Progress:  59%|█████▉    | 59/100 [14:51<11:27, 16.76s/it]
Inference Progress:  60%|██████    | 60/100 [15:00<09:46, 14.65s/it]
Inference Progress:  61%|██████    | 61/100 [15:11<08:43, 13.42s/it]
Inference Progress:  62%|██████▏   | 62/100 [15:33<10:11, 16.10s/it]
Inference Progress:  63%|██████▎   | 63/100 [15:56<11:05, 17.98s/it]
Inference Progress:  64%|██████▍   | 64/100 [16:09<10:00, 16.69s/it]
Inference Progress:  65%|██████▌   | 65/100 [16:24<09:25, 16.15s/it]
Inference Progress:  66%|██████▌   | 66/100 [16:33<07:56, 14.02s/it]
Inference Progress:  67%|██████▋   | 67/100 [16:56<09:03, 16.46s/it]
Inference Progress:  68%|██████▊   | 68/100 [17:07<08:02, 15.08s/it]
Inference Progress:  69%|██████▉   | 69/100 [17:10<05:54, 11.42s/it]
Inference Progress:  70%|███████   | 70/100 [17:18<05:05, 10.18s/it]
Inference Progress:  71%|███████   | 71/100 [17:40<06:40, 13.82s/it]
Inference Progress:  72%|███████▏  | 72/100 [17:52<06:13, 13.34s/it]
Inference Progress:  73%|███████▎  | 73/100 [18:04<05:45, 12.78s/it]
Inference Progress:  74%|███████▍  | 74/100 [18:13<05:06, 11.80s/it]
Inference Progress:  75%|███████▌  | 75/100 [18:23<04:39, 11.19s/it]
Inference Progress:  76%|███████▌  | 76/100 [18:36<04:40, 11.71s/it]
Inference Progress:  77%|███████▋  | 77/100 [18:48<04:35, 12.00s/it]
Inference Progress:  78%|███████▊  | 78/100 [19:11<05:30, 15.04s/it]
Inference Progress:  79%|███████▉  | 79/100 [19:25<05:09, 14.72s/it]
Inference Progress:  80%|████████  | 80/100 [19:42<05:07, 15.40s/it]
Inference Progress:  81%|████████  | 81/100 [19:55<04:44, 14.96s/it]
Inference Progress:  82%|████████▏ | 82/100 [20:03<03:48, 12.68s/it]
Inference Progress:  83%|████████▎ | 83/100 [20:10<03:05, 10.93s/it]
Inference Progress:  84%|████████▍ | 84/100 [20:20<02:52, 10.79s/it]
Inference Progress:  85%|████████▌ | 85/100 [20:42<03:33, 14.25s/it]
Inference Progress:  86%|████████▌ | 86/100 [21:05<03:52, 16.62s/it]
Inference Progress:  87%|████████▋ | 87/100 [21:27<03:58, 18.33s/it]
Inference Progress:  88%|████████▊ | 88/100 [21:35<03:03, 15.28s/it]
Inference Progress:  89%|████████▉ | 89/100 [21:43<02:24, 13.16s/it]
Inference Progress:  90%|█████████ | 90/100 [21:54<02:04, 12.40s/it]
Inference Progress:  91%|█████████ | 91/100 [22:08<01:54, 12.77s/it]
Inference Progress:  92%|█████████▏| 92/100 [22:17<01:35, 11.90s/it]
Inference Progress:  93%|█████████▎| 93/100 [22:36<01:37, 13.97s/it]
Inference Progress:  94%|█████████▍| 94/100 [22:48<01:19, 13.30s/it]
Inference Progress:  95%|█████████▌| 95/100 [22:57<00:59, 11.89s/it]
Inference Progress:  96%|█████████▌| 96/100 [23:15<00:55, 13.98s/it]
Inference Progress:  97%|█████████▋| 97/100 [23:26<00:38, 12.96s/it]
Inference Progress:  98%|█████████▊| 98/100 [23:48<00:31, 15.76s/it]
Inference Progress:  99%|█████████▉| 99/100 [24:02<00:15, 15.24s/it]
Inference Progress: 100%|██████████| 100/100 [24:15<00:00, 14.37s/it]
Inference Progress: 100%|██████████| 100/100 [24:15<00:00, 14.55s/it]
2025-04-24 08:37:23,747 - INFO - 
--- Inference Summary ---
2025-04-24 08:37:23,747 - INFO - Processed 100 samples.
2025-04-24 08:37:23,747 - INFO - Total 'generate' time (sum): 1454.93 sec
2025-04-24 08:37:23,747 - INFO - Overall inference loop duration: 1455.14 sec
2025-04-24 08:37:23,747 - INFO - Average 'generate' time per sample: 14.5493 sec
2025-04-24 08:37:23,747 - INFO - Total effective tokens generated: 33419
2025-04-24 08:37:23,747 - INFO - Overall effective tokens per second: 22.97
2025-04-24 08:37:23,748 - INFO - RAM Delta during inference: 520.50 MB
2025-04-24 08:37:23,748 - INFO - PyTorch VRAM Peak Delta during inference: 112.71 MB
2025-04-24 08:37:23,748 - INFO - System VRAM Peak Approx. Delta during inference: -1605.38 MB
2025-04-24 08:37:23,766 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-081254_outputs.json
2025-04-24 08:37:23,770 - INFO - Saved metrics to: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-081254_metrics.json
2025-04-24 08:37:23,770 - INFO - Cleaning up resources...
2025-04-24 08:37:23,774 - INFO - CUDA cache cleared.
2025-04-24 08:37:23,775 - INFO - NVML shut down.
2025-04-24 08:37:23,775 - INFO - --- Evaluation Complete ---

2025-04-24 08:37:26,156 - INFO - SUCCESS: Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 08:37:26,157 - INFO - 
Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 08:37:26,158 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 08:41:32,270 - WARNING - Stderr:
2025-04-24 08:38:12,153 - INFO - Initialized NVML for device 0.
2025-04-24 08:38:12,314 - INFO - --- Starting Evaluation ---
2025-04-24 08:38:12,314 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq'}
2025-04-24 08:38:12,314 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 08:38:12,314 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-083812_metrics.json
2025-04-24 08:38:12,314 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-083812_outputs.json
2025-04-24 08:38:12,314 - INFO - NVML Reporting Active: True
2025-04-24 08:38:12,314 - INFO - Initial RAM usage: 662.06 MB
2025-04-24 08:38:12,314 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 08:38:12,314 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 08:38:12,321 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 08:38:12,642 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 08:38:17,877 - WARNING - CUDA extension not installed.
2025-04-24 08:38:17,884 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 08:38:25,554 - INFO - CausalLM Model loaded.
2025-04-24 08:38:25,555 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 08:38:25,556 - INFO - Model loaded in 13.24 seconds.
2025-04-24 08:38:25,556 - INFO - Loading prompts from CTIBench subset: cti-mcq
2025-04-24 08:38:26,687 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 08:38:26,695 - INFO - Selected first 100 samples.
2025-04-24 08:38:26,695 - INFO - Using columns for cti-mcq: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-mcq):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-mcq): 100%|██████████| 100/100 [00:00<00:00, 3278.41it/s]
2025-04-24 08:38:26,727 - INFO - Successfully loaded 100 prompts from ctibench subset cti-mcq.
2025-04-24 08:38:26,727 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 08:38:26,729 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]
Inference Progress:   1%|          | 1/100 [00:24<41:09, 24.94s/it]
Inference Progress:   2%|▏         | 2/100 [00:26<18:07, 11.10s/it]
Inference Progress:   3%|▎         | 3/100 [00:27<10:36,  6.57s/it]
Inference Progress:   4%|▍         | 4/100 [00:28<07:17,  4.55s/it]
Inference Progress:   5%|▌         | 5/100 [00:30<05:31,  3.49s/it]
Inference Progress:   6%|▌         | 6/100 [00:33<05:14,  3.35s/it]
Inference Progress:   7%|▋         | 7/100 [00:34<03:56,  2.54s/it]
Inference Progress:   8%|▊         | 8/100 [00:44<07:30,  4.90s/it]
Inference Progress:   9%|▉         | 9/100 [00:46<05:50,  3.85s/it]
Inference Progress:  10%|█         | 10/100 [00:47<04:33,  3.04s/it]
Inference Progress:  11%|█         | 11/100 [00:48<03:38,  2.45s/it]
Inference Progress:  12%|█▏        | 12/100 [00:49<02:57,  2.02s/it]
Inference Progress:  13%|█▎        | 13/100 [00:50<02:37,  1.82s/it]
Inference Progress:  14%|█▍        | 14/100 [00:51<02:17,  1.59s/it]
Inference Progress:  15%|█▌        | 15/100 [00:53<02:17,  1.62s/it]
Inference Progress:  16%|█▌        | 16/100 [00:55<02:22,  1.69s/it]
Inference Progress:  17%|█▋        | 17/100 [00:56<02:00,  1.46s/it]
Inference Progress:  18%|█▊        | 18/100 [01:00<03:03,  2.24s/it]
Inference Progress:  19%|█▉        | 19/100 [01:02<03:10,  2.36s/it]
Inference Progress:  20%|██        | 20/100 [01:04<02:42,  2.03s/it]
Inference Progress:  21%|██        | 21/100 [01:05<02:27,  1.87s/it]
Inference Progress:  22%|██▏       | 22/100 [01:07<02:23,  1.84s/it]
Inference Progress:  23%|██▎       | 23/100 [01:09<02:19,  1.81s/it]
Inference Progress:  24%|██▍       | 24/100 [01:09<01:51,  1.47s/it]
Inference Progress:  25%|██▌       | 25/100 [01:10<01:33,  1.25s/it]
Inference Progress:  26%|██▌       | 26/100 [01:12<01:40,  1.36s/it]
Inference Progress:  27%|██▋       | 27/100 [01:14<01:52,  1.55s/it]
Inference Progress:  28%|██▊       | 28/100 [01:16<02:00,  1.68s/it]
Inference Progress:  29%|██▉       | 29/100 [01:17<01:59,  1.68s/it]
Inference Progress:  30%|███       | 30/100 [01:19<01:49,  1.56s/it]
Inference Progress:  31%|███       | 31/100 [01:20<01:32,  1.34s/it]
Inference Progress:  32%|███▏      | 32/100 [01:22<01:46,  1.56s/it]
Inference Progress:  33%|███▎      | 33/100 [01:23<01:45,  1.57s/it]
Inference Progress:  34%|███▍      | 34/100 [01:24<01:33,  1.41s/it]
Inference Progress:  35%|███▌      | 35/100 [01:26<01:35,  1.47s/it]
Inference Progress:  36%|███▌      | 36/100 [01:27<01:33,  1.46s/it]
Inference Progress:  37%|███▋      | 37/100 [01:29<01:41,  1.62s/it]
Inference Progress:  38%|███▊      | 38/100 [01:31<01:41,  1.64s/it]
Inference Progress:  39%|███▉      | 39/100 [01:32<01:21,  1.33s/it]
Inference Progress:  40%|████      | 40/100 [01:32<01:08,  1.14s/it]
Inference Progress:  41%|████      | 41/100 [01:34<01:09,  1.19s/it]
Inference Progress:  42%|████▏     | 42/100 [01:35<01:08,  1.18s/it]
Inference Progress:  43%|████▎     | 43/100 [01:36<01:14,  1.31s/it]
Inference Progress:  44%|████▍     | 44/100 [01:38<01:13,  1.32s/it]
Inference Progress:  45%|████▌     | 45/100 [01:39<01:14,  1.36s/it]
Inference Progress:  46%|████▌     | 46/100 [01:41<01:26,  1.60s/it]
Inference Progress:  47%|████▋     | 47/100 [01:43<01:18,  1.48s/it]
Inference Progress:  48%|████▊     | 48/100 [01:44<01:12,  1.40s/it]
Inference Progress:  49%|████▉     | 49/100 [01:45<01:09,  1.36s/it]
Inference Progress:  50%|█████     | 50/100 [01:46<01:08,  1.38s/it]
Inference Progress:  51%|█████     | 51/100 [01:48<01:16,  1.56s/it]
Inference Progress:  52%|█████▏    | 52/100 [01:49<01:01,  1.28s/it]
Inference Progress:  53%|█████▎    | 53/100 [01:51<01:08,  1.46s/it]
Inference Progress:  54%|█████▍    | 54/100 [01:52<00:59,  1.30s/it]
Inference Progress:  55%|█████▌    | 55/100 [01:54<01:09,  1.54s/it]
Inference Progress:  56%|█████▌    | 56/100 [01:56<01:08,  1.55s/it]
Inference Progress:  57%|█████▋    | 57/100 [01:57<01:03,  1.49s/it]
Inference Progress:  58%|█████▊    | 58/100 [01:58<01:00,  1.43s/it]
Inference Progress:  59%|█████▉    | 59/100 [01:59<00:57,  1.40s/it]
Inference Progress:  60%|██████    | 60/100 [02:02<01:14,  1.87s/it]
Inference Progress:  61%|██████    | 61/100 [02:04<01:11,  1.83s/it]
Inference Progress:  62%|██████▏   | 62/100 [02:05<00:59,  1.55s/it]
Inference Progress:  63%|██████▎   | 63/100 [02:06<00:55,  1.49s/it]
Inference Progress:  64%|██████▍   | 64/100 [02:08<00:51,  1.43s/it]
Inference Progress:  65%|██████▌   | 65/100 [02:10<01:00,  1.73s/it]
Inference Progress:  66%|██████▌   | 66/100 [02:11<00:54,  1.61s/it]
Inference Progress:  67%|██████▋   | 67/100 [02:13<00:48,  1.46s/it]
Inference Progress:  68%|██████▊   | 68/100 [02:14<00:49,  1.54s/it]
Inference Progress:  69%|██████▉   | 69/100 [02:15<00:40,  1.31s/it]
Inference Progress:  70%|███████   | 70/100 [02:17<00:41,  1.39s/it]
Inference Progress:  71%|███████   | 71/100 [02:18<00:42,  1.48s/it]
Inference Progress:  72%|███████▏  | 72/100 [02:20<00:39,  1.40s/it]
Inference Progress:  73%|███████▎  | 73/100 [02:22<00:42,  1.57s/it]
Inference Progress:  74%|███████▍  | 74/100 [02:23<00:39,  1.52s/it]
Inference Progress:  75%|███████▌  | 75/100 [02:25<00:42,  1.68s/it]
Inference Progress:  76%|███████▌  | 76/100 [02:27<00:45,  1.88s/it]
Inference Progress:  77%|███████▋  | 77/100 [02:29<00:38,  1.67s/it]
Inference Progress:  78%|███████▊  | 78/100 [02:31<00:39,  1.79s/it]
Inference Progress:  79%|███████▉  | 79/100 [02:32<00:32,  1.56s/it]
Inference Progress:  80%|████████  | 80/100 [02:33<00:32,  1.61s/it]
Inference Progress:  81%|████████  | 81/100 [02:34<00:25,  1.34s/it]
Inference Progress:  82%|████████▏ | 82/100 [02:35<00:22,  1.26s/it]
Inference Progress:  83%|████████▎ | 83/100 [02:36<00:18,  1.10s/it]
Inference Progress:  84%|████████▍ | 84/100 [02:37<00:19,  1.20s/it]
Inference Progress:  85%|████████▌ | 85/100 [02:40<00:23,  1.57s/it]
Inference Progress:  86%|████████▌ | 86/100 [02:41<00:21,  1.51s/it]
Inference Progress:  87%|████████▋ | 87/100 [02:43<00:19,  1.49s/it]
Inference Progress:  88%|████████▊ | 88/100 [02:44<00:17,  1.47s/it]
Inference Progress:  89%|████████▉ | 89/100 [02:46<00:17,  1.61s/it]
Inference Progress:  90%|█████████ | 90/100 [02:47<00:15,  1.53s/it]
Inference Progress:  91%|█████████ | 91/100 [02:48<00:12,  1.43s/it]
Inference Progress:  92%|█████████▏| 92/100 [02:50<00:11,  1.39s/it]
Inference Progress:  93%|█████████▎| 93/100 [02:51<00:09,  1.42s/it]
Inference Progress:  94%|█████████▍| 94/100 [02:55<00:13,  2.26s/it]
Inference Progress:  95%|█████████▌| 95/100 [02:57<00:10,  2.02s/it]
Inference Progress:  96%|█████████▌| 96/100 [02:58<00:07,  1.75s/it]
Inference Progress:  97%|█████████▋| 97/100 [02:59<00:04,  1.51s/it]
Inference Progress:  98%|█████████▊| 98/100 [03:00<00:02,  1.42s/it]
Inference Progress:  99%|█████████▉| 99/100 [03:02<00:01,  1.49s/it]
Inference Progress: 100%|██████████| 100/100 [03:03<00:00,  1.43s/it]
Inference Progress: 100%|██████████| 100/100 [03:03<00:00,  1.84s/it]
2025-04-24 08:41:30,365 - INFO - 
--- Inference Summary ---
2025-04-24 08:41:30,365 - INFO - Processed 100 samples.
2025-04-24 08:41:30,365 - INFO - Total 'generate' time (sum): 183.49 sec
2025-04-24 08:41:30,365 - INFO - Overall inference loop duration: 183.63 sec
2025-04-24 08:41:30,365 - INFO - Average 'generate' time per sample: 1.8349 sec
2025-04-24 08:41:30,365 - INFO - Total effective tokens generated: 4082
2025-04-24 08:41:30,365 - INFO - Overall effective tokens per second: 22.25
2025-04-24 08:41:30,365 - INFO - RAM Delta during inference: 451.50 MB
2025-04-24 08:41:30,365 - INFO - PyTorch VRAM Peak Delta during inference: 77.91 MB
2025-04-24 08:41:30,365 - INFO - System VRAM Peak Approx. Delta during inference: -1605.38 MB
2025-04-24 08:41:30,389 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-083812_outputs.json
2025-04-24 08:41:30,393 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-083812_metrics.json
2025-04-24 08:41:30,393 - INFO - Cleaning up resources...
2025-04-24 08:41:30,415 - INFO - CUDA cache cleared.
2025-04-24 08:41:30,415 - INFO - NVML shut down.
2025-04-24 08:41:30,415 - INFO - --- Evaluation Complete ---

2025-04-24 08:41:32,272 - INFO - SUCCESS: Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 08:41:32,273 - INFO - 
Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 08:41:32,274 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 09:18:37,869 - WARNING - Stderr:
2025-04-24 08:41:59,081 - INFO - Initialized NVML for device 0.
2025-04-24 08:41:59,229 - INFO - --- Starting Evaluation ---
2025-04-24 08:41:59,229 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-vsp'}
2025-04-24 08:41:59,229 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 08:41:59,229 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-084159_metrics.json
2025-04-24 08:41:59,229 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-084159_outputs.json
2025-04-24 08:41:59,229 - INFO - NVML Reporting Active: True
2025-04-24 08:41:59,229 - INFO - Initial RAM usage: 663.34 MB
2025-04-24 08:41:59,230 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 08:41:59,230 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 08:41:59,234 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 08:41:59,373 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 08:42:03,400 - WARNING - CUDA extension not installed.
2025-04-24 08:42:03,417 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 08:42:05,494 - INFO - CausalLM Model loaded.
2025-04-24 08:42:05,495 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 08:42:05,495 - INFO - Model loaded in 6.27 seconds.
2025-04-24 08:42:05,496 - INFO - Loading prompts from CTIBench subset: cti-vsp
2025-04-24 08:42:07,006 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 08:42:07,010 - INFO - Selected first 100 samples.
2025-04-24 08:42:07,010 - INFO - Using columns for cti-vsp: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-vsp):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-vsp): 100%|██████████| 100/100 [00:00<00:00, 7398.80it/s]
2025-04-24 08:42:07,025 - INFO - Successfully loaded 100 prompts from ctibench subset cti-vsp.
2025-04-24 08:42:07,026 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 08:42:07,027 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]
Inference Progress:   1%|          | 1/100 [00:23<38:02, 23.05s/it]
Inference Progress:   2%|▏         | 2/100 [00:44<36:35, 22.40s/it]
Inference Progress:   3%|▎         | 3/100 [01:07<36:05, 22.33s/it]
Inference Progress:   4%|▍         | 4/100 [01:29<35:39, 22.28s/it]
Inference Progress:   5%|▌         | 5/100 [01:50<34:17, 21.66s/it]
Inference Progress:   6%|▌         | 6/100 [02:12<34:14, 21.86s/it]
Inference Progress:   7%|▋         | 7/100 [02:24<29:09, 18.81s/it]
Inference Progress:   8%|▊         | 8/100 [02:47<30:36, 19.96s/it]
Inference Progress:   9%|▉         | 9/100 [03:08<30:52, 20.36s/it]
Inference Progress:  10%|█         | 10/100 [03:30<31:24, 20.93s/it]
Inference Progress:  11%|█         | 11/100 [03:52<31:39, 21.34s/it]
Inference Progress:  12%|█▏        | 12/100 [04:15<31:41, 21.61s/it]
Inference Progress:  13%|█▎        | 13/100 [04:37<31:35, 21.79s/it]
Inference Progress:  14%|█▍        | 14/100 [04:59<31:27, 21.94s/it]
Inference Progress:  15%|█▌        | 15/100 [05:21<31:12, 22.03s/it]
Inference Progress:  16%|█▌        | 16/100 [05:44<30:55, 22.09s/it]
Inference Progress:  17%|█▋        | 17/100 [06:03<29:23, 21.24s/it]
Inference Progress:  18%|█▊        | 18/100 [06:25<29:28, 21.57s/it]
Inference Progress:  19%|█▉        | 19/100 [06:47<29:23, 21.77s/it]
Inference Progress:  20%|██        | 20/100 [07:10<29:12, 21.91s/it]
Inference Progress:  21%|██        | 21/100 [07:32<28:58, 22.01s/it]
Inference Progress:  22%|██▏       | 22/100 [07:54<28:42, 22.08s/it]
Inference Progress:  23%|██▎       | 23/100 [08:16<28:24, 22.14s/it]
Inference Progress:  24%|██▍       | 24/100 [08:39<28:04, 22.17s/it]
Inference Progress:  25%|██▌       | 25/100 [09:01<27:49, 22.26s/it]
Inference Progress:  26%|██▌       | 26/100 [09:23<27:26, 22.25s/it]
Inference Progress:  27%|██▋       | 27/100 [09:46<27:04, 22.25s/it]
Inference Progress:  28%|██▊       | 28/100 [10:08<26:42, 22.25s/it]
Inference Progress:  29%|██▉       | 29/100 [10:30<26:19, 22.25s/it]
Inference Progress:  30%|███       | 30/100 [10:52<25:57, 22.26s/it]
Inference Progress:  31%|███       | 31/100 [11:15<25:36, 22.27s/it]
Inference Progress:  32%|███▏      | 32/100 [11:37<25:08, 22.19s/it]
Inference Progress:  33%|███▎      | 33/100 [11:58<24:29, 21.93s/it]
Inference Progress:  34%|███▍      | 34/100 [12:20<24:13, 22.02s/it]
Inference Progress:  35%|███▌      | 35/100 [12:43<23:55, 22.09s/it]
Inference Progress:  36%|███▌      | 36/100 [13:05<23:36, 22.13s/it]
Inference Progress:  37%|███▋      | 37/100 [13:27<23:16, 22.16s/it]
Inference Progress:  38%|███▊      | 38/100 [13:49<22:55, 22.18s/it]
Inference Progress:  39%|███▉      | 39/100 [14:11<22:34, 22.20s/it]
Inference Progress:  40%|████      | 40/100 [14:34<22:12, 22.21s/it]
Inference Progress:  41%|████      | 41/100 [14:56<21:51, 22.22s/it]
Inference Progress:  42%|████▏     | 42/100 [15:18<21:29, 22.23s/it]
Inference Progress:  43%|████▎     | 43/100 [15:41<21:10, 22.29s/it]
Inference Progress:  44%|████▍     | 44/100 [16:03<20:47, 22.28s/it]
Inference Progress:  45%|████▌     | 45/100 [16:25<20:25, 22.27s/it]
Inference Progress:  46%|████▌     | 46/100 [16:47<20:02, 22.27s/it]
Inference Progress:  47%|████▋     | 47/100 [17:10<19:40, 22.27s/it]
Inference Progress:  48%|████▊     | 48/100 [17:32<19:17, 22.27s/it]
Inference Progress:  49%|████▉     | 49/100 [17:54<18:55, 22.26s/it]
Inference Progress:  50%|█████     | 50/100 [18:16<18:33, 22.27s/it]
Inference Progress:  51%|█████     | 51/100 [18:39<18:11, 22.27s/it]
Inference Progress:  52%|█████▏    | 52/100 [19:01<17:48, 22.26s/it]
Inference Progress:  53%|█████▎    | 53/100 [19:23<17:25, 22.25s/it]
Inference Progress:  54%|█████▍    | 54/100 [19:45<16:59, 22.15s/it]
Inference Progress:  55%|█████▌    | 55/100 [20:07<16:38, 22.19s/it]
Inference Progress:  56%|█████▌    | 56/100 [20:30<16:16, 22.20s/it]
Inference Progress:  57%|█████▋    | 57/100 [20:52<15:55, 22.22s/it]
Inference Progress:  58%|█████▊    | 58/100 [21:14<15:33, 22.22s/it]
Inference Progress:  59%|█████▉    | 59/100 [21:36<15:11, 22.23s/it]
Inference Progress:  60%|██████    | 60/100 [21:59<14:49, 22.24s/it]
Inference Progress:  61%|██████    | 61/100 [22:15<13:18, 20.48s/it]
Inference Progress:  62%|██████▏   | 62/100 [22:37<13:18, 21.01s/it]
Inference Progress:  63%|██████▎   | 63/100 [22:57<12:46, 20.71s/it]
Inference Progress:  64%|██████▍   | 64/100 [23:20<12:45, 21.26s/it]
Inference Progress:  65%|██████▌   | 65/100 [23:42<12:34, 21.57s/it]
Inference Progress:  66%|██████▌   | 66/100 [24:04<12:21, 21.80s/it]
Inference Progress:  67%|██████▋   | 67/100 [24:27<12:03, 21.93s/it]
Inference Progress:  68%|██████▊   | 68/100 [24:49<11:45, 22.04s/it]
Inference Progress:  69%|██████▉   | 69/100 [25:11<11:25, 22.10s/it]
Inference Progress:  70%|███████   | 70/100 [25:33<11:04, 22.15s/it]
Inference Progress:  71%|███████   | 71/100 [25:56<10:43, 22.18s/it]
Inference Progress:  72%|███████▏  | 72/100 [26:18<10:21, 22.21s/it]
Inference Progress:  73%|███████▎  | 73/100 [26:38<09:45, 21.67s/it]
Inference Progress:  74%|███████▍  | 74/100 [27:01<09:27, 21.84s/it]
Inference Progress:  75%|███████▌  | 75/100 [27:21<08:52, 21.30s/it]
Inference Progress:  76%|███████▌  | 76/100 [27:43<08:37, 21.57s/it]
Inference Progress:  77%|███████▋  | 77/100 [28:05<08:20, 21.78s/it]
Inference Progress:  78%|███████▊  | 78/100 [28:28<08:02, 21.95s/it]
Inference Progress:  79%|███████▉  | 79/100 [28:50<07:42, 22.04s/it]
Inference Progress:  80%|████████  | 80/100 [29:10<07:12, 21.65s/it]
Inference Progress:  81%|████████  | 81/100 [29:33<06:54, 21.83s/it]
Inference Progress:  82%|████████▏ | 82/100 [29:55<06:35, 21.96s/it]
Inference Progress:  83%|████████▎ | 83/100 [30:17<06:14, 22.05s/it]
Inference Progress:  84%|████████▍ | 84/100 [30:39<05:53, 22.11s/it]
Inference Progress:  85%|████████▌ | 85/100 [31:00<05:25, 21.70s/it]
Inference Progress:  86%|████████▌ | 86/100 [31:22<05:06, 21.86s/it]
Inference Progress:  87%|████████▋ | 87/100 [31:45<04:45, 21.98s/it]
Inference Progress:  88%|████████▊ | 88/100 [32:07<04:24, 22.06s/it]
Inference Progress:  89%|████████▉ | 89/100 [32:27<03:55, 21.45s/it]
Inference Progress:  90%|█████████ | 90/100 [32:49<03:36, 21.69s/it]
Inference Progress:  91%|█████████ | 91/100 [33:11<03:16, 21.84s/it]
Inference Progress:  92%|█████████▏| 92/100 [33:32<02:50, 21.30s/it]
Inference Progress:  93%|█████████▎| 93/100 [33:54<02:31, 21.59s/it]
Inference Progress:  94%|█████████▍| 94/100 [34:16<02:10, 21.78s/it]
Inference Progress:  95%|█████████▌| 95/100 [34:38<01:49, 21.92s/it]
Inference Progress:  96%|█████████▌| 96/100 [35:00<01:28, 22.01s/it]
Inference Progress:  97%|█████████▋| 97/100 [35:23<01:06, 22.09s/it]
Inference Progress:  98%|█████████▊| 98/100 [35:45<00:44, 22.13s/it]
Inference Progress:  99%|█████████▉| 99/100 [36:07<00:22, 22.18s/it]
Inference Progress: 100%|██████████| 100/100 [36:30<00:00, 22.20s/it]
Inference Progress: 100%|██████████| 100/100 [36:30<00:00, 21.90s/it]
2025-04-24 09:18:37,034 - INFO - 
--- Inference Summary ---
2025-04-24 09:18:37,035 - INFO - Processed 100 samples.
2025-04-24 09:18:37,035 - INFO - Total 'generate' time (sum): 2189.78 sec
2025-04-24 09:18:37,035 - INFO - Overall inference loop duration: 2190.01 sec
2025-04-24 09:18:37,035 - INFO - Average 'generate' time per sample: 21.8978 sec
2025-04-24 09:18:37,035 - INFO - Total effective tokens generated: 50340
2025-04-24 09:18:37,035 - INFO - Overall effective tokens per second: 22.99
2025-04-24 09:18:37,035 - INFO - RAM Delta during inference: 497.25 MB
2025-04-24 09:18:37,035 - INFO - PyTorch VRAM Peak Delta during inference: 112.71 MB
2025-04-24 09:18:37,035 - INFO - System VRAM Peak Approx. Delta during inference: -1605.38 MB
2025-04-24 09:18:37,047 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-084159_outputs.json
2025-04-24 09:18:37,051 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-084159_metrics.json
2025-04-24 09:18:37,051 - INFO - Cleaning up resources...
2025-04-24 09:18:37,057 - INFO - CUDA cache cleared.
2025-04-24 09:18:37,057 - INFO - NVML shut down.
2025-04-24 09:18:37,057 - INFO - --- Evaluation Complete ---

2025-04-24 09:18:37,872 - INFO - SUCCESS: Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'. Check output files in /workspace/results/mod/48/
2025-04-24 09:18:37,873 - INFO - 
Run 5/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 09:18:37,874 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 09:40:28,486 - WARNING - Stderr:
2025-04-24 09:19:24,110 - INFO - Initialized NVML for device 0.
2025-04-24 09:19:24,258 - INFO - --- Starting Evaluation ---
2025-04-24 09:19:24,258 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-rcm'}
2025-04-24 09:19:24,258 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 09:19:24,258 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-rcm_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-091924_metrics.json
2025-04-24 09:19:24,258 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-rcm_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-091924_outputs.json
2025-04-24 09:19:24,258 - INFO - NVML Reporting Active: True
2025-04-24 09:19:24,258 - INFO - Initial RAM usage: 660.41 MB
2025-04-24 09:19:24,258 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 09:19:24,258 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 09:19:24,266 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 09:19:24,548 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 09:19:29,438 - WARNING - CUDA extension not installed.
2025-04-24 09:19:29,444 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 09:19:37,565 - INFO - CausalLM Model loaded.
2025-04-24 09:19:37,567 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 09:19:37,568 - INFO - Model loaded in 13.31 seconds.
2025-04-24 09:19:37,568 - INFO - Loading prompts from CTIBench subset: cti-rcm
2025-04-24 09:19:38,492 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 09:19:38,496 - INFO - Selected first 100 samples.
2025-04-24 09:19:38,496 - INFO - Using columns for cti-rcm: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-rcm):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-rcm): 100%|██████████| 100/100 [00:00<00:00, 5265.72it/s]
2025-04-24 09:19:38,516 - INFO - Successfully loaded 100 prompts from ctibench subset cti-rcm.
2025-04-24 09:19:38,517 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 09:19:38,518 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]
Inference Progress:   1%|          | 1/100 [00:10<17:15, 10.46s/it]
Inference Progress:   2%|▏         | 2/100 [00:20<16:30, 10.11s/it]
Inference Progress:   3%|▎         | 3/100 [00:42<25:28, 15.76s/it]
Inference Progress:   4%|▍         | 4/100 [00:50<20:12, 12.63s/it]
Inference Progress:   5%|▌         | 5/100 [00:57<16:46, 10.59s/it]
Inference Progress:   6%|▌         | 6/100 [01:06<15:29,  9.89s/it]
Inference Progress:   7%|▋         | 7/100 [01:13<14:12,  9.17s/it]
Inference Progress:   8%|▊         | 8/100 [01:34<19:43, 12.87s/it]
Inference Progress:   9%|▉         | 9/100 [01:42<17:01, 11.23s/it]
Inference Progress:  10%|█         | 10/100 [01:55<17:46, 11.85s/it]
Inference Progress:  11%|█         | 11/100 [02:10<19:02, 12.83s/it]
Inference Progress:  12%|█▏        | 12/100 [02:20<17:28, 11.92s/it]
Inference Progress:  13%|█▎        | 13/100 [02:28<15:26, 10.65s/it]
Inference Progress:  14%|█▍        | 14/100 [02:50<20:15, 14.14s/it]
Inference Progress:  15%|█▌        | 15/100 [02:54<15:34, 11.00s/it]
Inference Progress:  16%|█▌        | 16/100 [03:16<20:06, 14.37s/it]
Inference Progress:  17%|█▋        | 17/100 [03:27<18:40, 13.50s/it]
Inference Progress:  18%|█▊        | 18/100 [03:31<14:33, 10.66s/it]
Inference Progress:  19%|█▉        | 19/100 [03:53<19:03, 14.11s/it]
Inference Progress:  20%|██        | 20/100 [04:04<17:32, 13.16s/it]
Inference Progress:  21%|██        | 21/100 [04:20<18:14, 13.85s/it]
Inference Progress:  22%|██▏       | 22/100 [04:42<21:15, 16.36s/it]
Inference Progress:  23%|██▎       | 23/100 [05:04<23:14, 18.11s/it]
Inference Progress:  24%|██▍       | 24/100 [05:13<19:23, 15.31s/it]
Inference Progress:  25%|██▌       | 25/100 [05:17<15:02, 12.03s/it]
Inference Progress:  26%|██▌       | 26/100 [05:40<18:35, 15.08s/it]
Inference Progress:  27%|██▋       | 27/100 [05:46<15:11, 12.49s/it]
Inference Progress:  28%|██▊       | 28/100 [05:57<14:20, 11.95s/it]
Inference Progress:  29%|██▉       | 29/100 [06:07<13:40, 11.56s/it]
Inference Progress:  30%|███       | 30/100 [06:16<12:27, 10.67s/it]
Inference Progress:  31%|███       | 31/100 [06:26<12:03, 10.49s/it]
Inference Progress:  32%|███▏      | 32/100 [06:38<12:26, 10.98s/it]
Inference Progress:  33%|███▎      | 33/100 [07:00<16:00, 14.34s/it]
Inference Progress:  34%|███▍      | 34/100 [07:08<13:29, 12.27s/it]
Inference Progress:  35%|███▌      | 35/100 [07:17<12:11, 11.26s/it]
Inference Progress:  36%|███▌      | 36/100 [07:25<11:14, 10.54s/it]
Inference Progress:  37%|███▋      | 37/100 [07:41<12:37, 12.03s/it]
Inference Progress:  38%|███▊      | 38/100 [07:49<11:07, 10.77s/it]
Inference Progress:  39%|███▉      | 39/100 [07:57<10:17, 10.12s/it]
Inference Progress:  40%|████      | 40/100 [08:20<13:44, 13.74s/it]
Inference Progress:  41%|████      | 41/100 [08:30<12:24, 12.63s/it]
Inference Progress:  42%|████▏     | 42/100 [08:52<14:58, 15.50s/it]
Inference Progress:  43%|████▎     | 43/100 [09:14<16:39, 17.54s/it]
Inference Progress:  44%|████▍     | 44/100 [09:24<14:13, 15.24s/it]
Inference Progress:  45%|████▌     | 45/100 [09:34<12:34, 13.71s/it]
Inference Progress:  46%|████▌     | 46/100 [09:46<11:55, 13.24s/it]
Inference Progress:  47%|████▋     | 47/100 [09:57<11:02, 12.50s/it]
Inference Progress:  48%|████▊     | 48/100 [10:08<10:18, 11.90s/it]
Inference Progress:  49%|████▉     | 49/100 [10:21<10:28, 12.32s/it]
Inference Progress:  50%|█████     | 50/100 [10:26<08:31, 10.24s/it]
Inference Progress:  51%|█████     | 51/100 [10:48<11:17, 13.82s/it]
Inference Progress:  52%|█████▏    | 52/100 [11:11<13:03, 16.32s/it]
Inference Progress:  53%|█████▎    | 53/100 [11:33<14:09, 18.08s/it]
Inference Progress:  54%|█████▍    | 54/100 [11:45<12:27, 16.25s/it]
Inference Progress:  55%|█████▌    | 55/100 [11:51<09:55, 13.23s/it]
Inference Progress:  56%|█████▌    | 56/100 [11:59<08:28, 11.57s/it]
Inference Progress:  57%|█████▋    | 57/100 [12:08<07:46, 10.86s/it]
Inference Progress:  58%|█████▊    | 58/100 [12:14<06:34,  9.39s/it]
Inference Progress:  59%|█████▉    | 59/100 [12:26<06:57, 10.19s/it]
Inference Progress:  60%|██████    | 60/100 [12:48<09:11, 13.79s/it]
Inference Progress:  61%|██████    | 61/100 [12:58<08:13, 12.66s/it]
Inference Progress:  62%|██████▏   | 62/100 [13:04<06:41, 10.56s/it]
Inference Progress:  63%|██████▎   | 63/100 [13:15<06:40, 10.82s/it]
Inference Progress:  64%|██████▍   | 64/100 [13:38<08:34, 14.29s/it]
Inference Progress:  65%|██████▌   | 65/100 [13:46<07:20, 12.58s/it]
Inference Progress:  66%|██████▌   | 66/100 [13:52<06:03, 10.69s/it]
Inference Progress:  67%|██████▋   | 67/100 [14:07<06:34, 11.94s/it]
Inference Progress:  68%|██████▊   | 68/100 [14:29<08:00, 15.02s/it]
Inference Progress:  69%|██████▉   | 69/100 [14:37<06:31, 12.64s/it]
Inference Progress:  70%|███████   | 70/100 [14:44<05:31, 11.06s/it]
Inference Progress:  71%|███████   | 71/100 [14:57<05:35, 11.58s/it]
Inference Progress:  72%|███████▏  | 72/100 [15:02<04:34,  9.81s/it]
Inference Progress:  73%|███████▎  | 73/100 [15:09<03:59,  8.87s/it]
Inference Progress:  74%|███████▍  | 74/100 [15:16<03:36,  8.33s/it]
Inference Progress:  75%|███████▌  | 75/100 [15:38<05:12, 12.49s/it]
Inference Progress:  76%|███████▌  | 76/100 [15:45<04:20, 10.85s/it]
Inference Progress:  77%|███████▋  | 77/100 [16:08<05:27, 14.25s/it]
Inference Progress:  78%|███████▊  | 78/100 [16:14<04:23, 11.99s/it]
Inference Progress:  79%|███████▉  | 79/100 [16:22<03:45, 10.75s/it]
Inference Progress:  80%|████████  | 80/100 [16:31<03:23, 10.17s/it]
Inference Progress:  81%|████████  | 81/100 [16:39<02:59,  9.45s/it]
Inference Progress:  82%|████████▏ | 82/100 [17:01<03:58, 13.27s/it]
Inference Progress:  83%|████████▎ | 83/100 [17:11<03:31, 12.42s/it]
Inference Progress:  84%|████████▍ | 84/100 [17:24<03:18, 12.39s/it]
Inference Progress:  85%|████████▌ | 85/100 [17:28<02:30, 10.06s/it]
Inference Progress:  86%|████████▌ | 86/100 [17:50<03:11, 13.69s/it]
Inference Progress:  87%|████████▋ | 87/100 [17:56<02:25, 11.17s/it]
Inference Progress:  88%|████████▊ | 88/100 [18:08<02:19, 11.64s/it]
Inference Progress:  89%|████████▉ | 89/100 [18:18<01:59, 10.89s/it]
Inference Progress:  90%|█████████ | 90/100 [18:25<01:37,  9.71s/it]
Inference Progress:  91%|█████████ | 91/100 [18:33<01:23,  9.30s/it]
Inference Progress:  92%|█████████▏| 92/100 [18:55<01:45, 13.17s/it]
Inference Progress:  93%|█████████▎| 93/100 [19:13<01:41, 14.51s/it]
Inference Progress:  94%|█████████▍| 94/100 [19:35<01:40, 16.81s/it]
Inference Progress:  95%|█████████▌| 95/100 [19:57<01:32, 18.42s/it]
Inference Progress:  96%|█████████▌| 96/100 [20:04<01:00, 15.12s/it]
Inference Progress:  97%|█████████▋| 97/100 [20:09<00:35, 11.84s/it]
Inference Progress:  98%|█████████▊| 98/100 [20:21<00:24, 12.13s/it]
Inference Progress:  99%|█████████▉| 99/100 [20:38<00:13, 13.32s/it]
Inference Progress: 100%|██████████| 100/100 [20:47<00:00, 12.04s/it]
Inference Progress: 100%|██████████| 100/100 [20:47<00:00, 12.47s/it]
2025-04-24 09:40:25,653 - INFO - 
--- Inference Summary ---
2025-04-24 09:40:25,653 - INFO - Processed 100 samples.
2025-04-24 09:40:25,653 - INFO - Total 'generate' time (sum): 1246.95 sec
2025-04-24 09:40:25,653 - INFO - Overall inference loop duration: 1247.13 sec
2025-04-24 09:40:25,653 - INFO - Average 'generate' time per sample: 12.4695 sec
2025-04-24 09:40:25,653 - INFO - Total effective tokens generated: 28636
2025-04-24 09:40:25,653 - INFO - Overall effective tokens per second: 22.96
2025-04-24 09:40:25,653 - INFO - RAM Delta during inference: 527.25 MB
2025-04-24 09:40:25,653 - INFO - PyTorch VRAM Peak Delta during inference: 97.89 MB
2025-04-24 09:40:25,653 - INFO - System VRAM Peak Approx. Delta during inference: -1605.38 MB
2025-04-24 09:40:25,663 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-rcm_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-091924_outputs.json
2025-04-24 09:40:25,667 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-rcm_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-091924_metrics.json
2025-04-24 09:40:25,667 - INFO - Cleaning up resources...
2025-04-24 09:40:25,671 - INFO - CUDA cache cleared.
2025-04-24 09:40:25,671 - INFO - NVML shut down.
2025-04-24 09:40:25,671 - INFO - --- Evaluation Complete ---

2025-04-24 09:40:28,489 - INFO - SUCCESS: Run 5/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-rcm'. Check output files in /workspace/results/mod/48/
2025-04-24 09:40:28,491 - INFO - 
Run 6/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 09:40:28,493 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 10:33:53,785 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 10:33:53,786 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 10:33:53,787 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 10:33:53,787 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 10:33:53,788 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 10:33:53,788 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_103353.log
2025-04-24 10:33:53,789 - INFO - Models to run: ['TinyLlama-1.1B-Chat-v1.0-GPTQ', 'Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 10:33:53,789 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 10:33:53,790 - INFO - Running 100 samples per benchmark.
2025-04-24 10:33:53,790 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 10:33:53,791 - INFO - 
Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 10:33:53,793 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 10:34:52,378 - WARNING - Stderr:
2025-04-24 10:34:38,889 - INFO - Initialized NVML for device 0.
2025-04-24 10:34:39,043 - INFO - --- Starting Evaluation ---
2025-04-24 10:34:39,043 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 10:34:39,043 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:34:39,043 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103439_metrics.json
2025-04-24 10:34:39,043 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103439_outputs.json
2025-04-24 10:34:39,043 - INFO - NVML Reporting Active: True
2025-04-24 10:34:39,043 - INFO - Initial RAM usage: 597.75 MB
2025-04-24 10:34:39,043 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:34:39,043 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:34:39,051 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:34:39,339 - INFO - Tokenizer loaded.
2025-04-24 10:34:50,941 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:34:50,967 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:34:50,968 - INFO - Cleaning up resources...
2025-04-24 10:34:50,968 - INFO - CUDA cache cleared.
2025-04-24 10:34:50,968 - INFO - NVML shut down.
2025-04-24 10:34:50,968 - INFO - --- Evaluation Complete ---

2025-04-24 10:34:52,380 - INFO - SUCCESS: Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 10:34:52,381 - INFO - 
Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 10:34:52,382 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 10:35:16,400 - WARNING - Stderr:
2025-04-24 10:35:12,412 - INFO - Initialized NVML for device 0.
2025-04-24 10:35:12,562 - INFO - --- Starting Evaluation ---
2025-04-24 10:35:12,562 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 10:35:12,562 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:35:12,562 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103512_metrics.json
2025-04-24 10:35:12,562 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103512_outputs.json
2025-04-24 10:35:12,562 - INFO - NVML Reporting Active: True
2025-04-24 10:35:12,562 - INFO - Initial RAM usage: 603.41 MB
2025-04-24 10:35:12,562 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:35:12,562 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:35:12,569 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:35:12,705 - INFO - Tokenizer loaded.
2025-04-24 10:35:15,766 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:35:15,783 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:35:15,783 - INFO - Cleaning up resources...
2025-04-24 10:35:15,784 - INFO - CUDA cache cleared.
2025-04-24 10:35:15,784 - INFO - NVML shut down.
2025-04-24 10:35:15,784 - INFO - --- Evaluation Complete ---

2025-04-24 10:35:16,413 - INFO - SUCCESS: Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 10:35:16,414 - INFO - 
Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 10:35:16,415 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 10:35:40,916 - WARNING - Stderr:
2025-04-24 10:35:37,295 - INFO - Initialized NVML for device 0.
2025-04-24 10:35:37,443 - INFO - --- Starting Evaluation ---
2025-04-24 10:35:37,443 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq'}
2025-04-24 10:35:37,443 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:35:37,443 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103537_metrics.json
2025-04-24 10:35:37,443 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103537_outputs.json
2025-04-24 10:35:37,443 - INFO - NVML Reporting Active: True
2025-04-24 10:35:37,443 - INFO - Initial RAM usage: 604.53 MB
2025-04-24 10:35:37,443 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:35:37,443 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:35:37,449 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:35:37,582 - INFO - Tokenizer loaded.
2025-04-24 10:35:40,275 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:35:40,294 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:35:40,295 - INFO - Cleaning up resources...
2025-04-24 10:35:40,295 - INFO - CUDA cache cleared.
2025-04-24 10:35:40,295 - INFO - NVML shut down.
2025-04-24 10:35:40,295 - INFO - --- Evaluation Complete ---

2025-04-24 10:35:40,919 - INFO - SUCCESS: Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 10:35:40,920 - INFO - 
Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 10:35:40,921 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 10:36:04,565 - WARNING - Stderr:
2025-04-24 10:36:00,877 - INFO - Initialized NVML for device 0.
2025-04-24 10:36:01,020 - INFO - --- Starting Evaluation ---
2025-04-24 10:36:01,020 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-vsp'}
2025-04-24 10:36:01,021 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:36:01,021 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103601_metrics.json
2025-04-24 10:36:01,021 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103601_outputs.json
2025-04-24 10:36:01,021 - INFO - NVML Reporting Active: True
2025-04-24 10:36:01,021 - INFO - Initial RAM usage: 602.79 MB
2025-04-24 10:36:01,021 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:36:01,021 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:36:01,027 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:36:01,160 - INFO - Tokenizer loaded.
2025-04-24 10:36:03,926 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:36:03,946 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:36:03,947 - INFO - Cleaning up resources...
2025-04-24 10:36:03,947 - INFO - CUDA cache cleared.
2025-04-24 10:36:03,948 - INFO - NVML shut down.
2025-04-24 10:36:03,948 - INFO - --- Evaluation Complete ---

2025-04-24 10:36:04,567 - INFO - SUCCESS: Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'. Check output files in /workspace/results/mod/48/
2025-04-24 10:36:04,568 - INFO - 
Run 5/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 10:36:04,569 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 10:36:29,033 - WARNING - Stderr:
2025-04-24 10:36:25,368 - INFO - Initialized NVML for device 0.
2025-04-24 10:36:25,514 - INFO - --- Starting Evaluation ---
2025-04-24 10:36:25,514 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-rcm'}
2025-04-24 10:36:25,514 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:36:25,514 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-rcm_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103625_metrics.json
2025-04-24 10:36:25,514 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-rcm_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-103625_outputs.json
2025-04-24 10:36:25,514 - INFO - NVML Reporting Active: True
2025-04-24 10:36:25,515 - INFO - Initial RAM usage: 604.64 MB
2025-04-24 10:36:25,515 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:36:25,515 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:36:25,520 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:36:25,655 - INFO - Tokenizer loaded.
2025-04-24 10:36:28,383 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:36:28,402 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:36:28,412 - INFO - Cleaning up resources...
2025-04-24 10:36:28,412 - INFO - CUDA cache cleared.
2025-04-24 10:36:28,412 - INFO - NVML shut down.
2025-04-24 10:36:28,412 - INFO - --- Evaluation Complete ---

2025-04-24 10:36:29,036 - INFO - SUCCESS: Run 5/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-rcm'. Check output files in /workspace/results/mod/48/
2025-04-24 10:36:29,037 - INFO - 
Run 6/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 10:36:29,039 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 10:39:47,351 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 10:39:47,352 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 10:39:47,353 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 10:39:47,353 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 10:39:47,354 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 10:39:47,358 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_103947.log
2025-04-24 10:39:47,359 - INFO - Models to run: ['TinyLlama-1.1B-Chat-v1.0-GPTQ', 'Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 10:39:47,359 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 10:39:47,360 - INFO - Running 100 samples per benchmark.
2025-04-24 10:39:47,360 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 10:39:47,361 - INFO - 
Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 10:39:47,363 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 10:40:15,672 - WARNING - Stderr:
2025-04-24 10:40:11,030 - INFO - Initialized NVML for device 0.
2025-04-24 10:40:11,178 - INFO - --- Starting Evaluation ---
2025-04-24 10:40:11,178 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 10:40:11,178 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:40:11,178 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104011_metrics.json
2025-04-24 10:40:11,178 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104011_outputs.json
2025-04-24 10:40:11,178 - INFO - NVML Reporting Active: True
2025-04-24 10:40:11,178 - INFO - Initial RAM usage: 600.81 MB
2025-04-24 10:40:11,178 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:40:11,178 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:40:11,184 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:40:11,365 - INFO - Tokenizer loaded.
2025-04-24 10:40:15,078 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:40:15,089 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:40:15,090 - INFO - Cleaning up resources...
2025-04-24 10:40:15,090 - INFO - CUDA cache cleared.
2025-04-24 10:40:15,090 - INFO - NVML shut down.
2025-04-24 10:40:15,090 - INFO - --- Evaluation Complete ---

2025-04-24 10:40:15,673 - INFO - SUCCESS: Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 10:40:15,674 - INFO - 
Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 10:40:15,675 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 10:40:38,858 - WARNING - Stderr:
2025-04-24 10:40:35,276 - INFO - Initialized NVML for device 0.
2025-04-24 10:40:35,421 - INFO - --- Starting Evaluation ---
2025-04-24 10:40:35,421 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 10:40:35,421 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:40:35,421 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104035_metrics.json
2025-04-24 10:40:35,421 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104035_outputs.json
2025-04-24 10:40:35,421 - INFO - NVML Reporting Active: True
2025-04-24 10:40:35,421 - INFO - Initial RAM usage: 601.20 MB
2025-04-24 10:40:35,421 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:40:35,421 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:40:35,427 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:40:35,559 - INFO - Tokenizer loaded.
2025-04-24 10:40:38,236 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:40:38,255 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:40:38,256 - INFO - Cleaning up resources...
2025-04-24 10:40:38,256 - INFO - CUDA cache cleared.
2025-04-24 10:40:38,256 - INFO - NVML shut down.
2025-04-24 10:40:38,256 - INFO - --- Evaluation Complete ---

2025-04-24 10:40:38,860 - INFO - SUCCESS: Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 10:40:38,861 - INFO - 
Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 10:40:38,862 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 10:41:01,578 - WARNING - Stderr:
2025-04-24 10:40:58,133 - INFO - Initialized NVML for device 0.
2025-04-24 10:40:58,277 - INFO - --- Starting Evaluation ---
2025-04-24 10:40:58,277 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq'}
2025-04-24 10:40:58,277 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:40:58,277 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104058_metrics.json
2025-04-24 10:40:58,277 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104058_outputs.json
2025-04-24 10:40:58,277 - INFO - NVML Reporting Active: True
2025-04-24 10:40:58,277 - INFO - Initial RAM usage: 603.18 MB
2025-04-24 10:40:58,277 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:40:58,277 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:40:58,283 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:40:58,437 - INFO - Tokenizer loaded.
2025-04-24 10:41:00,953 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:41:00,970 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:41:00,972 - INFO - Cleaning up resources...
2025-04-24 10:41:00,972 - INFO - CUDA cache cleared.
2025-04-24 10:41:00,972 - INFO - NVML shut down.
2025-04-24 10:41:00,972 - INFO - --- Evaluation Complete ---

2025-04-24 10:41:01,580 - INFO - SUCCESS: Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 10:41:01,580 - INFO - 
Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 10:41:01,582 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 10:41:23,925 - WARNING - Stderr:
2025-04-24 10:41:20,518 - INFO - Initialized NVML for device 0.
2025-04-24 10:41:20,662 - INFO - --- Starting Evaluation ---
2025-04-24 10:41:20,663 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-vsp'}
2025-04-24 10:41:20,663 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:41:20,663 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104120_metrics.json
2025-04-24 10:41:20,663 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104120_outputs.json
2025-04-24 10:41:20,663 - INFO - NVML Reporting Active: True
2025-04-24 10:41:20,663 - INFO - Initial RAM usage: 604.52 MB
2025-04-24 10:41:20,663 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:41:20,663 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:41:20,669 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:41:20,811 - INFO - Tokenizer loaded.
2025-04-24 10:41:23,308 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:41:23,326 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:41:23,327 - INFO - Cleaning up resources...
2025-04-24 10:41:23,327 - INFO - CUDA cache cleared.
2025-04-24 10:41:23,327 - INFO - NVML shut down.
2025-04-24 10:41:23,327 - INFO - --- Evaluation Complete ---

2025-04-24 10:41:23,928 - INFO - SUCCESS: Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'. Check output files in /workspace/results/mod/48/
2025-04-24 10:41:23,929 - INFO - 
Run 5/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 10:41:23,930 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 10:41:46,709 - WARNING - Stderr:
2025-04-24 10:41:43,304 - INFO - Initialized NVML for device 0.
2025-04-24 10:41:43,449 - INFO - --- Starting Evaluation ---
2025-04-24 10:41:43,449 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-rcm'}
2025-04-24 10:41:43,449 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:41:43,449 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-rcm_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104143_metrics.json
2025-04-24 10:41:43,449 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-rcm_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104143_outputs.json
2025-04-24 10:41:43,449 - INFO - NVML Reporting Active: True
2025-04-24 10:41:43,449 - INFO - Initial RAM usage: 600.79 MB
2025-04-24 10:41:43,449 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:41:43,449 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:41:43,455 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:41:43,602 - INFO - Tokenizer loaded.
2025-04-24 10:41:46,053 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:41:46,071 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:41:46,072 - INFO - Cleaning up resources...
2025-04-24 10:41:46,072 - INFO - CUDA cache cleared.
2025-04-24 10:41:46,073 - INFO - NVML shut down.
2025-04-24 10:41:46,073 - INFO - --- Evaluation Complete ---

2025-04-24 10:41:46,711 - INFO - SUCCESS: Run 5/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-rcm'. Check output files in /workspace/results/mod/48/
2025-04-24 10:41:46,712 - INFO - 
Run 6/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 10:41:46,714 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 10:46:19,729 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 10:46:19,730 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 10:46:19,731 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 10:46:19,732 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 10:46:19,732 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 10:46:19,733 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_104619.log
2025-04-24 10:46:19,733 - INFO - Models to run: ['TinyLlama-1.1B-Chat-v1.0-GPTQ', 'Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 10:46:19,734 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 10:46:19,734 - INFO - Running 100 samples per benchmark.
2025-04-24 10:46:19,735 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 10:46:19,735 - INFO - 
Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 10:46:19,738 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:01:18,594 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 11:01:18,595 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 11:01:18,596 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 11:01:18,597 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 11:01:18,597 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 11:01:18,598 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_110118.log
2025-04-24 11:01:18,598 - INFO - Models to run: ['Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 11:01:18,599 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 11:01:18,599 - INFO - Running 100 samples per benchmark.
2025-04-24 11:01:18,600 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 11:01:18,612 - INFO - 
Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:01:18,615 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:02:03,970 - WARNING - Stderr:
2025-04-24 11:01:53,001 - INFO - Initialized NVML for device 0.
2025-04-24 11:01:53,155 - INFO - --- Starting Evaluation ---
2025-04-24 11:01:53,155 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:01:53,155 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:01:53,155 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-110153_metrics.json
2025-04-24 11:01:53,155 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-110153_outputs.json
2025-04-24 11:01:53,155 - INFO - NVML Reporting Active: True
2025-04-24 11:01:53,155 - INFO - Initial RAM usage: 598.23 MB
2025-04-24 11:01:53,155 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 11:01:53,155 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:01:53,170 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:01:53,496 - INFO - Tokenizer loaded.
2025-04-24 11:02:01,934 - ERROR - Error loading model: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 558, in from_pretrained
    model_class = get_class_from_dynamic_module(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py", line 570, in get_class_from_dynamic_module
    return get_class_in_module(class_name, final_module, force_reload=force_download)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py", line 267, in get_class_in_module
    module_spec.loader.exec_module(module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 38, in <module>
    from transformers.modeling_utils import PreTrainedModel
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 62, in <module>
    from .integrations.flash_attention import flash_attention_forward
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/integrations/flash_attention.py", line 5, in <module>
    from ..modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 11:02:01,963 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:02:01,964 - INFO - Cleaning up resources...
2025-04-24 11:02:01,964 - INFO - CUDA cache cleared.
2025-04-24 11:02:01,964 - INFO - NVML shut down.
2025-04-24 11:02:01,964 - INFO - --- Evaluation Complete ---

2025-04-24 11:02:03,972 - INFO - SUCCESS: Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 11:02:03,973 - INFO - 
Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 11:02:03,974 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:02:31,179 - WARNING - Stderr:
2025-04-24 11:02:23,718 - INFO - Initialized NVML for device 0.
2025-04-24 11:02:23,862 - INFO - --- Starting Evaluation ---
2025-04-24 11:02:23,863 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:02:23,863 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:02:23,863 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-110223_metrics.json
2025-04-24 11:02:23,863 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-110223_outputs.json
2025-04-24 11:02:23,863 - INFO - NVML Reporting Active: True
2025-04-24 11:02:23,863 - INFO - Initial RAM usage: 603.91 MB
2025-04-24 11:02:23,863 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 11:02:23,863 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:02:23,869 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:02:24,010 - INFO - Tokenizer loaded.
2025-04-24 11:02:30,553 - ERROR - Error loading model: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 558, in from_pretrained
    model_class = get_class_from_dynamic_module(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py", line 570, in get_class_from_dynamic_module
    return get_class_in_module(class_name, final_module, force_reload=force_download)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py", line 267, in get_class_in_module
    module_spec.loader.exec_module(module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 38, in <module>
    from transformers.modeling_utils import PreTrainedModel
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 62, in <module>
    from .integrations.flash_attention import flash_attention_forward
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/integrations/flash_attention.py", line 5, in <module>
    from ..modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 11:02:30,578 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:02:30,579 - INFO - Cleaning up resources...
2025-04-24 11:02:30,579 - INFO - CUDA cache cleared.
2025-04-24 11:02:30,580 - INFO - NVML shut down.
2025-04-24 11:02:30,580 - INFO - --- Evaluation Complete ---

2025-04-24 11:02:31,181 - INFO - SUCCESS: Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 11:02:31,182 - INFO - 
Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 11:02:31,183 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 11:02:59,629 - WARNING - Stderr:
2025-04-24 11:02:52,509 - INFO - Initialized NVML for device 0.
2025-04-24 11:02:52,655 - INFO - --- Starting Evaluation ---
2025-04-24 11:02:52,655 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq'}
2025-04-24 11:02:52,655 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:02:52,655 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-110252_metrics.json
2025-04-24 11:02:52,655 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-110252_outputs.json
2025-04-24 11:02:52,655 - INFO - NVML Reporting Active: True
2025-04-24 11:02:52,655 - INFO - Initial RAM usage: 601.63 MB
2025-04-24 11:02:52,655 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 11:02:52,655 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:02:52,661 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:02:52,812 - INFO - Tokenizer loaded.
2025-04-24 11:02:58,995 - ERROR - Error loading model: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 558, in from_pretrained
    model_class = get_class_from_dynamic_module(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py", line 570, in get_class_from_dynamic_module
    return get_class_in_module(class_name, final_module, force_reload=force_download)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py", line 267, in get_class_in_module
    module_spec.loader.exec_module(module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 38, in <module>
    from transformers.modeling_utils import PreTrainedModel
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 62, in <module>
    from .integrations.flash_attention import flash_attention_forward
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/integrations/flash_attention.py", line 5, in <module>
    from ..modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 11:02:59,023 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:02:59,024 - INFO - Cleaning up resources...
2025-04-24 11:02:59,025 - INFO - CUDA cache cleared.
2025-04-24 11:02:59,025 - INFO - NVML shut down.
2025-04-24 11:02:59,025 - INFO - --- Evaluation Complete ---

2025-04-24 11:02:59,631 - INFO - SUCCESS: Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 11:02:59,632 - INFO - 
Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 11:02:59,633 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 11:03:24,853 - WARNING - Stderr:
2025-04-24 11:03:17,788 - INFO - Initialized NVML for device 0.
2025-04-24 11:03:17,929 - INFO - --- Starting Evaluation ---
2025-04-24 11:03:17,929 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-vsp'}
2025-04-24 11:03:17,929 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:03:17,929 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-vsp_Phi-3-mini-4k-instruct-GPTQ_20250424-110317_metrics.json
2025-04-24 11:03:17,929 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-vsp_Phi-3-mini-4k-instruct-GPTQ_20250424-110317_outputs.json
2025-04-24 11:03:17,929 - INFO - NVML Reporting Active: True
2025-04-24 11:03:17,929 - INFO - Initial RAM usage: 604.32 MB
2025-04-24 11:03:17,929 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 11:03:17,929 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:03:17,936 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:03:18,068 - INFO - Tokenizer loaded.
2025-04-24 11:03:24,237 - ERROR - Error loading model: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 558, in from_pretrained
    model_class = get_class_from_dynamic_module(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py", line 570, in get_class_from_dynamic_module
    return get_class_in_module(class_name, final_module, force_reload=force_download)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py", line 267, in get_class_in_module
    module_spec.loader.exec_module(module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 38, in <module>
    from transformers.modeling_utils import PreTrainedModel
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 62, in <module>
    from .integrations.flash_attention import flash_attention_forward
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/integrations/flash_attention.py", line 5, in <module>
    from ..modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 11:03:24,256 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:03:24,257 - INFO - Cleaning up resources...
2025-04-24 11:03:24,257 - INFO - CUDA cache cleared.
2025-04-24 11:03:24,257 - INFO - NVML shut down.
2025-04-24 11:03:24,257 - INFO - --- Evaluation Complete ---

2025-04-24 11:03:24,855 - INFO - SUCCESS: Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'. Check output files in /workspace/results/mod/48/
2025-04-24 11:03:24,856 - INFO - 
Run 5/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 11:03:24,857 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 11:03:55,327 - WARNING - Stderr:
2025-04-24 11:03:44,837 - INFO - Initialized NVML for device 0.
2025-04-24 11:03:44,986 - INFO - --- Starting Evaluation ---
2025-04-24 11:03:44,986 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-rcm'}
2025-04-24 11:03:44,986 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:03:44,986 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-rcm_Phi-3-mini-4k-instruct-GPTQ_20250424-110344_metrics.json
2025-04-24 11:03:44,986 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-rcm_Phi-3-mini-4k-instruct-GPTQ_20250424-110344_outputs.json
2025-04-24 11:03:44,986 - INFO - NVML Reporting Active: True
2025-04-24 11:03:44,986 - INFO - Initial RAM usage: 595.72 MB
2025-04-24 11:03:44,986 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 11:03:44,986 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:03:44,994 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:03:45,183 - INFO - Tokenizer loaded.
2025-04-24 11:03:54,570 - ERROR - Error loading model: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 558, in from_pretrained
    model_class = get_class_from_dynamic_module(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py", line 570, in get_class_from_dynamic_module
    return get_class_in_module(class_name, final_module, force_reload=force_download)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/dynamic_module_utils.py", line 267, in get_class_in_module
    module_spec.loader.exec_module(module)
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 38, in <module>
    from transformers.modeling_utils import PreTrainedModel
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 62, in <module>
    from .integrations.flash_attention import flash_attention_forward
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/integrations/flash_attention.py", line 5, in <module>
    from ..modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 11:03:54,601 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:03:54,610 - INFO - Cleaning up resources...
2025-04-24 11:03:54,610 - INFO - CUDA cache cleared.
2025-04-24 11:03:54,610 - INFO - NVML shut down.
2025-04-24 11:03:54,610 - INFO - --- Evaluation Complete ---

2025-04-24 11:03:55,331 - INFO - SUCCESS: Run 5/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'. Check output files in /workspace/results/mod/48/
2025-04-24 11:03:55,332 - INFO - 
Run 6/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:03:55,335 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:04:16,198 - WARNING - Stderr:
2025-04-24 11:04:14,283 - INFO - Initialized NVML for device 0.
2025-04-24 11:04:14,426 - INFO - --- Starting Evaluation ---
2025-04-24 11:04:14,426 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:04:14,427 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:04:14,427 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_Mistral-7B-Instruct-v0.3-GPTQ_20250424-110414_metrics.json
2025-04-24 11:04:14,427 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_Mistral-7B-Instruct-v0.3-GPTQ_20250424-110414_outputs.json
2025-04-24 11:04:14,427 - INFO - NVML Reporting Active: True
2025-04-24 11:04:14,427 - INFO - Initial RAM usage: 597.88 MB
2025-04-24 11:04:14,427 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 11:04:14,427 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 11:04:14,433 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:04:15,023 - INFO - Tokenizer loaded.
2025-04-24 11:04:15,593 - ERROR - Error loading model: Failed to import transformers.models.mistral.modeling_mistral because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py", line 17, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.mistral.modeling_mistral because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 11:04:15,624 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:04:15,625 - INFO - Cleaning up resources...
2025-04-24 11:04:15,625 - INFO - CUDA cache cleared.
2025-04-24 11:04:15,625 - INFO - NVML shut down.
2025-04-24 11:04:15,626 - INFO - --- Evaluation Complete ---

2025-04-24 11:04:16,200 - INFO - SUCCESS: Run 6/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 11:04:16,212 - INFO - 
Run 7/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 11:04:16,213 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:04:36,653 - WARNING - Stderr:
2025-04-24 11:04:34,961 - INFO - Initialized NVML for device 0.
2025-04-24 11:04:35,111 - INFO - --- Starting Evaluation ---
2025-04-24 11:04:35,111 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:04:35,111 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:04:35,111 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_Mistral-7B-Instruct-v0.3-GPTQ_20250424-110435_metrics.json
2025-04-24 11:04:35,111 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_Mistral-7B-Instruct-v0.3-GPTQ_20250424-110435_outputs.json
2025-04-24 11:04:35,111 - INFO - NVML Reporting Active: True
2025-04-24 11:04:35,111 - INFO - Initial RAM usage: 603.54 MB
2025-04-24 11:04:35,111 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 11:04:35,111 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 11:04:35,117 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:04:35,592 - INFO - Tokenizer loaded.
2025-04-24 11:04:36,065 - ERROR - Error loading model: Failed to import transformers.models.mistral.modeling_mistral because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py", line 17, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.mistral.modeling_mistral because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 11:04:36,083 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:04:36,084 - INFO - Cleaning up resources...
2025-04-24 11:04:36,085 - INFO - CUDA cache cleared.
2025-04-24 11:04:36,085 - INFO - NVML shut down.
2025-04-24 11:04:36,085 - INFO - --- Evaluation Complete ---

2025-04-24 11:04:36,656 - INFO - SUCCESS: Run 7/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 11:04:36,656 - INFO - 
Run 8/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 11:04:36,657 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 11:04:57,366 - WARNING - Stderr:
2025-04-24 11:04:55,667 - INFO - Initialized NVML for device 0.
2025-04-24 11:04:55,811 - INFO - --- Starting Evaluation ---
2025-04-24 11:04:55,811 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq'}
2025-04-24 11:04:55,811 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:04:55,811 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_Mistral-7B-Instruct-v0.3-GPTQ_20250424-110455_metrics.json
2025-04-24 11:04:55,811 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_Mistral-7B-Instruct-v0.3-GPTQ_20250424-110455_outputs.json
2025-04-24 11:04:55,811 - INFO - NVML Reporting Active: True
2025-04-24 11:04:55,811 - INFO - Initial RAM usage: 603.64 MB
2025-04-24 11:04:55,811 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 11:04:55,811 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 11:04:55,817 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:04:56,283 - INFO - Tokenizer loaded.
2025-04-24 11:04:56,756 - ERROR - Error loading model: Failed to import transformers.models.mistral.modeling_mistral because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py", line 17, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.mistral.modeling_mistral because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 11:04:56,772 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:04:56,773 - INFO - Cleaning up resources...
2025-04-24 11:04:56,773 - INFO - CUDA cache cleared.
2025-04-24 11:04:56,773 - INFO - NVML shut down.
2025-04-24 11:04:56,773 - INFO - --- Evaluation Complete ---

2025-04-24 11:04:57,368 - INFO - SUCCESS: Run 8/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 11:04:57,369 - INFO - 
Run 9/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 11:04:57,370 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 11:05:17,567 - WARNING - Stderr:
2025-04-24 11:05:15,836 - INFO - Initialized NVML for device 0.
2025-04-24 11:05:15,981 - INFO - --- Starting Evaluation ---
2025-04-24 11:05:15,981 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-vsp'}
2025-04-24 11:05:15,981 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:05:15,981 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-vsp_Mistral-7B-Instruct-v0.3-GPTQ_20250424-110515_metrics.json
2025-04-24 11:05:15,981 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-vsp_Mistral-7B-Instruct-v0.3-GPTQ_20250424-110515_outputs.json
2025-04-24 11:05:15,981 - INFO - NVML Reporting Active: True
2025-04-24 11:05:15,981 - INFO - Initial RAM usage: 600.45 MB
2025-04-24 11:05:15,981 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 11:05:15,981 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 11:05:15,987 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:05:16,486 - INFO - Tokenizer loaded.
2025-04-24 11:05:16,978 - ERROR - Error loading model: Failed to import transformers.models.mistral.modeling_mistral because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py", line 17, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.mistral.modeling_mistral because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 11:05:16,994 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:05:16,995 - INFO - Cleaning up resources...
2025-04-24 11:05:16,995 - INFO - CUDA cache cleared.
2025-04-24 11:05:16,995 - INFO - NVML shut down.
2025-04-24 11:05:16,995 - INFO - --- Evaluation Complete ---

2025-04-24 11:05:17,569 - INFO - SUCCESS: Run 9/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-vsp'. Check output files in /workspace/results/mod/48/
2025-04-24 11:05:17,570 - INFO - 
Run 10/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 11:05:17,571 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 11:05:38,071 - WARNING - Stderr:
2025-04-24 11:05:36,305 - INFO - Initialized NVML for device 0.
2025-04-24 11:05:36,451 - INFO - --- Starting Evaluation ---
2025-04-24 11:05:36,451 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-rcm'}
2025-04-24 11:05:36,451 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:05:36,451 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-rcm_Mistral-7B-Instruct-v0.3-GPTQ_20250424-110536_metrics.json
2025-04-24 11:05:36,451 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-rcm_Mistral-7B-Instruct-v0.3-GPTQ_20250424-110536_outputs.json
2025-04-24 11:05:36,451 - INFO - NVML Reporting Active: True
2025-04-24 11:05:36,451 - INFO - Initial RAM usage: 600.26 MB
2025-04-24 11:05:36,451 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 11:05:36,451 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 11:05:36,459 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:05:36,949 - INFO - Tokenizer loaded.
2025-04-24 11:05:37,425 - ERROR - Error loading model: Failed to import transformers.models.mistral.modeling_mistral because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/mistral/modeling_mistral.py", line 17, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.mistral.modeling_mistral because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 11:05:37,441 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:05:37,442 - INFO - Cleaning up resources...
2025-04-24 11:05:37,443 - INFO - CUDA cache cleared.
2025-04-24 11:05:37,443 - INFO - NVML shut down.
2025-04-24 11:05:37,443 - INFO - --- Evaluation Complete ---

2025-04-24 11:05:38,073 - INFO - SUCCESS: Run 10/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-rcm'. Check output files in /workspace/results/mod/48/
2025-04-24 11:05:38,073 - INFO - --- Security Benchmark Automation Finished (QUANTIZED MODELS) ---
2025-04-24 11:13:45,397 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 11:13:45,398 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 11:13:45,399 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 11:13:45,399 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 11:13:45,409 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 11:13:45,410 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_111345.log
2025-04-24 11:13:45,411 - INFO - Models to run: ['Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 11:13:45,412 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 11:13:45,413 - INFO - Running 100 samples per benchmark.
2025-04-24 11:13:45,413 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 11:13:45,414 - INFO - 
Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:13:45,416 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:13:45,465 - WARNING - Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

2025-04-24 11:13:45,467 - ERROR - ERROR during: Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------

[2025-04-24 11:13:45.471922] ERROR during: Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------
2025-04-24 11:13:45,472 - INFO - 
Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 11:13:45,473 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:13:45,542 - WARNING - Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

2025-04-24 11:13:45,543 - ERROR - ERROR during: Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------

[2025-04-24 11:13:45.545407] ERROR during: Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------
2025-04-24 11:13:45,546 - INFO - 
Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 11:13:45,547 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 11:13:45,602 - WARNING - Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

2025-04-24 11:13:45,610 - ERROR - ERROR during: Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------

[2025-04-24 11:13:45.611954] ERROR during: Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------
2025-04-24 11:13:45,613 - INFO - 
Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 11:13:45,613 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 11:13:45,666 - WARNING - Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

2025-04-24 11:13:45,667 - ERROR - ERROR during: Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------

[2025-04-24 11:13:45.668695] ERROR during: Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------
2025-04-24 11:13:45,669 - INFO - 
Run 5/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 11:13:45,670 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 11:13:45,745 - WARNING - Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

2025-04-24 11:13:45,746 - ERROR - ERROR during: Run 5/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------

[2025-04-24 11:13:45.747889] ERROR during: Run 5/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------
2025-04-24 11:13:45,748 - INFO - 
Run 6/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:13:45,749 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:13:45,818 - WARNING - Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

2025-04-24 11:13:45,819 - ERROR - ERROR during: Run 6/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------

[2025-04-24 11:13:45.821445] ERROR during: Run 6/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------
2025-04-24 11:13:45,822 - INFO - 
Run 7/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 11:13:45,822 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:13:45,875 - WARNING - Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

2025-04-24 11:13:45,876 - ERROR - ERROR during: Run 7/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------

[2025-04-24 11:13:45.878149] ERROR during: Run 7/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------
2025-04-24 11:13:45,878 - INFO - 
Run 8/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 11:13:45,879 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 11:13:45,954 - WARNING - Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

2025-04-24 11:13:45,955 - ERROR - ERROR during: Run 8/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------

[2025-04-24 11:13:45.957199] ERROR during: Run 8/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------
2025-04-24 11:13:45,957 - INFO - 
Run 9/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 11:13:45,958 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 11:13:46,023 - WARNING - Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

2025-04-24 11:13:46,024 - ERROR - ERROR during: Run 9/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------

[2025-04-24 11:13:46.026480] ERROR during: Run 9/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------
2025-04-24 11:13:46,027 - INFO - 
Run 10/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 11:13:46,028 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 11:13:46,079 - WARNING - Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

2025-04-24 11:13:46,080 - ERROR - ERROR during: Run 10/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------

[2025-04-24 11:13:46.082116] ERROR during: Run 10/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 5, in <module>
    import torch
ModuleNotFoundError: No module named 'torch'

----------------------------------------
2025-04-24 11:13:46,085 - INFO - --- Security Benchmark Automation Finished (QUANTIZED MODELS) ---
2025-04-24 11:16:18,711 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 11:16:18,712 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 11:16:18,713 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 11:16:18,713 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 11:16:18,715 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 11:16:18,715 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_111618.log
2025-04-24 11:16:18,716 - INFO - Models to run: ['Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 11:16:18,716 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 11:16:18,717 - INFO - Running 100 samples per benchmark.
2025-04-24 11:16:18,717 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 11:16:18,718 - INFO - 
Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:16:18,720 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:16:38,633 - WARNING - Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 9, in <module>
    from evaluation_utils import setup_logging, init_nvml, shutdown_nvml, get_memory_usage, NVML_FOUND
  File "/workspace/code/model_evaluator/evaluation_utils.py", line 1, in <module>
    import psutil
ModuleNotFoundError: No module named 'psutil'

2025-04-24 11:16:38,635 - ERROR - ERROR during: Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 9, in <module>
    from evaluation_utils import setup_logging, init_nvml, shutdown_nvml, get_memory_usage, NVML_FOUND
  File "/workspace/code/model_evaluator/evaluation_utils.py", line 1, in <module>
    import psutil
ModuleNotFoundError: No module named 'psutil'

----------------------------------------

[2025-04-24 11:16:38.637629] ERROR during: Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 9, in <module>
    from evaluation_utils import setup_logging, init_nvml, shutdown_nvml, get_memory_usage, NVML_FOUND
  File "/workspace/code/model_evaluator/evaluation_utils.py", line 1, in <module>
    import psutil
ModuleNotFoundError: No module named 'psutil'

----------------------------------------
2025-04-24 11:16:38,638 - INFO - 
Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 11:16:38,640 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:16:45,152 - WARNING - Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 9, in <module>
    from evaluation_utils import setup_logging, init_nvml, shutdown_nvml, get_memory_usage, NVML_FOUND
  File "/workspace/code/model_evaluator/evaluation_utils.py", line 1, in <module>
    import psutil
ModuleNotFoundError: No module named 'psutil'

2025-04-24 11:16:45,153 - ERROR - ERROR during: Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 9, in <module>
    from evaluation_utils import setup_logging, init_nvml, shutdown_nvml, get_memory_usage, NVML_FOUND
  File "/workspace/code/model_evaluator/evaluation_utils.py", line 1, in <module>
    import psutil
ModuleNotFoundError: No module named 'psutil'

----------------------------------------

[2025-04-24 11:16:45.156311] ERROR during: Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 9, in <module>
    from evaluation_utils import setup_logging, init_nvml, shutdown_nvml, get_memory_usage, NVML_FOUND
  File "/workspace/code/model_evaluator/evaluation_utils.py", line 1, in <module>
    import psutil
ModuleNotFoundError: No module named 'psutil'

----------------------------------------
2025-04-24 11:16:45,157 - INFO - 
Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 11:16:45,158 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 11:16:50,946 - WARNING - Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 9, in <module>
    from evaluation_utils import setup_logging, init_nvml, shutdown_nvml, get_memory_usage, NVML_FOUND
  File "/workspace/code/model_evaluator/evaluation_utils.py", line 1, in <module>
    import psutil
ModuleNotFoundError: No module named 'psutil'

2025-04-24 11:16:50,947 - ERROR - ERROR during: Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 9, in <module>
    from evaluation_utils import setup_logging, init_nvml, shutdown_nvml, get_memory_usage, NVML_FOUND
  File "/workspace/code/model_evaluator/evaluation_utils.py", line 1, in <module>
    import psutil
ModuleNotFoundError: No module named 'psutil'

----------------------------------------

[2025-04-24 11:16:50.950302] ERROR during: Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 9, in <module>
    from evaluation_utils import setup_logging, init_nvml, shutdown_nvml, get_memory_usage, NVML_FOUND
  File "/workspace/code/model_evaluator/evaluation_utils.py", line 1, in <module>
    import psutil
ModuleNotFoundError: No module named 'psutil'

----------------------------------------
2025-04-24 11:16:50,951 - INFO - 
Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 11:16:50,952 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 11:16:57,928 - WARNING - Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 9, in <module>
    from evaluation_utils import setup_logging, init_nvml, shutdown_nvml, get_memory_usage, NVML_FOUND
  File "/workspace/code/model_evaluator/evaluation_utils.py", line 1, in <module>
    import psutil
ModuleNotFoundError: No module named 'psutil'

2025-04-24 11:16:57,930 - ERROR - ERROR during: Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 9, in <module>
    from evaluation_utils import setup_logging, init_nvml, shutdown_nvml, get_memory_usage, NVML_FOUND
  File "/workspace/code/model_evaluator/evaluation_utils.py", line 1, in <module>
    import psutil
ModuleNotFoundError: No module named 'psutil'

----------------------------------------

[2025-04-24 11:16:57.933032] ERROR during: Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 9, in <module>
    from evaluation_utils import setup_logging, init_nvml, shutdown_nvml, get_memory_usage, NVML_FOUND
  File "/workspace/code/model_evaluator/evaluation_utils.py", line 1, in <module>
    import psutil
ModuleNotFoundError: No module named 'psutil'

----------------------------------------
2025-04-24 11:16:57,933 - INFO - 
Run 5/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 11:16:57,935 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 11:17:31,142 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 11:17:31,143 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 11:17:31,143 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 11:17:31,144 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 11:17:31,145 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 11:17:31,146 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_111731.log
2025-04-24 11:17:31,146 - INFO - Models to run: ['Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 11:17:31,147 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 11:17:31,147 - INFO - Running 100 samples per benchmark.
2025-04-24 11:17:31,148 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 11:17:31,148 - INFO - 
Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:17:31,151 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:17:37,523 - WARNING - Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

2025-04-24 11:17:37,525 - ERROR - ERROR during: Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------

[2025-04-24 11:17:37.527696] ERROR during: Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------
2025-04-24 11:17:37,528 - INFO - 
Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 11:17:37,530 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:17:43,924 - WARNING - Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

2025-04-24 11:17:43,925 - ERROR - ERROR during: Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------

[2025-04-24 11:17:43.928505] ERROR during: Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------
2025-04-24 11:17:43,929 - INFO - 
Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 11:17:43,931 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 11:17:50,227 - WARNING - Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

2025-04-24 11:17:50,228 - ERROR - ERROR during: Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------

[2025-04-24 11:17:50.231871] ERROR during: Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------
2025-04-24 11:17:50,232 - INFO - 
Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 11:17:50,234 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 11:17:56,617 - WARNING - Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

2025-04-24 11:17:56,618 - ERROR - ERROR during: Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------

[2025-04-24 11:17:56.622122] ERROR during: Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------
2025-04-24 11:17:56,623 - INFO - 
Run 5/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 11:17:56,624 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 11:18:02,569 - WARNING - Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

2025-04-24 11:18:02,571 - ERROR - ERROR during: Run 5/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------

[2025-04-24 11:18:02.573610] ERROR during: Run 5/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------
2025-04-24 11:18:02,574 - INFO - 
Run 6/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:18:02,576 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:18:08,520 - WARNING - Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

2025-04-24 11:18:08,522 - ERROR - ERROR during: Run 6/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------

[2025-04-24 11:18:08.525017] ERROR during: Run 6/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------
2025-04-24 11:18:08,525 - INFO - 
Run 7/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 11:18:08,527 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:18:14,782 - WARNING - Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

2025-04-24 11:18:14,784 - ERROR - ERROR during: Run 7/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------

[2025-04-24 11:18:14.787284] ERROR during: Run 7/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------
2025-04-24 11:18:14,788 - INFO - 
Run 8/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 11:18:14,789 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 11:18:22,573 - WARNING - Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

2025-04-24 11:18:22,574 - ERROR - ERROR during: Run 8/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------

[2025-04-24 11:18:22.577895] ERROR during: Run 8/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------
2025-04-24 11:18:22,578 - INFO - 
Run 9/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 11:18:22,580 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 11:18:29,106 - WARNING - Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

2025-04-24 11:18:29,109 - ERROR - ERROR during: Run 9/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------

[2025-04-24 11:18:29.112464] ERROR during: Run 9/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------
2025-04-24 11:18:29,115 - INFO - 
Run 10/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 11:18:29,116 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 11:18:35,257 - WARNING - Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

2025-04-24 11:18:35,259 - ERROR - ERROR during: Run 10/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------

[2025-04-24 11:18:35.262213] ERROR during: Run 10/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
  Return Code: 1
  Stderr:
/workspace/testbedvenv/lib/python3.11/site-packages/torch/_subclasses/functional_tensor.py:276: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:81.)
  cpu = _conversion_method_template(device=torch.device("cpu"))
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 10, in <module>
    from model_loader import load_model_and_tokenizer
  File "/workspace/code/model_evaluator/model_loader.py", line 4, in <module>
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel
ModuleNotFoundError: No module named 'transformers'

----------------------------------------
2025-04-24 11:18:35,263 - INFO - --- Security Benchmark Automation Finished (QUANTIZED MODELS) ---
2025-04-24 11:20:54,796 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 11:20:54,797 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 11:20:54,797 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 11:20:54,798 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 11:20:54,798 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 11:20:54,808 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_112054.log
2025-04-24 11:20:54,809 - INFO - Models to run: ['Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 11:20:54,810 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 11:20:54,811 - INFO - Running 100 samples per benchmark.
2025-04-24 11:20:54,811 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 11:20:54,812 - INFO - 
Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:20:54,813 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:21:17,639 - WARNING - Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

2025-04-24 11:21:17,640 - ERROR - ERROR during: Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

----------------------------------------

[2025-04-24 11:21:17.642904] ERROR during: Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

----------------------------------------
2025-04-24 11:21:17,643 - INFO - 
Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 11:21:17,645 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:21:31,234 - WARNING - Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

2025-04-24 11:21:31,236 - ERROR - ERROR during: Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

----------------------------------------

[2025-04-24 11:21:31.238454] ERROR during: Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

----------------------------------------
2025-04-24 11:21:31,239 - INFO - 
Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 11:21:31,240 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 11:21:43,923 - WARNING - Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

2025-04-24 11:21:43,923 - ERROR - ERROR during: Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

----------------------------------------

[2025-04-24 11:21:43.926102] ERROR during: Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

----------------------------------------
2025-04-24 11:21:43,926 - INFO - 
Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 11:21:43,927 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 11:21:56,857 - WARNING - Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

2025-04-24 11:21:56,859 - ERROR - ERROR during: Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

----------------------------------------

[2025-04-24 11:21:56.862416] ERROR during: Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

----------------------------------------
2025-04-24 11:21:56,863 - INFO - 
Run 5/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 11:21:56,864 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 11:22:11,377 - WARNING - Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

2025-04-24 11:22:11,378 - ERROR - ERROR during: Run 5/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

----------------------------------------

[2025-04-24 11:22:11.381705] ERROR during: Run 5/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

----------------------------------------
2025-04-24 11:22:11,382 - INFO - 
Run 6/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:22:11,385 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:22:27,579 - WARNING - Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

2025-04-24 11:22:27,580 - ERROR - ERROR during: Run 6/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

----------------------------------------

[2025-04-24 11:22:27.583483] ERROR during: Run 6/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

----------------------------------------
2025-04-24 11:22:27,588 - INFO - 
Run 7/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 11:22:27,590 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:22:40,651 - WARNING - Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

2025-04-24 11:22:40,652 - ERROR - ERROR during: Run 7/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

----------------------------------------

[2025-04-24 11:22:40.655657] ERROR during: Run 7/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
  Return Code: 1
  Stderr:
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 11, in <module>
    from benchmark_loader import load_benchmark_prompts
  File "/workspace/code/model_evaluator/benchmark_loader.py", line 5, in <module>
    from datasets import load_dataset, load_from_disk, DatasetDict
ModuleNotFoundError: No module named 'datasets'

----------------------------------------
2025-04-24 11:22:40,656 - INFO - 
Run 8/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 11:22:40,657 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 11:23:32,872 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 11:23:32,873 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 11:23:32,874 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 11:23:32,874 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 11:23:32,875 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 11:23:32,875 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_112332.log
2025-04-24 11:23:32,876 - INFO - Models to run: ['Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 11:23:32,876 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 11:23:32,877 - INFO - Running 100 samples per benchmark.
2025-04-24 11:23:32,877 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 11:23:32,878 - INFO - 
Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:23:32,880 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:24:29,952 - WARNING - Stderr:
2025-04-24 11:23:56,362 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:23:56,516 - INFO - --- Starting Evaluation ---
2025-04-24 11:23:56,516 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:23:56,516 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:23:56,516 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-112356_metrics.json
2025-04-24 11:23:56,516 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-112356_outputs.json
2025-04-24 11:23:56,516 - INFO - NVML Reporting Active: False
2025-04-24 11:23:56,516 - INFO - Initial RAM usage: 523.46 MB
2025-04-24 11:23:56,517 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:23:56,543 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:23:56,740 - INFO - Tokenizer loaded.
2025-04-24 11:24:29,141 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:24:29,141 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
2025-04-24 11:24:29,166 - ERROR - Error loading model: Using a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4139, in from_pretrained
    raise ValueError(
ValueError: Using a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`
2025-04-24 11:24:29,180 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:24:29,181 - INFO - Cleaning up resources...
2025-04-24 11:24:29,181 - INFO - CUDA cache cleared.
2025-04-24 11:24:29,181 - INFO - --- Evaluation Complete ---

2025-04-24 11:24:29,955 - INFO - SUCCESS: Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 11:24:29,955 - INFO - 
Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 11:24:29,957 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:24:58,331 - WARNING - Stderr:
2025-04-24 11:24:45,976 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:24:46,126 - INFO - --- Starting Evaluation ---
2025-04-24 11:24:46,126 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:24:46,126 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:24:46,126 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-112446_metrics.json
2025-04-24 11:24:46,126 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-112446_outputs.json
2025-04-24 11:24:46,126 - INFO - NVML Reporting Active: False
2025-04-24 11:24:46,126 - INFO - Initial RAM usage: 515.34 MB
2025-04-24 11:24:46,126 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:24:46,149 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:24:46,272 - INFO - Tokenizer loaded.
2025-04-24 11:24:57,652 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:24:57,652 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
2025-04-24 11:24:57,677 - ERROR - Error loading model: Using a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4139, in from_pretrained
    raise ValueError(
ValueError: Using a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`
2025-04-24 11:24:57,686 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:24:57,687 - INFO - Cleaning up resources...
2025-04-24 11:24:57,688 - INFO - CUDA cache cleared.
2025-04-24 11:24:57,688 - INFO - --- Evaluation Complete ---

2025-04-24 11:24:58,332 - INFO - SUCCESS: Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 11:24:58,334 - INFO - 
Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 11:24:58,335 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 11:25:27,415 - WARNING - Stderr:
2025-04-24 11:25:14,924 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:25:15,068 - INFO - --- Starting Evaluation ---
2025-04-24 11:25:15,068 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq'}
2025-04-24 11:25:15,068 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:25:15,068 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-112515_metrics.json
2025-04-24 11:25:15,068 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-112515_outputs.json
2025-04-24 11:25:15,068 - INFO - NVML Reporting Active: False
2025-04-24 11:25:15,068 - INFO - Initial RAM usage: 516.04 MB
2025-04-24 11:25:15,068 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:25:15,091 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:25:15,273 - INFO - Tokenizer loaded.
2025-04-24 11:25:26,703 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:25:26,704 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
2025-04-24 11:25:26,728 - ERROR - Error loading model: Using a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4139, in from_pretrained
    raise ValueError(
ValueError: Using a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`
2025-04-24 11:25:26,738 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:25:26,739 - INFO - Cleaning up resources...
2025-04-24 11:25:26,739 - INFO - CUDA cache cleared.
2025-04-24 11:25:26,739 - INFO - --- Evaluation Complete ---

2025-04-24 11:25:27,417 - INFO - SUCCESS: Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 11:25:27,418 - INFO - 
Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 11:25:27,419 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 11:25:54,370 - WARNING - Stderr:
2025-04-24 11:25:42,699 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:25:42,847 - INFO - --- Starting Evaluation ---
2025-04-24 11:25:42,847 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-vsp'}
2025-04-24 11:25:42,847 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:25:42,847 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-vsp_Phi-3-mini-4k-instruct-GPTQ_20250424-112542_metrics.json
2025-04-24 11:25:42,847 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-vsp_Phi-3-mini-4k-instruct-GPTQ_20250424-112542_outputs.json
2025-04-24 11:25:42,847 - INFO - NVML Reporting Active: False
2025-04-24 11:25:42,847 - INFO - Initial RAM usage: 515.30 MB
2025-04-24 11:25:42,847 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:25:42,872 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:25:43,003 - INFO - Tokenizer loaded.
2025-04-24 11:25:53,699 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:25:53,700 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
2025-04-24 11:25:53,723 - ERROR - Error loading model: Using a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4139, in from_pretrained
    raise ValueError(
ValueError: Using a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`
2025-04-24 11:25:53,733 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:25:53,734 - INFO - Cleaning up resources...
2025-04-24 11:25:53,734 - INFO - CUDA cache cleared.
2025-04-24 11:25:53,734 - INFO - --- Evaluation Complete ---

2025-04-24 11:25:54,372 - INFO - SUCCESS: Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'. Check output files in /workspace/results/mod/48/
2025-04-24 11:25:54,374 - INFO - 
Run 5/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 11:25:54,375 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 11:26:32,849 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 11:26:32,849 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 11:26:32,850 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 11:26:32,851 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 11:26:32,851 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 11:26:32,852 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_112632.log
2025-04-24 11:26:32,852 - INFO - Models to run: ['Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 11:26:32,853 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 11:26:32,853 - INFO - Running 100 samples per benchmark.
2025-04-24 11:26:32,854 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 11:26:32,854 - INFO - 
Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:26:32,856 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:26:59,014 - WARNING - Stderr:
2025-04-24 11:26:48,983 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:26:49,143 - INFO - --- Starting Evaluation ---
2025-04-24 11:26:49,143 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:26:49,143 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:26:49,143 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-112649_metrics.json
2025-04-24 11:26:49,143 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-112649_outputs.json
2025-04-24 11:26:49,143 - INFO - NVML Reporting Active: False
2025-04-24 11:26:49,143 - INFO - Initial RAM usage: 521.80 MB
2025-04-24 11:26:49,143 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:26:49,173 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:26:49,313 - INFO - Tokenizer loaded.
2025-04-24 11:26:58,289 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:26:58,289 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
2025-04-24 11:26:58,313 - ERROR - Error loading model: Loading a GPTQ quantized model requires optimum (`pip install optimum`)
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4220, in from_pretrained
    hf_quantizer = AutoHfQuantizer.from_config(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 173, in from_config
    return target_cls(quantization_config, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/quantizer_gptq.py", line 49, in __init__
    raise ImportError("Loading a GPTQ quantized model requires optimum (`pip install optimum`)")
ImportError: Loading a GPTQ quantized model requires optimum (`pip install optimum`)
2025-04-24 11:26:58,326 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:26:58,331 - INFO - Cleaning up resources...
2025-04-24 11:26:58,331 - INFO - CUDA cache cleared.
2025-04-24 11:26:58,331 - INFO - --- Evaluation Complete ---

2025-04-24 11:26:59,015 - INFO - SUCCESS: Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 11:26:59,016 - INFO - 
Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 11:26:59,017 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:27:39,121 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 11:27:39,122 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 11:27:39,122 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 11:27:39,123 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 11:27:39,124 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 11:27:39,124 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_112739.log
2025-04-24 11:27:39,125 - INFO - Models to run: ['Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 11:27:39,125 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 11:27:39,126 - INFO - Running 100 samples per benchmark.
2025-04-24 11:27:39,126 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 11:27:39,127 - INFO - 
Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:27:39,129 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:28:05,878 - WARNING - Stderr:
2025-04-24 11:27:54,633 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:27:54,782 - INFO - --- Starting Evaluation ---
2025-04-24 11:27:54,783 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:27:54,783 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:27:54,783 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-112754_metrics.json
2025-04-24 11:27:54,783 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-112754_outputs.json
2025-04-24 11:27:54,783 - INFO - NVML Reporting Active: False
2025-04-24 11:27:54,783 - INFO - Initial RAM usage: 519.12 MB
2025-04-24 11:27:54,783 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:27:54,811 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:27:54,946 - INFO - Tokenizer loaded.
2025-04-24 11:28:04,866 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:28:04,866 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
2025-04-24 11:28:05,157 - ERROR - Error loading model: Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. 
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4228, in from_pretrained
    hf_quantizer.validate_environment(
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/quantizer_gptq.py", line 67, in validate_environment
    raise ImportError(
ImportError: Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. 
2025-04-24 11:28:05,168 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:28:05,169 - INFO - Cleaning up resources...
2025-04-24 11:28:05,169 - INFO - CUDA cache cleared.
2025-04-24 11:28:05,169 - INFO - --- Evaluation Complete ---

2025-04-24 11:28:05,880 - INFO - SUCCESS: Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 11:28:05,881 - INFO - 
Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 11:28:05,882 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:29:28,397 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 11:29:28,408 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 11:29:28,409 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 11:29:28,410 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 11:29:28,411 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 11:29:28,411 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_112928.log
2025-04-24 11:29:28,412 - INFO - Models to run: ['Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 11:29:28,412 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 11:29:28,414 - INFO - Running 100 samples per benchmark.
2025-04-24 11:29:28,414 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 11:29:28,415 - INFO - 
Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:29:28,416 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:29:56,889 - WARNING - Stderr:
2025-04-24 11:29:44,652 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:29:44,820 - INFO - --- Starting Evaluation ---
2025-04-24 11:29:44,821 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:29:44,821 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:29:44,821 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-112944_metrics.json
2025-04-24 11:29:44,821 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-112944_outputs.json
2025-04-24 11:29:44,821 - INFO - NVML Reporting Active: False
2025-04-24 11:29:44,821 - INFO - Initial RAM usage: 516.33 MB
2025-04-24 11:29:44,821 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:29:44,847 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:29:44,978 - INFO - Tokenizer loaded.
2025-04-24 11:29:55,985 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:29:55,985 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
2025-04-24 11:29:56,111 - ERROR - Error loading model: Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. 
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4228, in from_pretrained
    hf_quantizer.validate_environment(
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/quantizer_gptq.py", line 67, in validate_environment
    raise ImportError(
ImportError: Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. 
2025-04-24 11:29:56,124 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:29:56,125 - INFO - Cleaning up resources...
2025-04-24 11:29:56,125 - INFO - CUDA cache cleared.
2025-04-24 11:29:56,125 - INFO - --- Evaluation Complete ---

2025-04-24 11:29:56,891 - INFO - SUCCESS: Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 11:29:56,893 - INFO - 
Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 11:29:56,895 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:30:57,763 - WARNING - Stderr:
2025-04-24 11:30:30,475 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:30:30,668 - INFO - --- Starting Evaluation ---
2025-04-24 11:30:30,668 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:30:30,668 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:30:30,668 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-113030_metrics.json
2025-04-24 11:30:30,668 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-113030_outputs.json
2025-04-24 11:30:30,668 - INFO - NVML Reporting Active: False
2025-04-24 11:30:30,668 - INFO - Initial RAM usage: 512.86 MB
2025-04-24 11:30:30,668 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:30:30,764 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:30:30,998 - INFO - Tokenizer loaded.
2025-04-24 11:30:56,897 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:30:56,897 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
2025-04-24 11:30:56,989 - ERROR - Error loading model: Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. 
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4228, in from_pretrained
    hf_quantizer.validate_environment(
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/quantizer_gptq.py", line 67, in validate_environment
    raise ImportError(
ImportError: Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. 
2025-04-24 11:30:57,010 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:30:57,011 - INFO - Cleaning up resources...
2025-04-24 11:30:57,012 - INFO - CUDA cache cleared.
2025-04-24 11:30:57,012 - INFO - --- Evaluation Complete ---

2025-04-24 11:30:57,765 - INFO - SUCCESS: Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 11:30:57,765 - INFO - 
Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 11:30:57,766 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 11:31:27,534 - WARNING - Stderr:
2025-04-24 11:31:15,648 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:31:15,794 - INFO - --- Starting Evaluation ---
2025-04-24 11:31:15,794 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq'}
2025-04-24 11:31:15,794 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:31:15,794 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-113115_metrics.json
2025-04-24 11:31:15,794 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-113115_outputs.json
2025-04-24 11:31:15,794 - INFO - NVML Reporting Active: False
2025-04-24 11:31:15,794 - INFO - Initial RAM usage: 515.07 MB
2025-04-24 11:31:15,794 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:31:15,816 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:31:15,950 - INFO - Tokenizer loaded.
2025-04-24 11:31:26,702 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:31:26,703 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
2025-04-24 11:31:26,799 - ERROR - Error loading model: Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. 
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4228, in from_pretrained
    hf_quantizer.validate_environment(
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/quantizer_gptq.py", line 67, in validate_environment
    raise ImportError(
ImportError: Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. 
2025-04-24 11:31:26,821 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:31:26,822 - INFO - Cleaning up resources...
2025-04-24 11:31:26,823 - INFO - CUDA cache cleared.
2025-04-24 11:31:26,823 - INFO - --- Evaluation Complete ---

2025-04-24 11:31:27,536 - INFO - SUCCESS: Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 11:31:27,537 - INFO - 
Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 11:31:27,538 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 11:31:57,600 - WARNING - Stderr:
2025-04-24 11:31:45,979 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:31:46,131 - INFO - --- Starting Evaluation ---
2025-04-24 11:31:46,131 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-vsp'}
2025-04-24 11:31:46,131 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:31:46,131 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-vsp_Phi-3-mini-4k-instruct-GPTQ_20250424-113146_metrics.json
2025-04-24 11:31:46,131 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-vsp_Phi-3-mini-4k-instruct-GPTQ_20250424-113146_outputs.json
2025-04-24 11:31:46,131 - INFO - NVML Reporting Active: False
2025-04-24 11:31:46,131 - INFO - Initial RAM usage: 519.12 MB
2025-04-24 11:31:46,131 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:31:46,158 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:31:46,317 - INFO - Tokenizer loaded.
2025-04-24 11:31:56,761 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:31:56,761 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
2025-04-24 11:31:56,869 - ERROR - Error loading model: Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. 
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4228, in from_pretrained
    hf_quantizer.validate_environment(
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/quantizer_gptq.py", line 67, in validate_environment
    raise ImportError(
ImportError: Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. 
2025-04-24 11:31:56,881 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:31:56,883 - INFO - Cleaning up resources...
2025-04-24 11:31:56,883 - INFO - CUDA cache cleared.
2025-04-24 11:31:56,883 - INFO - --- Evaluation Complete ---

2025-04-24 11:31:57,611 - INFO - SUCCESS: Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'. Check output files in /workspace/results/mod/48/
2025-04-24 11:31:57,612 - INFO - 
Run 5/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 11:31:57,614 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 11:32:31,965 - WARNING - Stderr:
2025-04-24 11:32:20,269 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:32:20,422 - INFO - --- Starting Evaluation ---
2025-04-24 11:32:20,422 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-rcm'}
2025-04-24 11:32:20,422 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:32:20,422 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-rcm_Phi-3-mini-4k-instruct-GPTQ_20250424-113220_metrics.json
2025-04-24 11:32:20,422 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-rcm_Phi-3-mini-4k-instruct-GPTQ_20250424-113220_outputs.json
2025-04-24 11:32:20,422 - INFO - NVML Reporting Active: False
2025-04-24 11:32:20,422 - INFO - Initial RAM usage: 516.86 MB
2025-04-24 11:32:20,422 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:32:20,450 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:32:20,611 - INFO - Tokenizer loaded.
2025-04-24 11:32:31,147 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:32:31,147 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
2025-04-24 11:32:31,257 - ERROR - Error loading model: Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. 
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4228, in from_pretrained
    hf_quantizer.validate_environment(
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/quantizer_gptq.py", line 67, in validate_environment
    raise ImportError(
ImportError: Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. 
2025-04-24 11:32:31,270 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:32:31,271 - INFO - Cleaning up resources...
2025-04-24 11:32:31,271 - INFO - CUDA cache cleared.
2025-04-24 11:32:31,271 - INFO - --- Evaluation Complete ---

2025-04-24 11:32:31,968 - INFO - SUCCESS: Run 5/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'. Check output files in /workspace/results/mod/48/
2025-04-24 11:32:31,969 - INFO - 
Run 6/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:32:31,971 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:32:49,571 - WARNING - Stderr:
2025-04-24 11:32:48,890 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:32:49,039 - INFO - --- Starting Evaluation ---
2025-04-24 11:32:49,039 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:32:49,039 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:32:49,039 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_Mistral-7B-Instruct-v0.3-GPTQ_20250424-113249_metrics.json
2025-04-24 11:32:49,039 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_Mistral-7B-Instruct-v0.3-GPTQ_20250424-113249_outputs.json
2025-04-24 11:32:49,039 - INFO - NVML Reporting Active: False
2025-04-24 11:32:49,039 - INFO - Initial RAM usage: 517.66 MB
2025-04-24 11:32:49,039 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 11:32:49,060 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:32:49,089 - ERROR - Error loading tokenizer: 
 requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2302, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama_fast.py", line 157, in __init__
    super().__init__(
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py", line 108, in __init__
    raise ValueError(
ValueError: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 20, in load_model_and_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, legacy=False)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 1009, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2062, in from_pretrained
    return cls._from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2303, in _from_pretrained
    except import_protobuf_decode_error():
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 87, in import_protobuf_decode_error
    raise ImportError(PROTOBUF_IMPORT_ERROR.format(error_message))
ImportError: 
 requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.

2025-04-24 11:32:49,098 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:32:49,108 - INFO - Cleaning up resources...
2025-04-24 11:32:49,108 - INFO - CUDA cache cleared.
2025-04-24 11:32:49,108 - INFO - --- Evaluation Complete ---

2025-04-24 11:32:49,573 - INFO - SUCCESS: Run 6/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 11:32:49,574 - INFO - 
Run 7/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 11:32:49,576 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:33:07,125 - WARNING - Stderr:
2025-04-24 11:33:06,497 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:33:06,643 - INFO - --- Starting Evaluation ---
2025-04-24 11:33:06,643 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:33:06,643 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:33:06,643 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_Mistral-7B-Instruct-v0.3-GPTQ_20250424-113306_metrics.json
2025-04-24 11:33:06,643 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_Mistral-7B-Instruct-v0.3-GPTQ_20250424-113306_outputs.json
2025-04-24 11:33:06,643 - INFO - NVML Reporting Active: False
2025-04-24 11:33:06,643 - INFO - Initial RAM usage: 519.35 MB
2025-04-24 11:33:06,643 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 11:33:06,664 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:33:06,683 - ERROR - Error loading tokenizer: 
 requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2302, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama_fast.py", line 157, in __init__
    super().__init__(
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py", line 108, in __init__
    raise ValueError(
ValueError: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 20, in load_model_and_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, legacy=False)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 1009, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2062, in from_pretrained
    return cls._from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2303, in _from_pretrained
    except import_protobuf_decode_error():
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 87, in import_protobuf_decode_error
    raise ImportError(PROTOBUF_IMPORT_ERROR.format(error_message))
ImportError: 
 requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.

2025-04-24 11:33:06,691 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:33:06,692 - INFO - Cleaning up resources...
2025-04-24 11:33:06,692 - INFO - CUDA cache cleared.
2025-04-24 11:33:06,692 - INFO - --- Evaluation Complete ---

2025-04-24 11:33:07,127 - INFO - SUCCESS: Run 7/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 11:33:07,128 - INFO - 
Run 8/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 11:33:07,130 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 11:33:22,669 - WARNING - Stderr:
2025-04-24 11:33:22,043 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:33:22,189 - INFO - --- Starting Evaluation ---
2025-04-24 11:33:22,189 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq'}
2025-04-24 11:33:22,189 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:33:22,189 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_Mistral-7B-Instruct-v0.3-GPTQ_20250424-113322_metrics.json
2025-04-24 11:33:22,189 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_Mistral-7B-Instruct-v0.3-GPTQ_20250424-113322_outputs.json
2025-04-24 11:33:22,189 - INFO - NVML Reporting Active: False
2025-04-24 11:33:22,189 - INFO - Initial RAM usage: 521.80 MB
2025-04-24 11:33:22,189 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 11:33:22,211 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:33:22,233 - ERROR - Error loading tokenizer: 
 requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2302, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama_fast.py", line 157, in __init__
    super().__init__(
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py", line 108, in __init__
    raise ValueError(
ValueError: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 20, in load_model_and_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, legacy=False)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 1009, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2062, in from_pretrained
    return cls._from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2303, in _from_pretrained
    except import_protobuf_decode_error():
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 87, in import_protobuf_decode_error
    raise ImportError(PROTOBUF_IMPORT_ERROR.format(error_message))
ImportError: 
 requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.

2025-04-24 11:33:22,242 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:33:22,243 - INFO - Cleaning up resources...
2025-04-24 11:33:22,243 - INFO - CUDA cache cleared.
2025-04-24 11:33:22,243 - INFO - --- Evaluation Complete ---

2025-04-24 11:33:22,671 - INFO - SUCCESS: Run 8/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 11:33:22,672 - INFO - 
Run 9/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 11:33:22,673 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 11:33:43,851 - WARNING - Stderr:
2025-04-24 11:33:43,045 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:33:43,208 - INFO - --- Starting Evaluation ---
2025-04-24 11:33:43,208 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-vsp'}
2025-04-24 11:33:43,208 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:33:43,208 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-vsp_Mistral-7B-Instruct-v0.3-GPTQ_20250424-113343_metrics.json
2025-04-24 11:33:43,209 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-vsp_Mistral-7B-Instruct-v0.3-GPTQ_20250424-113343_outputs.json
2025-04-24 11:33:43,209 - INFO - NVML Reporting Active: False
2025-04-24 11:33:43,209 - INFO - Initial RAM usage: 519.27 MB
2025-04-24 11:33:43,209 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 11:33:43,237 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:33:43,287 - ERROR - Error loading tokenizer: 
 requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2302, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama_fast.py", line 157, in __init__
    super().__init__(
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py", line 108, in __init__
    raise ValueError(
ValueError: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 20, in load_model_and_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, legacy=False)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 1009, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2062, in from_pretrained
    return cls._from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2303, in _from_pretrained
    except import_protobuf_decode_error():
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 87, in import_protobuf_decode_error
    raise ImportError(PROTOBUF_IMPORT_ERROR.format(error_message))
ImportError: 
 requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.

2025-04-24 11:33:43,323 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:33:43,325 - INFO - Cleaning up resources...
2025-04-24 11:33:43,325 - INFO - CUDA cache cleared.
2025-04-24 11:33:43,325 - INFO - --- Evaluation Complete ---

2025-04-24 11:33:43,855 - INFO - SUCCESS: Run 9/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-vsp'. Check output files in /workspace/results/mod/48/
2025-04-24 11:33:43,857 - INFO - 
Run 10/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 11:33:43,859 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 11:34:00,890 - WARNING - Stderr:
2025-04-24 11:34:00,278 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:34:00,424 - INFO - --- Starting Evaluation ---
2025-04-24 11:34:00,424 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-rcm'}
2025-04-24 11:34:00,424 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:34:00,424 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-rcm_Mistral-7B-Instruct-v0.3-GPTQ_20250424-113400_metrics.json
2025-04-24 11:34:00,424 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-rcm_Mistral-7B-Instruct-v0.3-GPTQ_20250424-113400_outputs.json
2025-04-24 11:34:00,424 - INFO - NVML Reporting Active: False
2025-04-24 11:34:00,424 - INFO - Initial RAM usage: 517.41 MB
2025-04-24 11:34:00,424 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 11:34:00,445 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:34:00,466 - ERROR - Error loading tokenizer: 
 requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2302, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/tokenization_llama_fast.py", line 157, in __init__
    super().__init__(
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py", line 108, in __init__
    raise ValueError(
ValueError: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 20, in load_model_and_tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, legacy=False)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 1009, in from_pretrained
    return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2062, in from_pretrained
    return cls._from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 2303, in _from_pretrained
    except import_protobuf_decode_error():
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 87, in import_protobuf_decode_error
    raise ImportError(PROTOBUF_IMPORT_ERROR.format(error_message))
ImportError: 
 requires the protobuf library but it was not found in your environment. Checkout the instructions on the
installation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.

2025-04-24 11:34:00,476 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:34:00,477 - INFO - Cleaning up resources...
2025-04-24 11:34:00,477 - INFO - CUDA cache cleared.
2025-04-24 11:34:00,477 - INFO - --- Evaluation Complete ---

2025-04-24 11:34:00,891 - INFO - SUCCESS: Run 10/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-rcm'. Check output files in /workspace/results/mod/48/
2025-04-24 11:34:00,891 - INFO - --- Security Benchmark Automation Finished (QUANTIZED MODELS) ---
2025-04-24 11:38:43,222 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 11:38:43,222 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 11:38:43,223 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 11:38:43,224 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 11:38:43,224 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 11:38:43,225 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_113843.log
2025-04-24 11:38:43,225 - INFO - Models to run: ['Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 11:38:43,226 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 11:38:43,226 - INFO - Running 100 samples per benchmark.
2025-04-24 11:38:43,227 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 11:38:43,227 - INFO - 
Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:38:43,229 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:39:08,316 - WARNING - Stderr:
2025-04-24 11:38:58,355 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:38:58,509 - INFO - --- Starting Evaluation ---
2025-04-24 11:38:58,509 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:38:58,509 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:38:58,509 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-113858_metrics.json
2025-04-24 11:38:58,509 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-113858_outputs.json
2025-04-24 11:38:58,509 - INFO - NVML Reporting Active: False
2025-04-24 11:38:58,509 - INFO - Initial RAM usage: 517.13 MB
2025-04-24 11:38:58,509 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:38:58,532 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:38:58,662 - INFO - Tokenizer loaded.
2025-04-24 11:39:07,574 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:39:07,574 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
2025-04-24 11:39:07,671 - ERROR - Error loading model: Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. 
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4228, in from_pretrained
    hf_quantizer.validate_environment(
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/quantizer_gptq.py", line 67, in validate_environment
    raise ImportError(
ImportError: Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. 
2025-04-24 11:39:07,682 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:39:07,683 - INFO - Cleaning up resources...
2025-04-24 11:39:07,684 - INFO - CUDA cache cleared.
2025-04-24 11:39:07,684 - INFO - --- Evaluation Complete ---

2025-04-24 11:39:08,319 - INFO - SUCCESS: Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 11:39:08,319 - INFO - 
Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 11:39:08,321 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:39:34,660 - WARNING - Stderr:
2025-04-24 11:39:23,880 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:39:24,027 - INFO - --- Starting Evaluation ---
2025-04-24 11:39:24,027 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:39:24,027 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:39:24,027 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-113924_metrics.json
2025-04-24 11:39:24,027 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-113924_outputs.json
2025-04-24 11:39:24,027 - INFO - NVML Reporting Active: False
2025-04-24 11:39:24,027 - INFO - Initial RAM usage: 519.90 MB
2025-04-24 11:39:24,027 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:39:24,050 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:39:24,182 - INFO - Tokenizer loaded.
2025-04-24 11:39:33,887 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:39:33,887 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
2025-04-24 11:39:33,977 - ERROR - Error loading model: Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. 
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4228, in from_pretrained
    hf_quantizer.validate_environment(
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/quantizer_gptq.py", line 67, in validate_environment
    raise ImportError(
ImportError: Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. 
2025-04-24 11:39:33,988 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:39:33,989 - INFO - Cleaning up resources...
2025-04-24 11:39:33,989 - INFO - CUDA cache cleared.
2025-04-24 11:39:33,989 - INFO - --- Evaluation Complete ---

2025-04-24 11:39:34,662 - INFO - SUCCESS: Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 11:39:34,664 - INFO - 
Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 11:39:34,664 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 11:40:01,231 - WARNING - Stderr:
2025-04-24 11:39:50,391 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:39:50,536 - INFO - --- Starting Evaluation ---
2025-04-24 11:39:50,536 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq'}
2025-04-24 11:39:50,536 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:39:50,536 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-113950_metrics.json
2025-04-24 11:39:50,536 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-113950_outputs.json
2025-04-24 11:39:50,536 - INFO - NVML Reporting Active: False
2025-04-24 11:39:50,536 - INFO - Initial RAM usage: 517.84 MB
2025-04-24 11:39:50,536 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:39:50,560 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:39:50,686 - INFO - Tokenizer loaded.
2025-04-24 11:40:00,440 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:40:00,440 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
2025-04-24 11:40:00,546 - ERROR - Error loading model: Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. 
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4228, in from_pretrained
    hf_quantizer.validate_environment(
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/quantizer_gptq.py", line 67, in validate_environment
    raise ImportError(
ImportError: Loading a GPTQ quantized model requires gptqmodel (`pip install gptqmodel`) or auto-gptq (`pip install auto-gptq`) library. 
2025-04-24 11:40:00,557 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 11:40:00,558 - INFO - Cleaning up resources...
2025-04-24 11:40:00,558 - INFO - CUDA cache cleared.
2025-04-24 11:40:00,558 - INFO - --- Evaluation Complete ---

2025-04-24 11:40:01,233 - INFO - SUCCESS: Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 11:40:01,234 - INFO - 
Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 11:40:01,235 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 11:41:27,961 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 11:41:27,962 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 11:41:27,963 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 11:41:27,964 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 11:41:27,964 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 11:41:27,965 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_114127.log
2025-04-24 11:41:27,965 - INFO - Models to run: ['Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 11:41:27,966 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 11:41:27,967 - INFO - Running 100 samples per benchmark.
2025-04-24 11:41:27,967 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 11:41:27,968 - INFO - 
Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:41:27,970 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:42:13,261 - WARNING - Stderr:
2025-04-24 11:41:43,973 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:41:44,123 - INFO - --- Starting Evaluation ---
2025-04-24 11:41:44,124 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:41:44,124 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:41:44,124 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-114144_metrics.json
2025-04-24 11:41:44,124 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-114144_outputs.json
2025-04-24 11:41:44,124 - INFO - NVML Reporting Active: False
2025-04-24 11:41:44,124 - INFO - Initial RAM usage: 520.35 MB
2025-04-24 11:41:44,124 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:41:44,147 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:41:44,365 - INFO - Tokenizer loaded.
2025-04-24 11:41:53,663 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:41:53,664 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 11:41:56,782 - WARNING - CUDA extension not installed.
2025-04-24 11:41:56,808 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 11:42:11,300 - INFO - CausalLM Model loaded.
2025-04-24 11:42:11,304 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 11:42:11,305 - INFO - Model loaded in 27.18 seconds.
2025-04-24 11:42:11,305 - INFO - Loading prompts from CyberSecEval3 MITRE file: /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json
2025-04-24 11:42:11,350 - INFO - Processing the first 100 items.

Parsing MITRE prompts:   0%|          | 0/100 [00:00<?, ?it/s]2025-04-24 11:42:11,355 - WARNING - Item 5: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:42:11,356 - WARNING - Item 8: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:42:11,357 - WARNING - Item 14: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:42:11,358 - WARNING - Item 21: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:42:11,359 - WARNING - Item 25: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:42:11,360 - WARNING - Item 27: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:42:11,360 - WARNING - Item 30: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:42:11,361 - WARNING - Item 32: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:42:11,361 - WARNING - Item 35: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:42:11,362 - WARNING - Item 36: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:42:11,363 - WARNING - Item 48: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:42:11,364 - WARNING - Item 49: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:42:11,364 - WARNING - Item 52: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:42:11,365 - WARNING - Item 58: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:42:11,366 - WARNING - Item 63: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:42:11,367 - WARNING - Item 72: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:42:11,368 - WARNING - Item 81: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:42:11,369 - WARNING - Item 86: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:42:11,370 - WARNING - Item 88: Regex failed to find/extract 'prompt' value from mutated_prompt_str.

Parsing MITRE prompts: 100%|██████████| 100/100 [00:00<00:00, 5695.15it/s]
2025-04-24 11:42:11,371 - INFO - Successfully loaded 81 prompts from cyberseceval3_mitre.
2025-04-24 11:42:11,372 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 11:42:11,373 - INFO - Starting inference on 81 samples...

Inference Progress:   0%|          | 0/81 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
2025-04-24 11:42:11,735 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:   1%|          | 1/81 [00:00<00:29,  2.67it/s]2025-04-24 11:42:11,755 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,768 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,788 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,800 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,815 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,827 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,839 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,851 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  11%|█         | 9/81 [00:00<00:03, 23.60it/s]2025-04-24 11:42:11,862 - ERROR - Generation failed for T1024: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,873 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,883 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,892 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,900 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,911 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,920 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,929 - ERROR - Generation failed for T1090.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,935 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,944 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,952 - ERROR - Generation failed for T1090.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  25%|██▍       | 20/81 [00:00<00:01, 47.64it/s]2025-04-24 11:42:11,960 - ERROR - Generation failed for T1090.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,966 - ERROR - Generation failed for T1483: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,972 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,979 - ERROR - Generation failed for T1090.003: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,986 - ERROR - Generation failed for T1571: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,993 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:11,999 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,015 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,022 - ERROR - Generation failed for T1095: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,029 - ERROR - Generation failed for T1105: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,037 - ERROR - Generation failed for T1094.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,043 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,050 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,057 - ERROR - Generation failed for T1102: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  42%|████▏     | 34/81 [00:00<00:00, 72.91it/s]2025-04-24 11:42:12,065 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,072 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,078 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,085 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,092 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,099 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,112 - ERROR - Generation failed for T1074: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,119 - ERROR - Generation failed for T1097: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,126 - ERROR - Generation failed for T1483: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,134 - ERROR - Generation failed for T1021.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,141 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,147 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,152 - ERROR - Generation failed for T1090.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,159 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  59%|█████▉    | 48/81 [00:00<00:00, 91.71it/s]2025-04-24 11:42:12,164 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,171 - ERROR - Generation failed for T1090.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,176 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,183 - ERROR - Generation failed for T1071: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,189 - ERROR - Generation failed for T1074: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,193 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,199 - ERROR - Generation failed for T1090.003: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,211 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,217 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,224 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,230 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,235 - ERROR - Generation failed for T1090.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,242 - ERROR - Generation failed for T1090.003: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,247 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,252 - ERROR - Generation failed for T1094: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,259 - ERROR - Generation failed for T1132.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,265 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  80%|████████  | 65/81 [00:00<00:00, 112.27it/s]2025-04-24 11:42:12,271 - ERROR - Generation failed for T1059: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,280 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,287 - ERROR - Generation failed for T1071: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,292 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,299 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,314 - ERROR - Generation failed for T1094: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,321 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,327 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,333 - ERROR - Generation failed for T1071: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,340 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,346 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,354 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,360 - ERROR - Generation failed for T1095: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,366 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  98%|█████████▊| 79/81 [00:00<00:00, 119.88it/s]2025-04-24 11:42:12,372 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:12,377 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress: 100%|██████████| 81/81 [00:01<00:00, 80.53it/s] 
2025-04-24 11:42:12,380 - INFO - 
--- Inference Summary ---
2025-04-24 11:42:12,380 - INFO - Processed 81 samples.
2025-04-24 11:42:12,380 - INFO - Total 'generate' time (sum): 0.00 sec
2025-04-24 11:42:12,380 - INFO - Overall inference loop duration: 1.01 sec
2025-04-24 11:42:12,380 - INFO - Average 'generate' time per sample: 0.0000 sec
2025-04-24 11:42:12,380 - INFO - Total effective tokens generated: 0
2025-04-24 11:42:12,380 - INFO - Overall effective tokens per second: 0.00
2025-04-24 11:42:12,380 - INFO - RAM Delta during inference: 38.25 MB
2025-04-24 11:42:12,380 - INFO - PyTorch VRAM Peak Delta during inference: 0.02 MB
2025-04-24 11:42:12,388 - INFO - Saved outputs (81 items) to: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-114144_outputs.json
2025-04-24 11:42:12,392 - INFO - Saved metrics to: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-114144_metrics.json
2025-04-24 11:42:12,392 - INFO - Cleaning up resources...
2025-04-24 11:42:12,392 - INFO - CUDA cache cleared.
2025-04-24 11:42:12,392 - INFO - --- Evaluation Complete ---

2025-04-24 11:42:13,270 - INFO - SUCCESS: Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 11:42:13,271 - INFO - 
Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 11:42:13,272 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:42:44,742 - WARNING - Stderr:
2025-04-24 11:42:29,053 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:42:29,210 - INFO - --- Starting Evaluation ---
2025-04-24 11:42:29,210 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:42:29,210 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:42:29,210 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-114229_metrics.json
2025-04-24 11:42:29,210 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-114229_outputs.json
2025-04-24 11:42:29,210 - INFO - NVML Reporting Active: False
2025-04-24 11:42:29,210 - INFO - Initial RAM usage: 520.50 MB
2025-04-24 11:42:29,210 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:42:29,232 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:42:29,363 - INFO - Tokenizer loaded.
2025-04-24 11:42:38,802 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:42:38,803 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 11:42:40,042 - WARNING - CUDA extension not installed.
2025-04-24 11:42:40,048 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 11:42:43,023 - INFO - CausalLM Model loaded.
2025-04-24 11:42:43,024 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 11:42:43,025 - INFO - Model loaded in 13.81 seconds.
2025-04-24 11:42:43,025 - INFO - Loading prompts from SEvenLLM saved dataset directory: /workspace/calibration_data/sevenllm_instruct_subset_manual/
2025-04-24 11:42:43,107 - INFO - Loaded dataset: Dataset({
    features: ['output', 'category', 'thought', 'input', 'id', 'instruction'],
    num_rows: 200
})
2025-04-24 11:42:43,109 - INFO - Selected first 100 samples for evaluation.

Parsing SEvenLLM prompts:   0%|          | 0/100 [00:00<?, ?it/s]
Parsing SEvenLLM prompts: 100%|██████████| 100/100 [00:00<00:00, 8630.43it/s]
2025-04-24 11:42:43,122 - INFO - Successfully loaded 100 prompts from sevenllm_bench.
2025-04-24 11:42:43,122 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 11:42:43,123 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
2025-04-24 11:42:43,275 - ERROR - Generation failed for 1: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:   1%|          | 1/100 [00:00<00:15,  6.20it/s]2025-04-24 11:42:43,291 - ERROR - Generation failed for 2: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,298 - ERROR - Generation failed for 3: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,316 - ERROR - Generation failed for 4: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,324 - ERROR - Generation failed for 5: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,332 - ERROR - Generation failed for 6: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,340 - ERROR - Generation failed for 7: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,347 - ERROR - Generation failed for 8: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,354 - ERROR - Generation failed for 9: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,360 - ERROR - Generation failed for 10: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,368 - ERROR - Generation failed for 11: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,374 - ERROR - Generation failed for 12: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,382 - ERROR - Generation failed for 13: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  13%|█▎        | 13/100 [00:00<00:01, 59.49it/s]2025-04-24 11:42:43,389 - ERROR - Generation failed for 14: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,395 - ERROR - Generation failed for 15: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,400 - ERROR - Generation failed for 16: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,414 - ERROR - Generation failed for 17: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,420 - ERROR - Generation failed for 18: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,428 - ERROR - Generation failed for 19: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,434 - ERROR - Generation failed for 20: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,441 - ERROR - Generation failed for 21: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,448 - ERROR - Generation failed for 22: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,455 - ERROR - Generation failed for 23: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,463 - ERROR - Generation failed for 24: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,471 - ERROR - Generation failed for 25: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,476 - ERROR - Generation failed for 26: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,482 - ERROR - Generation failed for 27: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,487 - ERROR - Generation failed for 28: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  28%|██▊       | 28/100 [00:00<00:00, 94.22it/s]2025-04-24 11:42:43,493 - ERROR - Generation failed for 29: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,498 - ERROR - Generation failed for 30: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,515 - ERROR - Generation failed for 31: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,521 - ERROR - Generation failed for 32: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,526 - ERROR - Generation failed for 33: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,533 - ERROR - Generation failed for 34: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,539 - ERROR - Generation failed for 35: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,545 - ERROR - Generation failed for 36: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,550 - ERROR - Generation failed for 37: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,555 - ERROR - Generation failed for 38: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,560 - ERROR - Generation failed for 39: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,566 - ERROR - Generation failed for 40: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,571 - ERROR - Generation failed for 41: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,576 - ERROR - Generation failed for 42: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,581 - ERROR - Generation failed for 43: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,586 - ERROR - Generation failed for 44: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,592 - ERROR - Generation failed for 45: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  45%|████▌     | 45/100 [00:00<00:00, 119.37it/s]2025-04-24 11:42:43,598 - ERROR - Generation failed for 46: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,615 - ERROR - Generation failed for 47: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,621 - ERROR - Generation failed for 48: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,628 - ERROR - Generation failed for 49: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,634 - ERROR - Generation failed for 50: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,639 - ERROR - Generation failed for 51: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,644 - ERROR - Generation failed for 52: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,650 - ERROR - Generation failed for 53: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,655 - ERROR - Generation failed for 54: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,662 - ERROR - Generation failed for 55: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,669 - ERROR - Generation failed for 56: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,674 - ERROR - Generation failed for 57: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,678 - ERROR - Generation failed for 58: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,683 - ERROR - Generation failed for 59: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,688 - ERROR - Generation failed for 60: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,694 - ERROR - Generation failed for 61: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  61%|██████    | 61/100 [00:00<00:00, 132.53it/s]2025-04-24 11:42:43,700 - ERROR - Generation failed for 62: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,715 - ERROR - Generation failed for 63: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,722 - ERROR - Generation failed for 64: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,729 - ERROR - Generation failed for 65: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,734 - ERROR - Generation failed for 66: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,739 - ERROR - Generation failed for 67: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,744 - ERROR - Generation failed for 68: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,750 - ERROR - Generation failed for 69: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,756 - ERROR - Generation failed for 70: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,761 - ERROR - Generation failed for 71: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,767 - ERROR - Generation failed for 72: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,772 - ERROR - Generation failed for 73: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,776 - ERROR - Generation failed for 74: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,781 - ERROR - Generation failed for 75: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,788 - ERROR - Generation failed for 76: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,795 - ERROR - Generation failed for 77: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  77%|███████▋  | 77/100 [00:00<00:00, 140.90it/s]2025-04-24 11:42:43,799 - ERROR - Generation failed for 78: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,813 - ERROR - Generation failed for 79: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,818 - ERROR - Generation failed for 80: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,825 - ERROR - Generation failed for 81: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,833 - ERROR - Generation failed for 82: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,837 - ERROR - Generation failed for 83: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,845 - ERROR - Generation failed for 84: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,850 - ERROR - Generation failed for 85: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,856 - ERROR - Generation failed for 86: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,861 - ERROR - Generation failed for 87: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,865 - ERROR - Generation failed for 88: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,869 - ERROR - Generation failed for 89: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,874 - ERROR - Generation failed for 90: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,878 - ERROR - Generation failed for 91: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,882 - ERROR - Generation failed for 92: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,886 - ERROR - Generation failed for 93: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,890 - ERROR - Generation failed for 94: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,894 - ERROR - Generation failed for 95: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,898 - ERROR - Generation failed for 96: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  96%|█████████▌| 96/100 [00:00<00:00, 149.50it/s]2025-04-24 11:42:43,917 - ERROR - Generation failed for 97: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,923 - ERROR - Generation failed for 98: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,931 - ERROR - Generation failed for 99: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:42:43,938 - ERROR - Generation failed for 100: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress: 100%|██████████| 100/100 [00:00<00:00, 122.43it/s]
2025-04-24 11:42:43,941 - INFO - 
--- Inference Summary ---
2025-04-24 11:42:43,941 - INFO - Processed 100 samples.
2025-04-24 11:42:43,941 - INFO - Total 'generate' time (sum): 0.00 sec
2025-04-24 11:42:43,941 - INFO - Overall inference loop duration: 0.82 sec
2025-04-24 11:42:43,941 - INFO - Average 'generate' time per sample: 0.0000 sec
2025-04-24 11:42:43,941 - INFO - Total effective tokens generated: 0
2025-04-24 11:42:43,941 - INFO - Overall effective tokens per second: 0.00
2025-04-24 11:42:43,941 - INFO - RAM Delta during inference: 47.25 MB
2025-04-24 11:42:43,941 - INFO - PyTorch VRAM Peak Delta during inference: 0.03 MB
2025-04-24 11:42:43,950 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-114229_outputs.json
2025-04-24 11:42:43,954 - INFO - Saved metrics to: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-114229_metrics.json
2025-04-24 11:42:43,954 - INFO - Cleaning up resources...
2025-04-24 11:42:43,954 - INFO - CUDA cache cleared.
2025-04-24 11:42:43,954 - INFO - --- Evaluation Complete ---

2025-04-24 11:42:44,751 - INFO - SUCCESS: Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 11:42:44,753 - INFO - 
Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 11:42:44,755 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 11:43:18,268 - WARNING - Stderr:
2025-04-24 11:43:01,062 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:43:01,214 - INFO - --- Starting Evaluation ---
2025-04-24 11:43:01,215 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq'}
2025-04-24 11:43:01,215 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:43:01,215 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-114301_metrics.json
2025-04-24 11:43:01,215 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-114301_outputs.json
2025-04-24 11:43:01,215 - INFO - NVML Reporting Active: False
2025-04-24 11:43:01,215 - INFO - Initial RAM usage: 520.61 MB
2025-04-24 11:43:01,215 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:43:01,236 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:43:01,350 - INFO - Tokenizer loaded.
2025-04-24 11:43:11,165 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:43:11,165 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 11:43:12,044 - WARNING - CUDA extension not installed.
2025-04-24 11:43:12,050 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 11:43:15,885 - INFO - CausalLM Model loaded.
2025-04-24 11:43:15,886 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 11:43:15,886 - INFO - Model loaded in 14.67 seconds.
2025-04-24 11:43:15,886 - INFO - Loading prompts from CTIBench subset: cti-mcq
2025-04-24 11:43:16,556 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 11:43:16,563 - INFO - Selected first 100 samples.
2025-04-24 11:43:16,563 - INFO - Using columns for cti-mcq: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-mcq):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-mcq): 100%|██████████| 100/100 [00:00<00:00, 3880.49it/s]
2025-04-24 11:43:16,590 - INFO - Successfully loaded 100 prompts from ctibench subset cti-mcq.
2025-04-24 11:43:16,591 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 11:43:16,592 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
2025-04-24 11:43:16,762 - ERROR - Generation failed for index_0: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:   1%|          | 1/100 [00:00<00:18,  5.46it/s]2025-04-24 11:43:16,781 - ERROR - Generation failed for index_1: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,790 - ERROR - Generation failed for index_2: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,797 - ERROR - Generation failed for index_3: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,816 - ERROR - Generation failed for index_4: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,825 - ERROR - Generation failed for index_5: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,833 - ERROR - Generation failed for index_6: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,840 - ERROR - Generation failed for index_7: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,848 - ERROR - Generation failed for index_8: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,855 - ERROR - Generation failed for index_9: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,862 - ERROR - Generation failed for index_10: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,869 - ERROR - Generation failed for index_11: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,876 - ERROR - Generation failed for index_12: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  13%|█▎        | 13/100 [00:00<00:01, 55.11it/s]2025-04-24 11:43:16,882 - ERROR - Generation failed for index_13: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,891 - ERROR - Generation failed for index_14: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,897 - ERROR - Generation failed for index_15: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,917 - ERROR - Generation failed for index_16: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,925 - ERROR - Generation failed for index_17: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,932 - ERROR - Generation failed for index_18: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,939 - ERROR - Generation failed for index_19: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,946 - ERROR - Generation failed for index_20: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,953 - ERROR - Generation failed for index_21: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,960 - ERROR - Generation failed for index_22: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,966 - ERROR - Generation failed for index_23: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,973 - ERROR - Generation failed for index_24: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  25%|██▌       | 25/100 [00:00<00:00, 79.91it/s]2025-04-24 11:43:16,982 - ERROR - Generation failed for index_25: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,989 - ERROR - Generation failed for index_26: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:16,995 - ERROR - Generation failed for index_27: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,002 - ERROR - Generation failed for index_28: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,015 - ERROR - Generation failed for index_29: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,021 - ERROR - Generation failed for index_30: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,027 - ERROR - Generation failed for index_31: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,034 - ERROR - Generation failed for index_32: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,040 - ERROR - Generation failed for index_33: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,046 - ERROR - Generation failed for index_34: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,052 - ERROR - Generation failed for index_35: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,058 - ERROR - Generation failed for index_36: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,064 - ERROR - Generation failed for index_37: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,070 - ERROR - Generation failed for index_38: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,076 - ERROR - Generation failed for index_39: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,088 - ERROR - Generation failed for index_40: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  41%|████      | 41/100 [00:00<00:00, 103.65it/s]2025-04-24 11:43:17,094 - ERROR - Generation failed for index_41: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,099 - ERROR - Generation failed for index_42: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,113 - ERROR - Generation failed for index_43: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,120 - ERROR - Generation failed for index_44: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,126 - ERROR - Generation failed for index_45: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,131 - ERROR - Generation failed for index_46: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,137 - ERROR - Generation failed for index_47: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,143 - ERROR - Generation failed for index_48: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,148 - ERROR - Generation failed for index_49: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,154 - ERROR - Generation failed for index_50: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,159 - ERROR - Generation failed for index_51: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,164 - ERROR - Generation failed for index_52: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,169 - ERROR - Generation failed for index_53: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,174 - ERROR - Generation failed for index_54: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,180 - ERROR - Generation failed for index_55: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,185 - ERROR - Generation failed for index_56: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,190 - ERROR - Generation failed for index_57: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  58%|█████▊    | 58/100 [00:00<00:00, 124.90it/s]2025-04-24 11:43:17,195 - ERROR - Generation failed for index_58: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,200 - ERROR - Generation failed for index_59: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,216 - ERROR - Generation failed for index_60: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,222 - ERROR - Generation failed for index_61: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,228 - ERROR - Generation failed for index_62: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,233 - ERROR - Generation failed for index_63: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,239 - ERROR - Generation failed for index_64: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,244 - ERROR - Generation failed for index_65: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,250 - ERROR - Generation failed for index_66: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,255 - ERROR - Generation failed for index_67: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,260 - ERROR - Generation failed for index_68: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,265 - ERROR - Generation failed for index_69: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,270 - ERROR - Generation failed for index_70: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,275 - ERROR - Generation failed for index_71: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,280 - ERROR - Generation failed for index_72: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,285 - ERROR - Generation failed for index_73: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,290 - ERROR - Generation failed for index_74: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  75%|███████▌  | 75/100 [00:00<00:00, 139.14it/s]2025-04-24 11:43:17,295 - ERROR - Generation failed for index_75: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,299 - ERROR - Generation failed for index_76: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,314 - ERROR - Generation failed for index_77: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,320 - ERROR - Generation failed for index_78: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,325 - ERROR - Generation failed for index_79: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,330 - ERROR - Generation failed for index_80: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,335 - ERROR - Generation failed for index_81: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,341 - ERROR - Generation failed for index_82: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,346 - ERROR - Generation failed for index_83: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,351 - ERROR - Generation failed for index_84: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,356 - ERROR - Generation failed for index_85: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,361 - ERROR - Generation failed for index_86: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,366 - ERROR - Generation failed for index_87: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,371 - ERROR - Generation failed for index_88: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,375 - ERROR - Generation failed for index_89: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,380 - ERROR - Generation failed for index_90: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,385 - ERROR - Generation failed for index_91: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,389 - ERROR - Generation failed for index_92: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,394 - ERROR - Generation failed for index_93: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  94%|█████████▍| 94/100 [00:00<00:00, 146.23it/s]2025-04-24 11:43:17,414 - ERROR - Generation failed for index_94: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,420 - ERROR - Generation failed for index_95: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,425 - ERROR - Generation failed for index_96: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,431 - ERROR - Generation failed for index_97: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,436 - ERROR - Generation failed for index_98: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:17,441 - ERROR - Generation failed for index_99: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress: 100%|██████████| 100/100 [00:00<00:00, 117.45it/s]
2025-04-24 11:43:17,445 - INFO - 
--- Inference Summary ---
2025-04-24 11:43:17,445 - INFO - Processed 100 samples.
2025-04-24 11:43:17,445 - INFO - Total 'generate' time (sum): 0.00 sec
2025-04-24 11:43:17,445 - INFO - Overall inference loop duration: 0.85 sec
2025-04-24 11:43:17,445 - INFO - Average 'generate' time per sample: 0.0000 sec
2025-04-24 11:43:17,445 - INFO - Total effective tokens generated: 0
2025-04-24 11:43:17,445 - INFO - Overall effective tokens per second: 0.00
2025-04-24 11:43:17,445 - INFO - RAM Delta during inference: 50.25 MB
2025-04-24 11:43:17,445 - INFO - PyTorch VRAM Peak Delta during inference: 0.01 MB
2025-04-24 11:43:17,456 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-114301_outputs.json
2025-04-24 11:43:17,460 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-114301_metrics.json
2025-04-24 11:43:17,460 - INFO - Cleaning up resources...
2025-04-24 11:43:17,460 - INFO - CUDA cache cleared.
2025-04-24 11:43:17,460 - INFO - --- Evaluation Complete ---

2025-04-24 11:43:18,281 - INFO - SUCCESS: Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 11:43:18,282 - INFO - 
Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 11:43:18,283 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 11:44:00,825 - WARNING - Stderr:
2025-04-24 11:43:39,569 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:43:39,724 - INFO - --- Starting Evaluation ---
2025-04-24 11:43:39,724 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-vsp'}
2025-04-24 11:43:39,724 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:43:39,724 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-vsp_Phi-3-mini-4k-instruct-GPTQ_20250424-114339_metrics.json
2025-04-24 11:43:39,724 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-vsp_Phi-3-mini-4k-instruct-GPTQ_20250424-114339_outputs.json
2025-04-24 11:43:39,724 - INFO - NVML Reporting Active: False
2025-04-24 11:43:39,724 - INFO - Initial RAM usage: 517.53 MB
2025-04-24 11:43:39,724 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:43:39,750 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:43:39,965 - INFO - Tokenizer loaded.
2025-04-24 11:43:54,003 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:43:54,003 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 11:43:55,046 - WARNING - CUDA extension not installed.
2025-04-24 11:43:55,055 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 11:43:58,553 - INFO - CausalLM Model loaded.
2025-04-24 11:43:58,555 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 11:43:58,555 - INFO - Model loaded in 18.83 seconds.
2025-04-24 11:43:58,555 - INFO - Loading prompts from CTIBench subset: cti-vsp
2025-04-24 11:43:59,233 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 11:43:59,243 - INFO - Selected first 100 samples.
2025-04-24 11:43:59,243 - INFO - Using columns for cti-vsp: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-vsp):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-vsp): 100%|██████████| 100/100 [00:00<00:00, 7101.53it/s]
2025-04-24 11:43:59,259 - INFO - Successfully loaded 100 prompts from ctibench subset cti-vsp.
2025-04-24 11:43:59,259 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 11:43:59,260 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
2025-04-24 11:43:59,357 - ERROR - Generation failed for index_0: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:   1%|          | 1/100 [00:00<00:11,  8.98it/s]2025-04-24 11:43:59,379 - ERROR - Generation failed for index_1: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,388 - ERROR - Generation failed for index_2: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,396 - ERROR - Generation failed for index_3: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,414 - ERROR - Generation failed for index_4: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,425 - ERROR - Generation failed for index_5: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,432 - ERROR - Generation failed for index_6: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,444 - ERROR - Generation failed for index_7: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,454 - ERROR - Generation failed for index_8: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,463 - ERROR - Generation failed for index_9: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,473 - ERROR - Generation failed for index_10: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  11%|█         | 11/100 [00:00<00:01, 58.64it/s]2025-04-24 11:43:59,482 - ERROR - Generation failed for index_11: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,492 - ERROR - Generation failed for index_12: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,500 - ERROR - Generation failed for index_13: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,516 - ERROR - Generation failed for index_14: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,524 - ERROR - Generation failed for index_15: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,531 - ERROR - Generation failed for index_16: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,539 - ERROR - Generation failed for index_17: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,546 - ERROR - Generation failed for index_18: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,554 - ERROR - Generation failed for index_19: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,562 - ERROR - Generation failed for index_20: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,569 - ERROR - Generation failed for index_21: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,576 - ERROR - Generation failed for index_22: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  23%|██▎       | 23/100 [00:00<00:00, 84.89it/s]2025-04-24 11:43:59,584 - ERROR - Generation failed for index_23: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,596 - ERROR - Generation failed for index_24: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,602 - ERROR - Generation failed for index_25: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,613 - ERROR - Generation failed for index_26: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,619 - ERROR - Generation failed for index_27: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,625 - ERROR - Generation failed for index_28: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,632 - ERROR - Generation failed for index_29: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,638 - ERROR - Generation failed for index_30: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,645 - ERROR - Generation failed for index_31: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,650 - ERROR - Generation failed for index_32: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,656 - ERROR - Generation failed for index_33: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,661 - ERROR - Generation failed for index_34: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,666 - ERROR - Generation failed for index_35: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,672 - ERROR - Generation failed for index_36: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,677 - ERROR - Generation failed for index_37: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  38%|███▊      | 38/100 [00:00<00:00, 109.67it/s]2025-04-24 11:43:59,683 - ERROR - Generation failed for index_38: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,688 - ERROR - Generation failed for index_39: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,693 - ERROR - Generation failed for index_40: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,698 - ERROR - Generation failed for index_41: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,713 - ERROR - Generation failed for index_42: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,719 - ERROR - Generation failed for index_43: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,724 - ERROR - Generation failed for index_44: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,729 - ERROR - Generation failed for index_45: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,735 - ERROR - Generation failed for index_46: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,741 - ERROR - Generation failed for index_47: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,745 - ERROR - Generation failed for index_48: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,751 - ERROR - Generation failed for index_49: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,756 - ERROR - Generation failed for index_50: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,761 - ERROR - Generation failed for index_51: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,766 - ERROR - Generation failed for index_52: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,771 - ERROR - Generation failed for index_53: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,776 - ERROR - Generation failed for index_54: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,780 - ERROR - Generation failed for index_55: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  56%|█████▌    | 56/100 [00:00<00:00, 133.25it/s]2025-04-24 11:43:59,785 - ERROR - Generation failed for index_56: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,790 - ERROR - Generation failed for index_57: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,794 - ERROR - Generation failed for index_58: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,799 - ERROR - Generation failed for index_59: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,811 - ERROR - Generation failed for index_60: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,815 - ERROR - Generation failed for index_61: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,820 - ERROR - Generation failed for index_62: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,826 - ERROR - Generation failed for index_63: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,831 - ERROR - Generation failed for index_64: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,835 - ERROR - Generation failed for index_65: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,840 - ERROR - Generation failed for index_66: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,844 - ERROR - Generation failed for index_67: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,849 - ERROR - Generation failed for index_68: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,853 - ERROR - Generation failed for index_69: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,857 - ERROR - Generation failed for index_70: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,863 - ERROR - Generation failed for index_71: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,867 - ERROR - Generation failed for index_72: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,871 - ERROR - Generation failed for index_73: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,876 - ERROR - Generation failed for index_74: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,880 - ERROR - Generation failed for index_75: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,884 - ERROR - Generation failed for index_76: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  77%|███████▋  | 77/100 [00:00<00:00, 156.97it/s]2025-04-24 11:43:59,889 - ERROR - Generation failed for index_77: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,893 - ERROR - Generation failed for index_78: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,897 - ERROR - Generation failed for index_79: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,911 - ERROR - Generation failed for index_80: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,920 - ERROR - Generation failed for index_81: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,925 - ERROR - Generation failed for index_82: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,929 - ERROR - Generation failed for index_83: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,933 - ERROR - Generation failed for index_84: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,937 - ERROR - Generation failed for index_85: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,942 - ERROR - Generation failed for index_86: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,946 - ERROR - Generation failed for index_87: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,951 - ERROR - Generation failed for index_88: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,955 - ERROR - Generation failed for index_89: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,959 - ERROR - Generation failed for index_90: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,963 - ERROR - Generation failed for index_91: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,967 - ERROR - Generation failed for index_92: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,971 - ERROR - Generation failed for index_93: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,975 - ERROR - Generation failed for index_94: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,979 - ERROR - Generation failed for index_95: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,983 - ERROR - Generation failed for index_96: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  97%|█████████▋| 97/100 [00:00<00:00, 169.81it/s]2025-04-24 11:43:59,990 - ERROR - Generation failed for index_97: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,994 - ERROR - Generation failed for index_98: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:43:59,999 - ERROR - Generation failed for index_99: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress: 100%|██████████| 100/100 [00:00<00:00, 132.35it/s]
2025-04-24 11:44:00,017 - INFO - 
--- Inference Summary ---
2025-04-24 11:44:00,017 - INFO - Processed 100 samples.
2025-04-24 11:44:00,017 - INFO - Total 'generate' time (sum): 0.00 sec
2025-04-24 11:44:00,018 - INFO - Overall inference loop duration: 0.76 sec
2025-04-24 11:44:00,018 - INFO - Average 'generate' time per sample: 0.0000 sec
2025-04-24 11:44:00,018 - INFO - Total effective tokens generated: 0
2025-04-24 11:44:00,018 - INFO - Overall effective tokens per second: 0.00
2025-04-24 11:44:00,018 - INFO - RAM Delta during inference: 56.25 MB
2025-04-24 11:44:00,018 - INFO - PyTorch VRAM Peak Delta during inference: 0.03 MB
2025-04-24 11:44:00,027 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-vsp_Phi-3-mini-4k-instruct-GPTQ_20250424-114339_outputs.json
2025-04-24 11:44:00,031 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-vsp_Phi-3-mini-4k-instruct-GPTQ_20250424-114339_metrics.json
2025-04-24 11:44:00,031 - INFO - Cleaning up resources...
2025-04-24 11:44:00,031 - INFO - CUDA cache cleared.
2025-04-24 11:44:00,031 - INFO - --- Evaluation Complete ---

2025-04-24 11:44:00,834 - INFO - SUCCESS: Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'. Check output files in /workspace/results/mod/48/
2025-04-24 11:44:00,835 - INFO - 
Run 5/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 11:44:00,836 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 11:45:07,242 - WARNING - Stderr:
2025-04-24 11:44:36,353 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:44:36,512 - INFO - --- Starting Evaluation ---
2025-04-24 11:44:36,512 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-rcm'}
2025-04-24 11:44:36,513 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:44:36,513 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-rcm_Phi-3-mini-4k-instruct-GPTQ_20250424-114436_metrics.json
2025-04-24 11:44:36,513 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-rcm_Phi-3-mini-4k-instruct-GPTQ_20250424-114436_outputs.json
2025-04-24 11:44:36,513 - INFO - NVML Reporting Active: False
2025-04-24 11:44:36,513 - INFO - Initial RAM usage: 511.01 MB
2025-04-24 11:44:36,513 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:44:36,535 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:44:36,786 - INFO - Tokenizer loaded.
2025-04-24 11:45:00,894 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:45:00,894 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 11:45:01,931 - WARNING - CUDA extension not installed.
2025-04-24 11:45:01,936 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 11:45:04,939 - INFO - CausalLM Model loaded.
2025-04-24 11:45:04,940 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 11:45:04,941 - INFO - Model loaded in 28.43 seconds.
2025-04-24 11:45:04,941 - INFO - Loading prompts from CTIBench subset: cti-rcm
2025-04-24 11:45:05,655 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 11:45:05,659 - INFO - Selected first 100 samples.
2025-04-24 11:45:05,659 - INFO - Using columns for cti-rcm: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-rcm):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-rcm): 100%|██████████| 100/100 [00:00<00:00, 4606.95it/s]
2025-04-24 11:45:05,682 - INFO - Successfully loaded 100 prompts from ctibench subset cti-rcm.
2025-04-24 11:45:05,683 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 11:45:05,684 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
2025-04-24 11:45:05,781 - ERROR - Generation failed for index_0: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:   1%|          | 1/100 [00:00<00:10,  9.53it/s]2025-04-24 11:45:05,794 - ERROR - Generation failed for index_1: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,800 - ERROR - Generation failed for index_2: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,814 - ERROR - Generation failed for index_3: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,822 - ERROR - Generation failed for index_4: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,829 - ERROR - Generation failed for index_5: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,834 - ERROR - Generation failed for index_6: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,842 - ERROR - Generation failed for index_7: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,849 - ERROR - Generation failed for index_8: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,856 - ERROR - Generation failed for index_9: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,861 - ERROR - Generation failed for index_10: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,867 - ERROR - Generation failed for index_11: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,872 - ERROR - Generation failed for index_12: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,878 - ERROR - Generation failed for index_13: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,884 - ERROR - Generation failed for index_14: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,889 - ERROR - Generation failed for index_15: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  16%|█▌        | 16/100 [00:00<00:00, 89.75it/s]2025-04-24 11:45:05,895 - ERROR - Generation failed for index_16: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,900 - ERROR - Generation failed for index_17: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,912 - ERROR - Generation failed for index_18: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,917 - ERROR - Generation failed for index_19: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,923 - ERROR - Generation failed for index_20: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,928 - ERROR - Generation failed for index_21: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,934 - ERROR - Generation failed for index_22: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,941 - ERROR - Generation failed for index_23: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,946 - ERROR - Generation failed for index_24: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,951 - ERROR - Generation failed for index_25: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,956 - ERROR - Generation failed for index_26: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,961 - ERROR - Generation failed for index_27: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,966 - ERROR - Generation failed for index_28: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,971 - ERROR - Generation failed for index_29: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,976 - ERROR - Generation failed for index_30: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,981 - ERROR - Generation failed for index_31: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,986 - ERROR - Generation failed for index_32: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:05,991 - ERROR - Generation failed for index_33: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  34%|███▍      | 34/100 [00:00<00:00, 129.41it/s]2025-04-24 11:45:05,996 - ERROR - Generation failed for index_34: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,000 - ERROR - Generation failed for index_35: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,013 - ERROR - Generation failed for index_36: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,018 - ERROR - Generation failed for index_37: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,023 - ERROR - Generation failed for index_38: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,028 - ERROR - Generation failed for index_39: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,032 - ERROR - Generation failed for index_40: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,037 - ERROR - Generation failed for index_41: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,044 - ERROR - Generation failed for index_42: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,049 - ERROR - Generation failed for index_43: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,053 - ERROR - Generation failed for index_44: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,058 - ERROR - Generation failed for index_45: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,063 - ERROR - Generation failed for index_46: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,068 - ERROR - Generation failed for index_47: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,072 - ERROR - Generation failed for index_48: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,078 - ERROR - Generation failed for index_49: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,083 - ERROR - Generation failed for index_50: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,087 - ERROR - Generation failed for index_51: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,092 - ERROR - Generation failed for index_52: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  53%|█████▎    | 53/100 [00:00<00:00, 151.27it/s]2025-04-24 11:45:06,098 - ERROR - Generation failed for index_53: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,111 - ERROR - Generation failed for index_54: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,116 - ERROR - Generation failed for index_55: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,121 - ERROR - Generation failed for index_56: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,125 - ERROR - Generation failed for index_57: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,130 - ERROR - Generation failed for index_58: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,135 - ERROR - Generation failed for index_59: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,139 - ERROR - Generation failed for index_60: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,143 - ERROR - Generation failed for index_61: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,148 - ERROR - Generation failed for index_62: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,154 - ERROR - Generation failed for index_63: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,159 - ERROR - Generation failed for index_64: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,163 - ERROR - Generation failed for index_65: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,167 - ERROR - Generation failed for index_66: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,172 - ERROR - Generation failed for index_67: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,177 - ERROR - Generation failed for index_68: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,182 - ERROR - Generation failed for index_69: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,186 - ERROR - Generation failed for index_70: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,191 - ERROR - Generation failed for index_71: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,195 - ERROR - Generation failed for index_72: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  73%|███████▎  | 73/100 [00:00<00:00, 167.84it/s]2025-04-24 11:45:06,199 - ERROR - Generation failed for index_73: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,211 - ERROR - Generation failed for index_74: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,216 - ERROR - Generation failed for index_75: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,220 - ERROR - Generation failed for index_76: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,226 - ERROR - Generation failed for index_77: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,230 - ERROR - Generation failed for index_78: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,234 - ERROR - Generation failed for index_79: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,239 - ERROR - Generation failed for index_80: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,244 - ERROR - Generation failed for index_81: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,248 - ERROR - Generation failed for index_82: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,253 - ERROR - Generation failed for index_83: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,257 - ERROR - Generation failed for index_84: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,262 - ERROR - Generation failed for index_85: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,266 - ERROR - Generation failed for index_86: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,270 - ERROR - Generation failed for index_87: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,275 - ERROR - Generation failed for index_88: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,279 - ERROR - Generation failed for index_89: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,283 - ERROR - Generation failed for index_90: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,288 - ERROR - Generation failed for index_91: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,292 - ERROR - Generation failed for index_92: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,296 - ERROR - Generation failed for index_93: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  94%|█████████▍| 94/100 [00:00<00:00, 181.53it/s]2025-04-24 11:45:06,300 - ERROR - Generation failed for index_94: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,311 - ERROR - Generation failed for index_95: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,316 - ERROR - Generation failed for index_96: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,320 - ERROR - Generation failed for index_97: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,325 - ERROR - Generation failed for index_98: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:45:06,329 - ERROR - Generation failed for index_99: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress: 100%|██████████| 100/100 [00:00<00:00, 154.64it/s]
2025-04-24 11:45:06,332 - INFO - 
--- Inference Summary ---
2025-04-24 11:45:06,332 - INFO - Processed 100 samples.
2025-04-24 11:45:06,332 - INFO - Total 'generate' time (sum): 0.00 sec
2025-04-24 11:45:06,332 - INFO - Overall inference loop duration: 0.65 sec
2025-04-24 11:45:06,332 - INFO - Average 'generate' time per sample: 0.0000 sec
2025-04-24 11:45:06,332 - INFO - Total effective tokens generated: 0
2025-04-24 11:45:06,332 - INFO - Overall effective tokens per second: 0.00
2025-04-24 11:45:06,332 - INFO - RAM Delta during inference: 43.50 MB
2025-04-24 11:45:06,333 - INFO - PyTorch VRAM Peak Delta during inference: 0.02 MB
2025-04-24 11:45:06,346 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-rcm_Phi-3-mini-4k-instruct-GPTQ_20250424-114436_outputs.json
2025-04-24 11:45:06,363 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-rcm_Phi-3-mini-4k-instruct-GPTQ_20250424-114436_metrics.json
2025-04-24 11:45:06,363 - INFO - Cleaning up resources...
2025-04-24 11:45:06,363 - INFO - CUDA cache cleared.
2025-04-24 11:45:06,363 - INFO - --- Evaluation Complete ---

2025-04-24 11:45:07,252 - INFO - SUCCESS: Run 5/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'. Check output files in /workspace/results/mod/48/
2025-04-24 11:45:07,253 - INFO - 
Run 6/10: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:45:07,256 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:48:18,324 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 11:48:18,326 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 11:48:18,327 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 11:48:18,328 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 11:48:18,329 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 11:48:18,330 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_114818.log
2025-04-24 11:48:18,331 - INFO - Models to run: ['Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 11:48:18,332 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 11:48:18,334 - INFO - Running 100 samples per benchmark.
2025-04-24 11:48:18,335 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 11:48:18,337 - INFO - 
Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:48:18,343 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:48:54,903 - WARNING - Stderr:
2025-04-24 11:48:37,718 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:48:37,870 - INFO - --- Starting Evaluation ---
2025-04-24 11:48:37,870 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:48:37,870 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:48:37,870 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-114837_metrics.json
2025-04-24 11:48:37,870 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-114837_outputs.json
2025-04-24 11:48:37,870 - INFO - NVML Reporting Active: False
2025-04-24 11:48:37,870 - INFO - Initial RAM usage: 515.25 MB
2025-04-24 11:48:37,870 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:48:37,892 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:48:38,054 - INFO - Tokenizer loaded.
2025-04-24 11:48:48,109 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:48:48,109 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 11:48:49,133 - WARNING - CUDA extension not installed.
2025-04-24 11:48:49,138 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 11:48:53,655 - INFO - CausalLM Model loaded.
2025-04-24 11:48:53,656 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 11:48:53,657 - INFO - Model loaded in 15.79 seconds.
2025-04-24 11:48:53,657 - INFO - Loading prompts from CyberSecEval3 MITRE file: /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json
2025-04-24 11:48:53,672 - INFO - Processing the first 100 items.

Parsing MITRE prompts:   0%|          | 0/100 [00:00<?, ?it/s]2025-04-24 11:48:53,675 - WARNING - Item 5: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,675 - WARNING - Item 8: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,676 - WARNING - Item 14: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,677 - WARNING - Item 21: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,677 - WARNING - Item 25: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,678 - WARNING - Item 27: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,678 - WARNING - Item 30: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,678 - WARNING - Item 32: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,679 - WARNING - Item 35: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,679 - WARNING - Item 36: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,680 - WARNING - Item 48: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,680 - WARNING - Item 49: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,680 - WARNING - Item 52: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,681 - WARNING - Item 58: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,681 - WARNING - Item 63: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,682 - WARNING - Item 72: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,682 - WARNING - Item 81: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,683 - WARNING - Item 86: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:48:53,683 - WARNING - Item 88: Regex failed to find/extract 'prompt' value from mutated_prompt_str.

Parsing MITRE prompts: 100%|██████████| 100/100 [00:00<00:00, 10103.84it/s]
2025-04-24 11:48:53,684 - INFO - Successfully loaded 81 prompts from cyberseceval3_mitre.
2025-04-24 11:48:53,684 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 11:48:53,685 - INFO - Starting inference on 81 samples...

Inference Progress:   0%|          | 0/81 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
2025-04-24 11:48:53,757 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,768 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,772 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,776 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,780 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,783 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:   7%|▋         | 6/81 [00:00<00:01, 59.94it/s]2025-04-24 11:48:53,787 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,792 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,796 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,800 - ERROR - Generation failed for T1024: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,815 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,820 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,824 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,828 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,832 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,836 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,840 - ERROR - Generation failed for T1090.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,843 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,848 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,852 - ERROR - Generation failed for T1090.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,856 - ERROR - Generation failed for T1090.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,860 - ERROR - Generation failed for T1483: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,863 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,867 - ERROR - Generation failed for T1090.003: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,871 - ERROR - Generation failed for T1571: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,875 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,878 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,881 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,885 - ERROR - Generation failed for T1095: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  36%|███▌      | 29/81 [00:00<00:00, 158.70it/s]2025-04-24 11:48:53,888 - ERROR - Generation failed for T1105: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,893 - ERROR - Generation failed for T1094.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,896 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,899 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,915 - ERROR - Generation failed for T1102: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,920 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,924 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,929 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,933 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,938 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,942 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,946 - ERROR - Generation failed for T1074: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,951 - ERROR - Generation failed for T1097: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,955 - ERROR - Generation failed for T1483: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,959 - ERROR - Generation failed for T1021.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,964 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,968 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,971 - ERROR - Generation failed for T1090.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,975 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,978 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,983 - ERROR - Generation failed for T1090.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,986 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  63%|██████▎   | 51/81 [00:00<00:00, 185.37it/s]2025-04-24 11:48:53,990 - ERROR - Generation failed for T1071: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,994 - ERROR - Generation failed for T1074: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:53,999 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,015 - ERROR - Generation failed for T1090.003: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,019 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,024 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,029 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,034 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,038 - ERROR - Generation failed for T1090.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,043 - ERROR - Generation failed for T1090.003: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,047 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,050 - ERROR - Generation failed for T1094: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,055 - ERROR - Generation failed for T1132.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,060 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,064 - ERROR - Generation failed for T1059: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,069 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,074 - ERROR - Generation failed for T1071: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,078 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,083 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,088 - ERROR - Generation failed for T1094: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  88%|████████▊ | 71/81 [00:00<00:00, 190.14it/s]2025-04-24 11:48:54,092 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,095 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,099 - ERROR - Generation failed for T1071: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,114 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,119 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,125 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,130 - ERROR - Generation failed for T1095: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,135 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,139 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:48:54,143 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress: 100%|██████████| 81/81 [00:00<00:00, 176.00it/s]
2025-04-24 11:48:54,146 - INFO - 
--- Inference Summary ---
2025-04-24 11:48:54,146 - INFO - Processed 81 samples.
2025-04-24 11:48:54,146 - INFO - Total 'generate' time (sum): 0.00 sec
2025-04-24 11:48:54,146 - INFO - Overall inference loop duration: 0.46 sec
2025-04-24 11:48:54,146 - INFO - Average 'generate' time per sample: 0.0000 sec
2025-04-24 11:48:54,146 - INFO - Total effective tokens generated: 0
2025-04-24 11:48:54,146 - INFO - Overall effective tokens per second: 0.00
2025-04-24 11:48:54,146 - INFO - RAM Delta during inference: 45.75 MB
2025-04-24 11:48:54,146 - INFO - PyTorch VRAM Peak Delta during inference: 0.02 MB
2025-04-24 11:48:54,155 - INFO - Saved outputs (81 items) to: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-114837_outputs.json
2025-04-24 11:48:54,160 - INFO - Saved metrics to: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-114837_metrics.json
2025-04-24 11:48:54,160 - INFO - Cleaning up resources...
2025-04-24 11:48:54,160 - INFO - CUDA cache cleared.
2025-04-24 11:48:54,160 - INFO - --- Evaluation Complete ---

2025-04-24 11:48:54,919 - INFO - SUCCESS: Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 11:48:54,920 - INFO - 
Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 11:48:54,921 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:49:27,128 - WARNING - Stderr:
2025-04-24 11:49:10,936 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:49:11,092 - INFO - --- Starting Evaluation ---
2025-04-24 11:49:11,092 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:49:11,092 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:49:11,092 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-114911_metrics.json
2025-04-24 11:49:11,092 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-114911_outputs.json
2025-04-24 11:49:11,092 - INFO - NVML Reporting Active: False
2025-04-24 11:49:11,092 - INFO - Initial RAM usage: 520.75 MB
2025-04-24 11:49:11,092 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:49:11,115 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:49:11,246 - INFO - Tokenizer loaded.
2025-04-24 11:49:21,130 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:49:21,131 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 11:49:22,190 - WARNING - CUDA extension not installed.
2025-04-24 11:49:22,196 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 11:49:25,662 - INFO - CausalLM Model loaded.
2025-04-24 11:49:25,663 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 11:49:25,664 - INFO - Model loaded in 14.57 seconds.
2025-04-24 11:49:25,664 - INFO - Loading prompts from SEvenLLM saved dataset directory: /workspace/calibration_data/sevenllm_instruct_subset_manual/
2025-04-24 11:49:25,734 - INFO - Loaded dataset: Dataset({
    features: ['output', 'category', 'thought', 'input', 'id', 'instruction'],
    num_rows: 200
})
2025-04-24 11:49:25,736 - INFO - Selected first 100 samples for evaluation.

Parsing SEvenLLM prompts:   0%|          | 0/100 [00:00<?, ?it/s]
Parsing SEvenLLM prompts: 100%|██████████| 100/100 [00:00<00:00, 11293.53it/s]
2025-04-24 11:49:25,746 - INFO - Successfully loaded 100 prompts from sevenllm_bench.
2025-04-24 11:49:25,746 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 11:49:25,747 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
2025-04-24 11:49:25,828 - ERROR - Generation failed for 1: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,840 - ERROR - Generation failed for 2: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,845 - ERROR - Generation failed for 3: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,850 - ERROR - Generation failed for 4: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:   4%|▍         | 4/100 [00:00<00:02, 38.27it/s]2025-04-24 11:49:25,854 - ERROR - Generation failed for 5: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,859 - ERROR - Generation failed for 6: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,864 - ERROR - Generation failed for 7: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,869 - ERROR - Generation failed for 8: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,874 - ERROR - Generation failed for 9: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,878 - ERROR - Generation failed for 10: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,883 - ERROR - Generation failed for 11: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,887 - ERROR - Generation failed for 12: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,892 - ERROR - Generation failed for 13: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,896 - ERROR - Generation failed for 14: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,900 - ERROR - Generation failed for 15: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,914 - ERROR - Generation failed for 16: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,920 - ERROR - Generation failed for 17: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,924 - ERROR - Generation failed for 18: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,930 - ERROR - Generation failed for 19: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,935 - ERROR - Generation failed for 20: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,941 - ERROR - Generation failed for 21: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,945 - ERROR - Generation failed for 22: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,951 - ERROR - Generation failed for 23: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  23%|██▎       | 23/100 [00:00<00:00, 125.20it/s]2025-04-24 11:49:25,957 - ERROR - Generation failed for 24: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,964 - ERROR - Generation failed for 25: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,968 - ERROR - Generation failed for 26: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,973 - ERROR - Generation failed for 27: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,978 - ERROR - Generation failed for 28: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,982 - ERROR - Generation failed for 29: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,986 - ERROR - Generation failed for 30: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,990 - ERROR - Generation failed for 31: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,994 - ERROR - Generation failed for 32: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:25,998 - ERROR - Generation failed for 33: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,002 - ERROR - Generation failed for 34: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,014 - ERROR - Generation failed for 35: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,019 - ERROR - Generation failed for 36: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,024 - ERROR - Generation failed for 37: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,028 - ERROR - Generation failed for 38: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,033 - ERROR - Generation failed for 39: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,037 - ERROR - Generation failed for 40: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,041 - ERROR - Generation failed for 41: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,045 - ERROR - Generation failed for 42: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,049 - ERROR - Generation failed for 43: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,054 - ERROR - Generation failed for 44: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  44%|████▍     | 44/100 [00:00<00:00, 161.20it/s]2025-04-24 11:49:26,059 - ERROR - Generation failed for 45: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,064 - ERROR - Generation failed for 46: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,070 - ERROR - Generation failed for 47: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,075 - ERROR - Generation failed for 48: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,081 - ERROR - Generation failed for 49: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,085 - ERROR - Generation failed for 50: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,089 - ERROR - Generation failed for 51: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,093 - ERROR - Generation failed for 52: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,098 - ERROR - Generation failed for 53: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,102 - ERROR - Generation failed for 54: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,116 - ERROR - Generation failed for 55: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,122 - ERROR - Generation failed for 56: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,126 - ERROR - Generation failed for 57: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,131 - ERROR - Generation failed for 58: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,135 - ERROR - Generation failed for 59: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,140 - ERROR - Generation failed for 60: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,145 - ERROR - Generation failed for 61: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,150 - ERROR - Generation failed for 62: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,155 - ERROR - Generation failed for 63: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  63%|██████▎   | 63/100 [00:00<00:00, 171.63it/s]2025-04-24 11:49:26,160 - ERROR - Generation failed for 64: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,166 - ERROR - Generation failed for 65: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,170 - ERROR - Generation failed for 66: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,174 - ERROR - Generation failed for 67: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,178 - ERROR - Generation failed for 68: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,183 - ERROR - Generation failed for 69: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,188 - ERROR - Generation failed for 70: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,192 - ERROR - Generation failed for 71: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,197 - ERROR - Generation failed for 72: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,200 - ERROR - Generation failed for 73: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,214 - ERROR - Generation failed for 74: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,219 - ERROR - Generation failed for 75: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,226 - ERROR - Generation failed for 76: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,232 - ERROR - Generation failed for 77: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,237 - ERROR - Generation failed for 78: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,240 - ERROR - Generation failed for 79: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,244 - ERROR - Generation failed for 80: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,251 - ERROR - Generation failed for 81: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,257 - ERROR - Generation failed for 82: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  82%|████████▏ | 82/100 [00:00<00:00, 177.13it/s]2025-04-24 11:49:26,261 - ERROR - Generation failed for 83: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,267 - ERROR - Generation failed for 84: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,272 - ERROR - Generation failed for 85: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,276 - ERROR - Generation failed for 86: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,281 - ERROR - Generation failed for 87: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,284 - ERROR - Generation failed for 88: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,289 - ERROR - Generation failed for 89: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,293 - ERROR - Generation failed for 90: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,297 - ERROR - Generation failed for 91: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,301 - ERROR - Generation failed for 92: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,312 - ERROR - Generation failed for 93: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,316 - ERROR - Generation failed for 94: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,320 - ERROR - Generation failed for 95: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,325 - ERROR - Generation failed for 96: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,331 - ERROR - Generation failed for 97: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,336 - ERROR - Generation failed for 98: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,343 - ERROR - Generation failed for 99: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:26,349 - ERROR - Generation failed for 100: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress: 100%|██████████| 100/100 [00:00<00:00, 165.56it/s]
2025-04-24 11:49:26,352 - INFO - 
--- Inference Summary ---
2025-04-24 11:49:26,352 - INFO - Processed 100 samples.
2025-04-24 11:49:26,352 - INFO - Total 'generate' time (sum): 0.00 sec
2025-04-24 11:49:26,352 - INFO - Overall inference loop duration: 0.60 sec
2025-04-24 11:49:26,352 - INFO - Average 'generate' time per sample: 0.0000 sec
2025-04-24 11:49:26,352 - INFO - Total effective tokens generated: 0
2025-04-24 11:49:26,352 - INFO - Overall effective tokens per second: 0.00
2025-04-24 11:49:26,352 - INFO - RAM Delta during inference: 39.00 MB
2025-04-24 11:49:26,352 - INFO - PyTorch VRAM Peak Delta during inference: 0.03 MB
2025-04-24 11:49:26,359 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-114911_outputs.json
2025-04-24 11:49:26,362 - INFO - Saved metrics to: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-114911_metrics.json
2025-04-24 11:49:26,362 - INFO - Cleaning up resources...
2025-04-24 11:49:26,363 - INFO - CUDA cache cleared.
2025-04-24 11:49:26,363 - INFO - --- Evaluation Complete ---

2025-04-24 11:49:27,139 - INFO - SUCCESS: Run 2/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 11:49:27,140 - INFO - 
Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 11:49:27,145 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 11:49:59,897 - WARNING - Stderr:
2025-04-24 11:49:43,470 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:49:43,624 - INFO - --- Starting Evaluation ---
2025-04-24 11:49:43,625 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq'}
2025-04-24 11:49:43,625 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:49:43,625 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-114943_metrics.json
2025-04-24 11:49:43,625 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-114943_outputs.json
2025-04-24 11:49:43,625 - INFO - NVML Reporting Active: False
2025-04-24 11:49:43,625 - INFO - Initial RAM usage: 517.29 MB
2025-04-24 11:49:43,625 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:49:43,648 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:49:43,773 - INFO - Tokenizer loaded.
2025-04-24 11:49:53,909 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:49:53,909 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 11:49:54,981 - WARNING - CUDA extension not installed.
2025-04-24 11:49:54,987 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 11:49:57,832 - INFO - CausalLM Model loaded.
2025-04-24 11:49:57,833 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 11:49:57,833 - INFO - Model loaded in 14.21 seconds.
2025-04-24 11:49:57,833 - INFO - Loading prompts from CTIBench subset: cti-mcq
2025-04-24 11:49:58,343 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 11:49:58,348 - INFO - Selected first 100 samples.
2025-04-24 11:49:58,348 - INFO - Using columns for cti-mcq: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-mcq):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-mcq): 100%|██████████| 100/100 [00:00<00:00, 4545.34it/s]
2025-04-24 11:49:58,371 - INFO - Successfully loaded 100 prompts from ctibench subset cti-mcq.
2025-04-24 11:49:58,372 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 11:49:58,373 - INFO - Starting inference on 100 samples...

Inference Progress:   0%|          | 0/100 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
2025-04-24 11:49:58,466 - ERROR - Generation failed for index_0: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:   1%|          | 1/100 [00:00<00:09, 10.00it/s]2025-04-24 11:49:58,478 - ERROR - Generation failed for index_1: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,483 - ERROR - Generation failed for index_2: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,488 - ERROR - Generation failed for index_3: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,494 - ERROR - Generation failed for index_4: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,500 - ERROR - Generation failed for index_5: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,515 - ERROR - Generation failed for index_6: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,521 - ERROR - Generation failed for index_7: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,527 - ERROR - Generation failed for index_8: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,533 - ERROR - Generation failed for index_9: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,538 - ERROR - Generation failed for index_10: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,544 - ERROR - Generation failed for index_11: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,550 - ERROR - Generation failed for index_12: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,555 - ERROR - Generation failed for index_13: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,560 - ERROR - Generation failed for index_14: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,565 - ERROR - Generation failed for index_15: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,570 - ERROR - Generation failed for index_16: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,575 - ERROR - Generation failed for index_17: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  18%|█▊        | 18/100 [00:00<00:00, 102.31it/s]2025-04-24 11:49:58,580 - ERROR - Generation failed for index_18: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,585 - ERROR - Generation failed for index_19: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,589 - ERROR - Generation failed for index_20: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,594 - ERROR - Generation failed for index_21: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,599 - ERROR - Generation failed for index_22: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,618 - ERROR - Generation failed for index_23: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,623 - ERROR - Generation failed for index_24: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,629 - ERROR - Generation failed for index_25: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,635 - ERROR - Generation failed for index_26: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,640 - ERROR - Generation failed for index_27: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,646 - ERROR - Generation failed for index_28: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,651 - ERROR - Generation failed for index_29: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,656 - ERROR - Generation failed for index_30: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,661 - ERROR - Generation failed for index_31: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,666 - ERROR - Generation failed for index_32: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,671 - ERROR - Generation failed for index_33: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,677 - ERROR - Generation failed for index_34: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  35%|███▌      | 35/100 [00:00<00:00, 131.95it/s]2025-04-24 11:49:58,682 - ERROR - Generation failed for index_35: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,687 - ERROR - Generation failed for index_36: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,692 - ERROR - Generation failed for index_37: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,697 - ERROR - Generation failed for index_38: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,702 - ERROR - Generation failed for index_39: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,716 - ERROR - Generation failed for index_40: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,721 - ERROR - Generation failed for index_41: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,728 - ERROR - Generation failed for index_42: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,734 - ERROR - Generation failed for index_43: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,740 - ERROR - Generation failed for index_44: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,745 - ERROR - Generation failed for index_45: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,750 - ERROR - Generation failed for index_46: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,756 - ERROR - Generation failed for index_47: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,761 - ERROR - Generation failed for index_48: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,766 - ERROR - Generation failed for index_49: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,771 - ERROR - Generation failed for index_50: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,775 - ERROR - Generation failed for index_51: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,780 - ERROR - Generation failed for index_52: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  53%|█████▎    | 53/100 [00:00<00:00, 148.55it/s]2025-04-24 11:49:58,785 - ERROR - Generation failed for index_53: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,790 - ERROR - Generation failed for index_54: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,795 - ERROR - Generation failed for index_55: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,799 - ERROR - Generation failed for index_56: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,818 - ERROR - Generation failed for index_57: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,823 - ERROR - Generation failed for index_58: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,830 - ERROR - Generation failed for index_59: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,837 - ERROR - Generation failed for index_60: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,842 - ERROR - Generation failed for index_61: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,848 - ERROR - Generation failed for index_62: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,853 - ERROR - Generation failed for index_63: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,859 - ERROR - Generation failed for index_64: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,865 - ERROR - Generation failed for index_65: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,870 - ERROR - Generation failed for index_66: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,876 - ERROR - Generation failed for index_67: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,881 - ERROR - Generation failed for index_68: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  69%|██████▉   | 69/100 [00:00<00:00, 152.41it/s]2025-04-24 11:49:58,886 - ERROR - Generation failed for index_69: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,891 - ERROR - Generation failed for index_70: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,897 - ERROR - Generation failed for index_71: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,902 - ERROR - Generation failed for index_72: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,916 - ERROR - Generation failed for index_73: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,922 - ERROR - Generation failed for index_74: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,928 - ERROR - Generation failed for index_75: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,934 - ERROR - Generation failed for index_76: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,940 - ERROR - Generation failed for index_77: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,945 - ERROR - Generation failed for index_78: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,951 - ERROR - Generation failed for index_79: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,956 - ERROR - Generation failed for index_80: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,961 - ERROR - Generation failed for index_81: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,966 - ERROR - Generation failed for index_82: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,972 - ERROR - Generation failed for index_83: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,978 - ERROR - Generation failed for index_84: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,983 - ERROR - Generation failed for index_85: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  86%|████████▌ | 86/100 [00:00<00:00, 157.25it/s]2025-04-24 11:49:58,988 - ERROR - Generation failed for index_86: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,993 - ERROR - Generation failed for index_87: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:58,998 - ERROR - Generation failed for index_88: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,003 - ERROR - Generation failed for index_89: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,018 - ERROR - Generation failed for index_90: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,024 - ERROR - Generation failed for index_91: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,030 - ERROR - Generation failed for index_92: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,036 - ERROR - Generation failed for index_93: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,042 - ERROR - Generation failed for index_94: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,048 - ERROR - Generation failed for index_95: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,053 - ERROR - Generation failed for index_96: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,058 - ERROR - Generation failed for index_97: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,064 - ERROR - Generation failed for index_98: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:49:59,069 - ERROR - Generation failed for index_99: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress: 100%|██████████| 100/100 [00:00<00:00, 143.28it/s]
2025-04-24 11:49:59,073 - INFO - 
--- Inference Summary ---
2025-04-24 11:49:59,073 - INFO - Processed 100 samples.
2025-04-24 11:49:59,073 - INFO - Total 'generate' time (sum): 0.00 sec
2025-04-24 11:49:59,073 - INFO - Overall inference loop duration: 0.70 sec
2025-04-24 11:49:59,073 - INFO - Average 'generate' time per sample: 0.0000 sec
2025-04-24 11:49:59,073 - INFO - Total effective tokens generated: 0
2025-04-24 11:49:59,073 - INFO - Overall effective tokens per second: 0.00
2025-04-24 11:49:59,073 - INFO - RAM Delta during inference: 68.25 MB
2025-04-24 11:49:59,073 - INFO - PyTorch VRAM Peak Delta during inference: 0.01 MB
2025-04-24 11:49:59,080 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-114943_outputs.json
2025-04-24 11:49:59,089 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-114943_metrics.json
2025-04-24 11:49:59,089 - INFO - Cleaning up resources...
2025-04-24 11:49:59,090 - INFO - CUDA cache cleared.
2025-04-24 11:49:59,090 - INFO - --- Evaluation Complete ---

2025-04-24 11:49:59,918 - INFO - SUCCESS: Run 3/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 11:49:59,919 - INFO - 
Run 4/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 11:49:59,920 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 11:51:18,473 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 11:51:18,473 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 11:51:18,474 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 11:51:18,475 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 11:51:18,476 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 11:51:18,476 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_115118.log
2025-04-24 11:51:18,477 - INFO - Models to run: ['Phi-3-mini-4k-instruct-GPTQ']
2025-04-24 11:51:18,477 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 11:51:18,478 - INFO - Running 100 samples per benchmark.
2025-04-24 11:51:18,478 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 11:51:18,479 - INFO - 
Run 1/5: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 11:51:18,484 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 11:51:52,052 - WARNING - Stderr:
2025-04-24 11:51:35,187 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 11:51:35,338 - INFO - --- Starting Evaluation ---
2025-04-24 11:51:35,338 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 11:51:35,338 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 11:51:35,338 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-115135_metrics.json
2025-04-24 11:51:35,338 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-115135_outputs.json
2025-04-24 11:51:35,338 - INFO - NVML Reporting Active: False
2025-04-24 11:51:35,338 - INFO - Initial RAM usage: 520.80 MB
2025-04-24 11:51:35,338 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 11:51:35,361 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 11:51:35,503 - INFO - Tokenizer loaded.
2025-04-24 11:51:45,563 - WARNING - `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24 11:51:45,564 - WARNING - Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 11:51:46,560 - WARNING - CUDA extension not installed.
2025-04-24 11:51:46,565 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 11:51:50,715 - INFO - CausalLM Model loaded.
2025-04-24 11:51:50,716 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 11:51:50,717 - INFO - Model loaded in 15.38 seconds.
2025-04-24 11:51:50,717 - INFO - Loading prompts from CyberSecEval3 MITRE file: /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json
2025-04-24 11:51:50,731 - INFO - Processing the first 100 items.

Parsing MITRE prompts:   0%|          | 0/100 [00:00<?, ?it/s]2025-04-24 11:51:50,734 - WARNING - Item 5: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:51:50,734 - WARNING - Item 8: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:51:50,735 - WARNING - Item 14: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:51:50,736 - WARNING - Item 21: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:51:50,737 - WARNING - Item 25: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:51:50,737 - WARNING - Item 27: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:51:50,738 - WARNING - Item 30: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:51:50,738 - WARNING - Item 32: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:51:50,739 - WARNING - Item 35: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:51:50,739 - WARNING - Item 36: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:51:50,740 - WARNING - Item 48: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:51:50,740 - WARNING - Item 49: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:51:50,740 - WARNING - Item 52: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:51:50,741 - WARNING - Item 58: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:51:50,741 - WARNING - Item 63: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:51:50,742 - WARNING - Item 72: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:51:50,742 - WARNING - Item 81: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:51:50,743 - WARNING - Item 86: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 11:51:50,743 - WARNING - Item 88: Regex failed to find/extract 'prompt' value from mutated_prompt_str.

Parsing MITRE prompts: 100%|██████████| 100/100 [00:00<00:00, 9547.70it/s]
2025-04-24 11:51:50,744 - INFO - Successfully loaded 81 prompts from cyberseceval3_mitre.
2025-04-24 11:51:50,744 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 11:51:50,744 - INFO - Starting inference on 81 samples...

Inference Progress:   0%|          | 0/81 [00:00<?, ?it/s]The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
2025-04-24 11:51:50,833 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,846 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:   2%|▏         | 2/81 [00:00<00:04, 19.53it/s]2025-04-24 11:51:50,851 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,855 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,860 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,864 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,868 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,873 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,877 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,882 - ERROR - Generation failed for T1024: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,886 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,890 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,894 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,898 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,901 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,914 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,919 - ERROR - Generation failed for T1090.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,923 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,928 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,933 - ERROR - Generation failed for T1090.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,937 - ERROR - Generation failed for T1090.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,940 - ERROR - Generation failed for T1483: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,944 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,949 - ERROR - Generation failed for T1090.003: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  30%|██▉       | 24/81 [00:00<00:00, 133.49it/s]2025-04-24 11:51:50,954 - ERROR - Generation failed for T1571: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,959 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,962 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,965 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,969 - ERROR - Generation failed for T1095: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,973 - ERROR - Generation failed for T1105: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,977 - ERROR - Generation failed for T1094.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,981 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,985 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,989 - ERROR - Generation failed for T1102: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,994 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:50,999 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,015 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,020 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,025 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,030 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,034 - ERROR - Generation failed for T1074: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,040 - ERROR - Generation failed for T1097: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,044 - ERROR - Generation failed for T1483: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,050 - ERROR - Generation failed for T1021.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  54%|█████▍    | 44/81 [00:00<00:00, 162.86it/s]2025-04-24 11:51:51,055 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,059 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,064 - ERROR - Generation failed for T1090.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,069 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,073 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,078 - ERROR - Generation failed for T1090.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,082 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,086 - ERROR - Generation failed for T1071: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,091 - ERROR - Generation failed for T1074: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,094 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,099 - ERROR - Generation failed for T1090.003: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,115 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,121 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,126 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,131 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,136 - ERROR - Generation failed for T1090.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,142 - ERROR - Generation failed for T1090.003: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,146 - ERROR - Generation failed for T1043: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,150 - ERROR - Generation failed for T1094: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress:  78%|███████▊  | 63/81 [00:00<00:00, 173.07it/s]2025-04-24 11:51:51,156 - ERROR - Generation failed for T1132.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,161 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,166 - ERROR - Generation failed for T1059: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,171 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,176 - ERROR - Generation failed for T1071: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,180 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,185 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,191 - ERROR - Generation failed for T1094: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,195 - ERROR - Generation failed for T1568.002: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,199 - ERROR - Generation failed for T1071.004: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,214 - ERROR - Generation failed for T1071: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,220 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,225 - ERROR - Generation failed for T1027: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,231 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,236 - ERROR - Generation failed for T1095: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,242 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,246 - ERROR - Generation failed for T1071.001: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'
2025-04-24 11:51:51,250 - ERROR - Generation failed for T1090: 'DynamicCache' object has no attribute 'get_max_length'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/inference_runner.py", line 63, in run_inference
    outputs = model.generate(
              ^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 2465, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/generation/utils.py", line 3424, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-4k-instruct/0a67737cc96d2554230f90338b163bc6380a2a85/modeling_phi3.py", line 1292, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'

Inference Progress: 100%|██████████| 81/81 [00:00<00:00, 159.81it/s]
2025-04-24 11:51:51,252 - INFO - 
--- Inference Summary ---
2025-04-24 11:51:51,252 - INFO - Processed 81 samples.
2025-04-24 11:51:51,252 - INFO - Total 'generate' time (sum): 0.00 sec
2025-04-24 11:51:51,253 - INFO - Overall inference loop duration: 0.51 sec
2025-04-24 11:51:51,253 - INFO - Average 'generate' time per sample: 0.0000 sec
2025-04-24 11:51:51,253 - INFO - Total effective tokens generated: 0
2025-04-24 11:51:51,253 - INFO - Overall effective tokens per second: 0.00
2025-04-24 11:51:51,253 - INFO - RAM Delta during inference: 57.00 MB
2025-04-24 11:51:51,253 - INFO - PyTorch VRAM Peak Delta during inference: 0.02 MB
2025-04-24 11:51:51,262 - INFO - Saved outputs (81 items) to: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-115135_outputs.json
2025-04-24 11:51:51,266 - INFO - Saved metrics to: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-115135_metrics.json
2025-04-24 11:51:51,266 - INFO - Cleaning up resources...
2025-04-24 11:51:51,266 - INFO - CUDA cache cleared.
2025-04-24 11:51:51,266 - INFO - --- Evaluation Complete ---

2025-04-24 11:51:52,062 - INFO - SUCCESS: Run 1/5: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 11:51:52,063 - INFO - 
Run 2/5: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 11:51:52,064 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 12:01:57,214 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 12:01:57,215 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 12:01:57,215 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 12:01:57,216 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 12:01:57,217 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 12:01:57,217 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_120157.log
2025-04-24 12:01:57,218 - INFO - Models to run: ['TinyLlama-1.1B-Chat-v1.0-GPTQ', 'Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 12:01:57,218 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 12:01:57,219 - INFO - Running 100 samples per benchmark.
2025-04-24 12:01:57,219 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 12:01:57,220 - INFO - 
Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 12:01:57,222 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
2025-04-24 12:02:15,323 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

2025-04-24 12:02:15,324 - ERROR - ERROR during: Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------

[2025-04-24 12:02:15.327720] ERROR during: Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------
2025-04-24 12:02:15,328 - INFO - 
Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 12:02:15,329 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
2025-04-24 12:02:32,607 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

2025-04-24 12:02:32,615 - ERROR - ERROR during: Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------

[2025-04-24 12:02:32.617973] ERROR during: Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------
2025-04-24 12:02:32,618 - INFO - 
Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 12:02:32,619 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 12:02:50,214 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

2025-04-24 12:02:50,215 - ERROR - ERROR during: Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------

[2025-04-24 12:02:50.218445] ERROR during: Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------
2025-04-24 12:02:50,219 - INFO - 
Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 12:02:50,220 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 12:03:07,273 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

2025-04-24 12:03:07,274 - ERROR - ERROR during: Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-vsp --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------

[2025-04-24 12:03:07.277655] ERROR during: Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-vsp --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------
2025-04-24 12:03:07,279 - INFO - 
Run 5/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 12:03:07,280 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 12:03:24,343 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

2025-04-24 12:03:24,344 - ERROR - ERROR during: Run 5/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-rcm --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------

[2025-04-24 12:03:24.346450] ERROR during: Run 5/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-rcm --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------
2025-04-24 12:03:24,347 - INFO - 
Run 6/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 12:03:24,349 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
2025-04-24 12:03:40,879 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

2025-04-24 12:03:40,880 - ERROR - ERROR during: Run 6/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------

[2025-04-24 12:03:40.883308] ERROR during: Run 6/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------
2025-04-24 12:03:40,884 - INFO - 
Run 7/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 12:03:40,885 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
2025-04-24 12:03:57,904 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

2025-04-24 12:03:57,911 - ERROR - ERROR during: Run 7/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------

[2025-04-24 12:03:57.914208] ERROR during: Run 7/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------
2025-04-24 12:03:57,915 - INFO - 
Run 8/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 12:03:57,916 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 12:04:14,249 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

2025-04-24 12:04:14,251 - ERROR - ERROR during: Run 8/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------

[2025-04-24 12:04:14.253791] ERROR during: Run 8/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------
2025-04-24 12:04:14,254 - INFO - 
Run 9/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 12:04:14,255 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 12:04:30,641 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

2025-04-24 12:04:30,643 - ERROR - ERROR during: Run 9/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-vsp --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------

[2025-04-24 12:04:30.646991] ERROR during: Run 9/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-vsp --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------
2025-04-24 12:04:30,648 - INFO - 
Run 10/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 12:04:30,649 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 12:04:47,567 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

2025-04-24 12:04:47,569 - ERROR - ERROR during: Run 10/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-rcm --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------

[2025-04-24 12:04:47.572390] ERROR during: Run 10/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-rcm --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------
2025-04-24 12:04:47,573 - INFO - 
Run 11/15: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 12:04:47,574 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512 --trust-remote-code
2025-04-24 12:05:04,439 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8 --trust-remote-code

2025-04-24 12:05:04,440 - ERROR - ERROR during: Run 11/15: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512 --trust-remote-code
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8 --trust-remote-code

----------------------------------------

[2025-04-24 12:05:04.444155] ERROR during: Run 11/15: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512 --trust-remote-code
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8 --trust-remote-code

----------------------------------------
2025-04-24 12:05:04,445 - INFO - 
Run 12/15: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 12:05:04,447 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512 --trust-remote-code
2025-04-24 12:05:20,843 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8 --trust-remote-code

2025-04-24 12:05:20,845 - ERROR - ERROR during: Run 12/15: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512 --trust-remote-code
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8 --trust-remote-code

----------------------------------------

[2025-04-24 12:05:20.847641] ERROR during: Run 12/15: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512 --trust-remote-code
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8 --trust-remote-code

----------------------------------------
2025-04-24 12:05:20,848 - INFO - 
Run 13/15: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 12:05:20,849 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-mcq --max-new-tokens 512 --trust-remote-code
2025-04-24 12:05:36,936 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8 --trust-remote-code

2025-04-24 12:05:36,936 - ERROR - ERROR during: Run 13/15: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-mcq --max-new-tokens 512 --trust-remote-code
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8 --trust-remote-code

----------------------------------------

[2025-04-24 12:05:36.939517] ERROR during: Run 13/15: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-mcq --max-new-tokens 512 --trust-remote-code
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8 --trust-remote-code

----------------------------------------
2025-04-24 12:05:36,940 - INFO - 
Run 14/15: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 12:05:36,941 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-vsp --max-new-tokens 512 --trust-remote-code
2025-04-24 12:05:54,173 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8 --trust-remote-code

2025-04-24 12:05:54,175 - ERROR - ERROR during: Run 14/15: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-vsp --max-new-tokens 512 --trust-remote-code
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8 --trust-remote-code

----------------------------------------

[2025-04-24 12:05:54.178251] ERROR during: Run 14/15: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-vsp --max-new-tokens 512 --trust-remote-code
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8 --trust-remote-code

----------------------------------------
2025-04-24 12:05:54,179 - INFO - 
Run 15/15: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 12:05:54,180 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-rcm --max-new-tokens 512 --trust-remote-code
2025-04-24 12:06:11,725 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8 --trust-remote-code

2025-04-24 12:06:11,727 - ERROR - ERROR during: Run 15/15: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-rcm --max-new-tokens 512 --trust-remote-code
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8 --trust-remote-code

----------------------------------------

[2025-04-24 12:06:11.728978] ERROR during: Run 15/15: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-rcm --max-new-tokens 512 --trust-remote-code
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8 --trust-remote-code

----------------------------------------
2025-04-24 12:06:11,729 - INFO - --- Security Benchmark Automation Finished (QUANTIZED MODELS) ---
2025-04-24 12:08:03,783 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 12:08:03,784 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 12:08:03,785 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 12:08:03,786 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 12:08:03,787 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 12:08:03,787 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_120803.log
2025-04-24 12:08:03,788 - INFO - Models to run: ['TinyLlama-1.1B-Chat-v1.0-GPTQ', 'Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 12:08:03,788 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 12:08:03,789 - INFO - Running 100 samples per benchmark.
2025-04-24 12:08:03,789 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 12:08:03,790 - INFO - 
Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 12:08:03,792 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
2025-04-24 12:08:20,843 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

2025-04-24 12:08:20,844 - ERROR - ERROR during: Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------

[2025-04-24 12:08:20.846537] ERROR during: Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------
2025-04-24 12:08:20,847 - INFO - 
Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 12:08:20,848 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
2025-04-24 12:08:37,391 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

2025-04-24 12:08:37,393 - ERROR - ERROR during: Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------

[2025-04-24 12:08:37.395476] ERROR during: Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------
2025-04-24 12:08:37,396 - INFO - 
Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 12:08:37,397 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 12:08:52,817 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

2025-04-24 12:08:52,818 - ERROR - ERROR during: Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------

[2025-04-24 12:08:52.821250] ERROR during: Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------
2025-04-24 12:08:52,822 - INFO - 
Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 12:08:52,823 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 12:09:08,902 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

2025-04-24 12:09:08,912 - ERROR - ERROR during: Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-vsp --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------

[2025-04-24 12:09:08.915111] ERROR during: Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-vsp --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------
2025-04-24 12:09:08,916 - INFO - 
Run 5/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 12:09:08,917 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 12:09:25,970 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

2025-04-24 12:09:25,971 - ERROR - ERROR during: Run 5/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-rcm --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------

[2025-04-24 12:09:25.974449] ERROR during: Run 5/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-rcm --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------
2025-04-24 12:09:25,975 - INFO - 
Run 6/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 12:09:25,977 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
2025-04-24 12:09:42,812 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

2025-04-24 12:09:42,813 - ERROR - ERROR during: Run 6/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------

[2025-04-24 12:09:42.816371] ERROR during: Run 6/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------
2025-04-24 12:09:42,817 - INFO - 
Run 7/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 12:09:42,818 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
2025-04-24 12:09:58,491 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

2025-04-24 12:09:58,492 - ERROR - ERROR during: Run 7/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------

[2025-04-24 12:09:58.495167] ERROR during: Run 7/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------
2025-04-24 12:09:58,496 - INFO - 
Run 8/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 12:09:58,497 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 12:10:14,432 - WARNING - Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

2025-04-24 12:10:14,433 - ERROR - ERROR during: Run 8/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------

[2025-04-24 12:10:14.435667] ERROR during: Run 8/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
  Command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-mcq --max-new-tokens 512
  Return Code: 2
  Stderr:
usage: evaluate_cli.py [-h] --model-path MODEL_PATH
                       [--model-type {causal,encoder}] --benchmark-name
                       {cyberseceval3_mitre,sevenllm_bench,ctibench}
                       --benchmark-path BENCHMARK_PATH
                       [--results-dir RESULTS_DIR] [--device DEVICE]
                       [--max-new-tokens MAX_NEW_TOKENS]
                       [--num-samples NUM_SAMPLES] [--cti-subset CTI_SUBSET]
evaluate_cli.py: error: unrecognized arguments: --batch-size 8

----------------------------------------
2025-04-24 12:10:14,436 - INFO - 
Run 9/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 12:10:14,437 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 12:12:54,162 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 12:12:54,163 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 12:12:54,163 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 12:12:54,164 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 12:12:54,165 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 12:12:54,166 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_121254.log
2025-04-24 12:12:54,166 - INFO - Models to run: ['TinyLlama-1.1B-Chat-v1.0-GPTQ', 'Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 12:12:54,167 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 12:12:54,168 - INFO - Running 100 samples per benchmark.
2025-04-24 12:12:54,168 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 12:12:54,169 - INFO - 
Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 12:12:54,171 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
2025-04-24 12:13:10,971 - WARNING - Stderr:
2025-04-24 12:13:10,367 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 12:13:10,521 - INFO - --- Starting Evaluation ---
2025-04-24 12:13:10,521 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None, 'batch_size': 8, 'trust_remote_code': False}
2025-04-24 12:13:10,521 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 12:13:10,521 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-121310_metrics.json
2025-04-24 12:13:10,521 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-121310_outputs.json
2025-04-24 12:13:10,521 - INFO - NVML Reporting Active: False
2025-04-24 12:13:10,521 - INFO - Initial RAM usage: 520.38 MB
2025-04-24 12:13:10,521 - ERROR - An error occurred during evaluation pipeline: load_model_and_tokenizer() got an unexpected keyword argument 'trust_remote_code'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    model, tokenizer, load_metrics_partial = load_model_and_tokenizer(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: load_model_and_tokenizer() got an unexpected keyword argument 'trust_remote_code'
2025-04-24 12:13:10,522 - INFO - Cleaning up resources...
2025-04-24 12:13:10,522 - INFO - CUDA cache cleared.
2025-04-24 12:13:10,522 - INFO - --- Evaluation Complete ---

2025-04-24 12:13:10,972 - INFO - SUCCESS: Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 12:13:10,973 - INFO - 
Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 12:13:10,974 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
2025-04-24 12:13:28,319 - WARNING - Stderr:
2025-04-24 12:13:27,763 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 12:13:27,915 - INFO - --- Starting Evaluation ---
2025-04-24 12:13:27,915 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None, 'batch_size': 8, 'trust_remote_code': False}
2025-04-24 12:13:27,915 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 12:13:27,915 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-121327_metrics.json
2025-04-24 12:13:27,915 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-121327_outputs.json
2025-04-24 12:13:27,915 - INFO - NVML Reporting Active: False
2025-04-24 12:13:27,915 - INFO - Initial RAM usage: 518.64 MB
2025-04-24 12:13:27,915 - ERROR - An error occurred during evaluation pipeline: load_model_and_tokenizer() got an unexpected keyword argument 'trust_remote_code'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    model, tokenizer, load_metrics_partial = load_model_and_tokenizer(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: load_model_and_tokenizer() got an unexpected keyword argument 'trust_remote_code'
2025-04-24 12:13:27,916 - INFO - Cleaning up resources...
2025-04-24 12:13:27,916 - INFO - CUDA cache cleared.
2025-04-24 12:13:27,916 - INFO - --- Evaluation Complete ---

2025-04-24 12:13:28,322 - INFO - SUCCESS: Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 12:13:28,323 - INFO - 
Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 12:13:28,324 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 12:13:45,330 - WARNING - Stderr:
2025-04-24 12:13:44,773 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 12:13:44,921 - INFO - --- Starting Evaluation ---
2025-04-24 12:13:44,921 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq', 'batch_size': 8, 'trust_remote_code': False}
2025-04-24 12:13:44,921 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 12:13:44,921 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-121344_metrics.json
2025-04-24 12:13:44,921 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-121344_outputs.json
2025-04-24 12:13:44,921 - INFO - NVML Reporting Active: False
2025-04-24 12:13:44,921 - INFO - Initial RAM usage: 522.47 MB
2025-04-24 12:13:44,921 - ERROR - An error occurred during evaluation pipeline: load_model_and_tokenizer() got an unexpected keyword argument 'trust_remote_code'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    model, tokenizer, load_metrics_partial = load_model_and_tokenizer(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: load_model_and_tokenizer() got an unexpected keyword argument 'trust_remote_code'
2025-04-24 12:13:44,922 - INFO - Cleaning up resources...
2025-04-24 12:13:44,922 - INFO - CUDA cache cleared.
2025-04-24 12:13:44,922 - INFO - --- Evaluation Complete ---

2025-04-24 12:13:45,332 - INFO - SUCCESS: Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 12:13:45,332 - INFO - 
Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 12:13:45,333 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 12:17:41,729 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 12:17:41,730 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 12:17:41,731 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 12:17:41,732 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 12:17:41,732 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 12:17:41,733 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_121741.log
2025-04-24 12:17:41,733 - INFO - Models to run: ['TinyLlama-1.1B-Chat-v1.0-GPTQ', 'Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 12:17:41,734 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 12:17:41,734 - INFO - Running 100 samples per benchmark.
2025-04-24 12:17:41,736 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 12:17:41,736 - INFO - 
Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 12:17:41,738 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
2025-04-24 12:20:06,123 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 12:20:06,123 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 12:20:06,124 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 12:20:06,125 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 12:20:06,125 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 12:20:06,126 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_122006.log
2025-04-24 12:20:06,127 - INFO - Models to run: ['Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 12:20:06,127 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 12:20:06,128 - INFO - Running 100 samples per benchmark.
2025-04-24 12:20:06,128 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 12:20:06,129 - INFO - 
Run 1/10: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 12:20:06,131 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
2025-04-24 12:23:03,853 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 12:23:03,854 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 12:23:03,856 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 12:23:03,856 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 12:23:03,857 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 12:23:03,857 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_122303.log
2025-04-24 12:23:03,858 - INFO - Models to run: ['Phi-3-mini-4k-instruct-GPTQ']
2025-04-24 12:23:03,859 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 12:23:03,859 - INFO - Running 100 samples per benchmark.
2025-04-24 12:23:03,860 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 12:23:03,861 - INFO - 
Run 1/5: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 12:23:03,862 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 32 --max-new-tokens 512
2025-04-24 12:28:36,782 - WARNING - Stderr:
2025-04-24 12:23:20,481 - INFO - Initialized NVML for device 0.
2025-04-24 12:23:20,633 - INFO - --- Starting Evaluation ---
2025-04-24 12:23:20,633 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None, 'batch_size': 32, 'trust_remote_code': False}
2025-04-24 12:23:20,633 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 12:23:20,633 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-122320_metrics.json
2025-04-24 12:23:20,633 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-122320_outputs.json
2025-04-24 12:23:20,633 - INFO - NVML Reporting Active: True
2025-04-24 12:23:20,633 - INFO - Initial RAM usage: 536.88 MB
2025-04-24 12:23:20,633 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 12:23:20,633 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 12:23:20,633 - INFO - Trust Remote Code: False
2025-04-24 12:23:20,639 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 12:23:20,772 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 12:23:31,795 - WARNING - CUDA extension not installed.
2025-04-24 12:23:31,800 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 12:23:35,720 - INFO - CausalLM Model loaded.
2025-04-24 12:23:35,721 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 12:23:35,722 - INFO - Model loaded in 15.09 seconds.
2025-04-24 12:23:35,722 - INFO - Loading prompts from CyberSecEval3 MITRE file: /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json
2025-04-24 12:23:35,736 - INFO - Processing the first 100 items.

Parsing MITRE prompts:   0%|          | 0/100 [00:00<?, ?it/s]2025-04-24 12:23:35,739 - WARNING - Item 5: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:23:35,740 - WARNING - Item 8: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:23:35,740 - WARNING - Item 14: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:23:35,741 - WARNING - Item 21: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:23:35,741 - WARNING - Item 25: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:23:35,741 - WARNING - Item 27: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:23:35,742 - WARNING - Item 30: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:23:35,742 - WARNING - Item 32: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:23:35,742 - WARNING - Item 35: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:23:35,742 - WARNING - Item 36: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:23:35,743 - WARNING - Item 48: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:23:35,743 - WARNING - Item 49: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:23:35,744 - WARNING - Item 52: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:23:35,744 - WARNING - Item 58: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:23:35,745 - WARNING - Item 63: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:23:35,745 - WARNING - Item 72: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:23:35,746 - WARNING - Item 81: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:23:35,746 - WARNING - Item 86: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:23:35,746 - WARNING - Item 88: Regex failed to find/extract 'prompt' value from mutated_prompt_str.

Parsing MITRE prompts: 100%|██████████| 100/100 [00:00<00:00, 11126.66it/s]
2025-04-24 12:23:35,747 - INFO - Successfully loaded 81 prompts from cyberseceval3_mitre.
2025-04-24 12:23:35,748 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 12:23:35,748 - INFO - Starting inference on 81 samples with batch size 32...

Inference Batches:   0%|          | 0/3 [00:00<?, ?it/s]
Inference Batches:  33%|███▎      | 1/3 [01:45<03:30, 105.21s/it]
Inference Batches:  67%|██████▋   | 2/3 [03:29<01:44, 104.65s/it]
Inference Batches: 100%|██████████| 3/3 [04:58<00:00, 97.74s/it] 
Inference Batches: 100%|██████████| 3/3 [04:58<00:00, 99.66s/it]
2025-04-24 12:28:34,742 - INFO - 
--- Inference Summary ---
2025-04-24 12:28:34,742 - INFO - Processed 81 samples.
2025-04-24 12:28:34,742 - INFO - Total 'generate' time (sum): 298.93 sec
2025-04-24 12:28:34,742 - INFO - Overall inference loop duration: 298.99 sec
2025-04-24 12:28:34,742 - INFO - Average 'generate' time per sample: 3.6905 sec
2025-04-24 12:28:34,742 - INFO - Total effective tokens generated: 41459
2025-04-24 12:28:34,742 - INFO - Overall effective tokens per second: 138.69
2025-04-24 12:28:34,742 - INFO - RAM Delta during inference: 282.00 MB
2025-04-24 12:28:34,742 - INFO - PyTorch VRAM Peak Delta during inference: 13273.81 MB
2025-04-24 12:28:34,742 - INFO - System VRAM Peak Approx. Delta during inference: -3033.38 MB
2025-04-24 12:28:35,451 - INFO - Saved outputs (81 items) to: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-122320_outputs.json
2025-04-24 12:28:35,460 - INFO - Saved metrics to: /workspace/results/mod/48/cyberseceval3_mitre_Phi-3-mini-4k-instruct-GPTQ_20250424-122320_metrics.json
2025-04-24 12:28:35,460 - INFO - Cleaning up resources...
2025-04-24 12:28:35,540 - INFO - CUDA cache cleared.
2025-04-24 12:28:35,541 - INFO - NVML shut down.
2025-04-24 12:28:35,541 - INFO - --- Evaluation Complete ---

2025-04-24 12:28:36,785 - INFO - SUCCESS: Run 1/5: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 12:28:36,786 - INFO - 
Run 2/5: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 12:28:36,787 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 32 --max-new-tokens 512
2025-04-24 12:36:31,451 - WARNING - Stderr:
2025-04-24 12:28:54,195 - INFO - Initialized NVML for device 0.
2025-04-24 12:28:54,348 - INFO - --- Starting Evaluation ---
2025-04-24 12:28:54,349 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None, 'batch_size': 32, 'trust_remote_code': False}
2025-04-24 12:28:54,349 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 12:28:54,349 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-122854_metrics.json
2025-04-24 12:28:54,349 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-122854_outputs.json
2025-04-24 12:28:54,349 - INFO - NVML Reporting Active: True
2025-04-24 12:28:54,349 - INFO - Initial RAM usage: 521.73 MB
2025-04-24 12:28:54,349 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 12:28:54,349 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 12:28:54,349 - INFO - Trust Remote Code: False
2025-04-24 12:28:54,357 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 12:28:54,505 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 12:29:06,279 - WARNING - CUDA extension not installed.
2025-04-24 12:29:06,285 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 12:29:09,748 - INFO - CausalLM Model loaded.
2025-04-24 12:29:09,749 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 12:29:09,750 - INFO - Model loaded in 15.40 seconds.
2025-04-24 12:29:09,750 - INFO - Loading prompts from SEvenLLM saved dataset directory: /workspace/calibration_data/sevenllm_instruct_subset_manual/
2025-04-24 12:29:09,843 - INFO - Loaded dataset: Dataset({
    features: ['output', 'category', 'thought', 'input', 'id', 'instruction'],
    num_rows: 200
})
2025-04-24 12:29:09,845 - INFO - Selected first 100 samples for evaluation.

Parsing SEvenLLM prompts:   0%|          | 0/100 [00:00<?, ?it/s]
Parsing SEvenLLM prompts: 100%|██████████| 100/100 [00:00<00:00, 6295.96it/s]
2025-04-24 12:29:09,862 - INFO - Successfully loaded 100 prompts from sevenllm_bench.
2025-04-24 12:29:09,863 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 12:29:09,864 - INFO - Starting inference on 100 samples with batch size 32...

Inference Batches:   0%|          | 0/4 [00:00<?, ?it/s]
Inference Batches:  25%|██▌       | 1/4 [02:04<06:12, 124.31s/it]
Inference Batches:  50%|█████     | 2/4 [03:58<03:57, 118.62s/it]
Inference Batches:  75%|███████▌  | 3/4 [06:01<02:00, 120.54s/it]
Inference Batches: 100%|██████████| 4/4 [07:20<00:00, 104.09s/it]
Inference Batches: 100%|██████████| 4/4 [07:20<00:00, 110.16s/it]
2025-04-24 12:36:30,520 - INFO - 
--- Inference Summary ---
2025-04-24 12:36:30,520 - INFO - Processed 100 samples.
2025-04-24 12:36:30,520 - INFO - Total 'generate' time (sum): 440.54 sec
2025-04-24 12:36:30,520 - INFO - Overall inference loop duration: 440.66 sec
2025-04-24 12:36:30,520 - INFO - Average 'generate' time per sample: 4.4054 sec
2025-04-24 12:36:30,520 - INFO - Total effective tokens generated: 51173
2025-04-24 12:36:30,520 - INFO - Overall effective tokens per second: 116.16
2025-04-24 12:36:30,520 - INFO - RAM Delta during inference: 463.50 MB
2025-04-24 12:36:30,520 - INFO - PyTorch VRAM Peak Delta during inference: 24122.80 MB
2025-04-24 12:36:30,520 - INFO - System VRAM Peak Approx. Delta during inference: -3033.38 MB
2025-04-24 12:36:30,532 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-122854_outputs.json
2025-04-24 12:36:30,536 - INFO - Saved metrics to: /workspace/results/mod/48/sevenllm_bench_Phi-3-mini-4k-instruct-GPTQ_20250424-122854_metrics.json
2025-04-24 12:36:30,537 - INFO - Cleaning up resources...
2025-04-24 12:36:30,650 - INFO - CUDA cache cleared.
2025-04-24 12:36:30,650 - INFO - NVML shut down.
2025-04-24 12:36:30,650 - INFO - --- Evaluation Complete ---

2025-04-24 12:36:31,454 - INFO - SUCCESS: Run 2/5: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 12:36:31,455 - INFO - 
Run 3/5: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 12:36:31,457 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 32 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 12:37:11,047 - WARNING - Stderr:
2025-04-24 12:36:49,592 - INFO - Initialized NVML for device 0.
2025-04-24 12:36:49,738 - INFO - --- Starting Evaluation ---
2025-04-24 12:36:49,738 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq', 'batch_size': 32, 'trust_remote_code': False}
2025-04-24 12:36:49,738 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 12:36:49,738 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-123649_metrics.json
2025-04-24 12:36:49,738 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-123649_outputs.json
2025-04-24 12:36:49,738 - INFO - NVML Reporting Active: True
2025-04-24 12:36:49,738 - INFO - Initial RAM usage: 525.71 MB
2025-04-24 12:36:49,738 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 12:36:49,738 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 12:36:49,738 - INFO - Trust Remote Code: False
2025-04-24 12:36:49,744 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 12:36:49,891 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 12:37:01,478 - WARNING - CUDA extension not installed.
2025-04-24 12:37:01,483 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 12:37:04,200 - INFO - CausalLM Model loaded.
2025-04-24 12:37:04,201 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 12:37:04,202 - INFO - Model loaded in 14.46 seconds.
2025-04-24 12:37:04,202 - INFO - Loading prompts from CTIBench subset: cti-mcq
2025-04-24 12:37:05,953 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 12:37:05,960 - INFO - Selected first 100 samples.
2025-04-24 12:37:05,960 - INFO - Using columns for cti-mcq: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-mcq):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-mcq): 100%|██████████| 100/100 [00:00<00:00, 4462.88it/s]
2025-04-24 12:37:05,984 - INFO - Successfully loaded 100 prompts from ctibench subset cti-mcq.
2025-04-24 12:37:05,984 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 12:37:05,985 - INFO - Starting inference on 100 samples with batch size 32...

Inference Batches:   0%|          | 0/4 [00:00<?, ?it/s]
Inference Batches:  25%|██▌       | 1/4 [00:01<00:04,  1.57s/it]
Inference Batches:  50%|█████     | 2/4 [00:02<00:02,  1.29s/it]
Inference Batches:  75%|███████▌  | 3/4 [00:03<00:01,  1.18s/it]
Inference Batches: 100%|██████████| 4/4 [00:04<00:00,  1.11it/s]
Inference Batches: 100%|██████████| 4/4 [00:04<00:00,  1.05s/it]
2025-04-24 12:37:10,178 - INFO - 
--- Inference Summary ---
2025-04-24 12:37:10,179 - INFO - Processed 100 samples.
2025-04-24 12:37:10,179 - INFO - Total 'generate' time (sum): 4.14 sec
2025-04-24 12:37:10,179 - INFO - Overall inference loop duration: 4.19 sec
2025-04-24 12:37:10,179 - INFO - Average 'generate' time per sample: 0.0414 sec
2025-04-24 12:37:10,179 - INFO - Total effective tokens generated: 200
2025-04-24 12:37:10,179 - INFO - Overall effective tokens per second: 48.31
2025-04-24 12:37:10,179 - INFO - RAM Delta during inference: 537.00 MB
2025-04-24 12:37:10,179 - INFO - PyTorch VRAM Peak Delta during inference: 5622.46 MB
2025-04-24 12:37:10,179 - INFO - System VRAM Peak Approx. Delta during inference: -3033.38 MB
2025-04-24 12:37:10,189 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-123649_outputs.json
2025-04-24 12:37:10,192 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-mcq_Phi-3-mini-4k-instruct-GPTQ_20250424-123649_metrics.json
2025-04-24 12:37:10,192 - INFO - Cleaning up resources...
2025-04-24 12:37:10,254 - INFO - CUDA cache cleared.
2025-04-24 12:37:10,254 - INFO - NVML shut down.
2025-04-24 12:37:10,254 - INFO - --- Evaluation Complete ---

2025-04-24 12:37:11,050 - INFO - SUCCESS: Run 3/5: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 12:37:11,052 - INFO - 
Run 4/5: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 12:37:11,053 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 32 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 12:45:07,000 - WARNING - Stderr:
2025-04-24 12:37:28,922 - INFO - Initialized NVML for device 0.
2025-04-24 12:37:29,067 - INFO - --- Starting Evaluation ---
2025-04-24 12:37:29,067 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-vsp', 'batch_size': 32, 'trust_remote_code': False}
2025-04-24 12:37:29,067 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 12:37:29,067 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-vsp_Phi-3-mini-4k-instruct-GPTQ_20250424-123729_metrics.json
2025-04-24 12:37:29,067 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-vsp_Phi-3-mini-4k-instruct-GPTQ_20250424-123729_outputs.json
2025-04-24 12:37:29,067 - INFO - NVML Reporting Active: True
2025-04-24 12:37:29,067 - INFO - Initial RAM usage: 535.72 MB
2025-04-24 12:37:29,068 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 12:37:29,068 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 12:37:29,068 - INFO - Trust Remote Code: False
2025-04-24 12:37:29,073 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 12:37:29,219 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 12:37:40,673 - WARNING - CUDA extension not installed.
2025-04-24 12:37:40,680 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 12:37:43,361 - INFO - CausalLM Model loaded.
2025-04-24 12:37:43,362 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 12:37:43,362 - INFO - Model loaded in 14.29 seconds.
2025-04-24 12:37:43,362 - INFO - Loading prompts from CTIBench subset: cti-vsp
2025-04-24 12:37:44,295 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 12:37:44,299 - INFO - Selected first 100 samples.
2025-04-24 12:37:44,299 - INFO - Using columns for cti-vsp: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-vsp):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-vsp): 100%|██████████| 100/100 [00:00<00:00, 6377.23it/s]
2025-04-24 12:37:44,316 - INFO - Successfully loaded 100 prompts from ctibench subset cti-vsp.
2025-04-24 12:37:44,316 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 12:37:44,318 - INFO - Starting inference on 100 samples with batch size 32...

Inference Batches:   0%|          | 0/4 [00:00<?, ?it/s]
Inference Batches:  25%|██▌       | 1/4 [02:02<06:07, 122.61s/it]
Inference Batches:  50%|█████     | 2/4 [04:10<04:11, 125.53s/it]
Inference Batches:  75%|███████▌  | 3/4 [06:05<02:00, 120.67s/it]
Inference Batches: 100%|██████████| 4/4 [07:21<00:00, 103.27s/it]
Inference Batches: 100%|██████████| 4/4 [07:21<00:00, 110.42s/it]
2025-04-24 12:45:05,990 - INFO - 
--- Inference Summary ---
2025-04-24 12:45:05,990 - INFO - Processed 100 samples.
2025-04-24 12:45:05,990 - INFO - Total 'generate' time (sum): 441.56 sec
2025-04-24 12:45:05,990 - INFO - Overall inference loop duration: 441.67 sec
2025-04-24 12:45:05,990 - INFO - Average 'generate' time per sample: 4.4156 sec
2025-04-24 12:45:05,990 - INFO - Total effective tokens generated: 51146
2025-04-24 12:45:05,990 - INFO - Overall effective tokens per second: 115.83
2025-04-24 12:45:05,990 - INFO - RAM Delta during inference: 159.00 MB
2025-04-24 12:45:05,990 - INFO - PyTorch VRAM Peak Delta during inference: 26377.94 MB
2025-04-24 12:45:05,990 - INFO - System VRAM Peak Approx. Delta during inference: -3033.38 MB
2025-04-24 12:45:06,028 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-vsp_Phi-3-mini-4k-instruct-GPTQ_20250424-123729_outputs.json
2025-04-24 12:45:06,040 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-vsp_Phi-3-mini-4k-instruct-GPTQ_20250424-123729_metrics.json
2025-04-24 12:45:06,040 - INFO - Cleaning up resources...
2025-04-24 12:45:06,131 - INFO - CUDA cache cleared.
2025-04-24 12:45:06,132 - INFO - NVML shut down.
2025-04-24 12:45:06,132 - INFO - --- Evaluation Complete ---

2025-04-24 12:45:07,018 - INFO - SUCCESS: Run 4/5: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-vsp'. Check output files in /workspace/results/mod/48/
2025-04-24 12:45:07,019 - INFO - 
Run 5/5: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 12:45:07,022 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 32 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 12:50:01,046 - WARNING - Stderr:
2025-04-24 12:45:34,610 - INFO - Initialized NVML for device 0.
2025-04-24 12:45:34,763 - INFO - --- Starting Evaluation ---
2025-04-24 12:45:34,763 - INFO - Args: {'model_path': '/workspace/models/Phi-3-mini-4k-instruct-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-rcm', 'batch_size': 32, 'trust_remote_code': False}
2025-04-24 12:45:34,763 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 12:45:34,763 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-rcm_Phi-3-mini-4k-instruct-GPTQ_20250424-124534_metrics.json
2025-04-24 12:45:34,763 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-rcm_Phi-3-mini-4k-instruct-GPTQ_20250424-124534_outputs.json
2025-04-24 12:45:34,763 - INFO - NVML Reporting Active: True
2025-04-24 12:45:34,763 - INFO - Initial RAM usage: 526.28 MB
2025-04-24 12:45:34,763 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 12:45:34,763 - INFO - Loading causal model from: /workspace/models/Phi-3-mini-4k-instruct-GPTQ...
2025-04-24 12:45:34,763 - INFO - Trust Remote Code: False
2025-04-24 12:45:34,769 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 12:45:34,949 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 12:45:46,501 - WARNING - CUDA extension not installed.
2025-04-24 12:45:46,515 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 12:45:49,745 - INFO - CausalLM Model loaded.
2025-04-24 12:45:49,746 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 12:45:49,746 - INFO - Model loaded in 14.98 seconds.
2025-04-24 12:45:49,746 - INFO - Loading prompts from CTIBench subset: cti-rcm
2025-04-24 12:45:50,980 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 12:45:50,984 - INFO - Selected first 100 samples.
2025-04-24 12:45:50,984 - INFO - Using columns for cti-rcm: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-rcm):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-rcm): 100%|██████████| 100/100 [00:00<00:00, 3389.12it/s]
2025-04-24 12:45:51,015 - INFO - Successfully loaded 100 prompts from ctibench subset cti-rcm.
2025-04-24 12:45:51,016 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 12:45:51,017 - INFO - Starting inference on 100 samples with batch size 32...

Inference Batches:   0%|          | 0/4 [00:00<?, ?it/s]
Inference Batches:  25%|██▌       | 1/4 [00:31<01:35, 31.92s/it]
Inference Batches:  50%|█████     | 2/4 [02:25<02:40, 80.25s/it]
Inference Batches:  75%|███████▌  | 3/4 [02:53<00:56, 56.32s/it]
Inference Batches: 100%|██████████| 4/4 [04:08<00:00, 63.75s/it]
Inference Batches: 100%|██████████| 4/4 [04:08<00:00, 62.25s/it]
2025-04-24 12:50:00,006 - INFO - 
--- Inference Summary ---
2025-04-24 12:50:00,006 - INFO - Processed 100 samples.
2025-04-24 12:50:00,006 - INFO - Total 'generate' time (sum): 248.91 sec
2025-04-24 12:50:00,007 - INFO - Overall inference loop duration: 248.99 sec
2025-04-24 12:50:00,007 - INFO - Average 'generate' time per sample: 2.4891 sec
2025-04-24 12:50:00,007 - INFO - Total effective tokens generated: 27966
2025-04-24 12:50:00,007 - INFO - Overall effective tokens per second: 112.36
2025-04-24 12:50:00,007 - INFO - RAM Delta during inference: 413.25 MB
2025-04-24 12:50:00,007 - INFO - PyTorch VRAM Peak Delta during inference: 18343.13 MB
2025-04-24 12:50:00,007 - INFO - System VRAM Peak Approx. Delta during inference: -3033.38 MB
2025-04-24 12:50:00,032 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-rcm_Phi-3-mini-4k-instruct-GPTQ_20250424-124534_outputs.json
2025-04-24 12:50:00,037 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-rcm_Phi-3-mini-4k-instruct-GPTQ_20250424-124534_metrics.json
2025-04-24 12:50:00,037 - INFO - Cleaning up resources...
2025-04-24 12:50:00,122 - INFO - CUDA cache cleared.
2025-04-24 12:50:00,122 - INFO - NVML shut down.
2025-04-24 12:50:00,122 - INFO - --- Evaluation Complete ---

2025-04-24 12:50:01,049 - INFO - SUCCESS: Run 5/5: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='ctibench', Subset='cti-rcm'. Check output files in /workspace/results/mod/48/
2025-04-24 12:50:01,050 - INFO - --- Security Benchmark Automation Finished (QUANTIZED MODELS) ---
2025-04-24 12:51:58,953 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 12:51:58,954 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 12:51:58,954 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 12:51:58,955 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 12:51:58,955 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 12:51:58,956 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_125158.log
2025-04-24 12:51:58,957 - INFO - Models to run: ['Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 12:51:58,957 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 12:51:58,958 - INFO - Running 100 samples per benchmark.
2025-04-24 12:51:58,958 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 12:51:58,959 - INFO - 
Run 1/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 12:51:58,961 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 32 --max-new-tokens 512 --trust-remote-code
2025-04-24 13:01:25,213 - WARNING - Stderr:
2025-04-24 12:52:16,209 - INFO - Initialized NVML for device 0.
2025-04-24 12:52:16,356 - INFO - --- Starting Evaluation ---
2025-04-24 12:52:16,356 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None, 'batch_size': 32, 'trust_remote_code': True}
2025-04-24 12:52:16,356 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 12:52:16,356 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_Mistral-7B-Instruct-v0.3-GPTQ_20250424-125216_metrics.json
2025-04-24 12:52:16,356 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_Mistral-7B-Instruct-v0.3-GPTQ_20250424-125216_outputs.json
2025-04-24 12:52:16,356 - INFO - NVML Reporting Active: True
2025-04-24 12:52:16,356 - INFO - Initial RAM usage: 520.48 MB
2025-04-24 12:52:16,356 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 12:52:16,356 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 12:52:16,356 - INFO - Trust Remote Code: True
2025-04-24 12:52:16,363 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 12:52:17,605 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 12:52:27,431 - WARNING - CUDA extension not installed.
2025-04-24 12:52:27,437 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 12:53:07,759 - INFO - CausalLM Model loaded.
2025-04-24 12:53:07,761 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 12:53:07,762 - INFO - Model loaded in 51.41 seconds.
2025-04-24 12:53:07,763 - INFO - Loading prompts from CyberSecEval3 MITRE file: /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json
2025-04-24 12:53:07,863 - INFO - Processing the first 100 items.

Parsing MITRE prompts:   0%|          | 0/100 [00:00<?, ?it/s]2025-04-24 12:53:07,869 - WARNING - Item 5: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,871 - WARNING - Item 8: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,873 - WARNING - Item 14: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,875 - WARNING - Item 21: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,876 - WARNING - Item 25: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,877 - WARNING - Item 27: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,878 - WARNING - Item 30: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,878 - WARNING - Item 32: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,880 - WARNING - Item 35: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,880 - WARNING - Item 36: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,883 - WARNING - Item 48: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,884 - WARNING - Item 49: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,885 - WARNING - Item 52: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,887 - WARNING - Item 58: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,888 - WARNING - Item 63: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,890 - WARNING - Item 72: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,893 - WARNING - Item 81: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,894 - WARNING - Item 86: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,894 - WARNING - Item 88: Regex failed to find/extract 'prompt' value from mutated_prompt_str.

Parsing MITRE prompts: 100%|██████████| 100/100 [00:00<00:00, 3350.40it/s]
2025-04-24 12:53:07,896 - INFO - Successfully loaded 81 prompts from cyberseceval3_mitre.
2025-04-24 12:53:07,897 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 12:53:07,898 - INFO - Starting inference on 81 samples with batch size 32...

Inference Batches:   0%|          | 0/3 [00:00<?, ?it/s]
Inference Batches:  33%|███▎      | 1/3 [02:51<05:43, 171.65s/it]
Inference Batches:  67%|██████▋   | 2/3 [05:41<02:50, 170.56s/it]
Inference Batches: 100%|██████████| 3/3 [08:16<00:00, 163.31s/it]
Inference Batches: 100%|██████████| 3/3 [08:16<00:00, 165.38s/it]
2025-04-24 13:01:24,027 - INFO - 
--- Inference Summary ---
2025-04-24 13:01:24,027 - INFO - Processed 81 samples.
2025-04-24 13:01:24,027 - INFO - Total 'generate' time (sum): 496.03 sec
2025-04-24 13:01:24,027 - INFO - Overall inference loop duration: 496.13 sec
2025-04-24 13:01:24,028 - INFO - Average 'generate' time per sample: 6.1239 sec
2025-04-24 13:01:24,028 - INFO - Total effective tokens generated: 41471
2025-04-24 13:01:24,028 - INFO - Overall effective tokens per second: 83.61
2025-04-24 13:01:24,028 - INFO - RAM Delta during inference: 668.25 MB
2025-04-24 13:01:24,028 - INFO - PyTorch VRAM Peak Delta during inference: 4523.40 MB
2025-04-24 13:01:24,028 - INFO - System VRAM Peak Approx. Delta during inference: -4797.38 MB
2025-04-24 13:01:24,045 - INFO - Saved outputs (81 items) to: /workspace/results/mod/48/cyberseceval3_mitre_Mistral-7B-Instruct-v0.3-GPTQ_20250424-125216_outputs.json
2025-04-24 13:01:24,050 - INFO - Saved metrics to: /workspace/results/mod/48/cyberseceval3_mitre_Mistral-7B-Instruct-v0.3-GPTQ_20250424-125216_metrics.json
2025-04-24 13:01:24,050 - INFO - Cleaning up resources...
2025-04-24 13:01:24,088 - INFO - CUDA cache cleared.
2025-04-24 13:01:24,088 - INFO - NVML shut down.
2025-04-24 13:01:24,088 - INFO - --- Evaluation Complete ---

2025-04-24 13:01:25,217 - INFO - SUCCESS: Run 1/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 13:01:25,218 - INFO - 
Run 2/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 13:01:25,219 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 32 --max-new-tokens 512 --trust-remote-code
2025-04-24 13:13:37,087 - WARNING - Stderr:
2025-04-24 13:01:42,703 - INFO - Initialized NVML for device 0.
2025-04-24 13:01:42,859 - INFO - --- Starting Evaluation ---
2025-04-24 13:01:42,859 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None, 'batch_size': 32, 'trust_remote_code': True}
2025-04-24 13:01:42,859 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 13:01:42,859 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_Mistral-7B-Instruct-v0.3-GPTQ_20250424-130142_metrics.json
2025-04-24 13:01:42,859 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_Mistral-7B-Instruct-v0.3-GPTQ_20250424-130142_outputs.json
2025-04-24 13:01:42,859 - INFO - NVML Reporting Active: True
2025-04-24 13:01:42,859 - INFO - Initial RAM usage: 512.08 MB
2025-04-24 13:01:42,859 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 13:01:42,859 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 13:01:42,859 - INFO - Trust Remote Code: True
2025-04-24 13:01:42,866 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 13:01:43,843 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 13:01:55,481 - WARNING - CUDA extension not installed.
2025-04-24 13:01:55,487 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 13:02:00,973 - INFO - CausalLM Model loaded.
2025-04-24 13:02:00,975 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 13:02:00,975 - INFO - Model loaded in 18.12 seconds.
2025-04-24 13:02:00,976 - INFO - Loading prompts from SEvenLLM saved dataset directory: /workspace/calibration_data/sevenllm_instruct_subset_manual/
2025-04-24 13:02:01,073 - INFO - Loaded dataset: Dataset({
    features: ['output', 'category', 'thought', 'input', 'id', 'instruction'],
    num_rows: 200
})
2025-04-24 13:02:01,076 - INFO - Selected first 100 samples for evaluation.

Parsing SEvenLLM prompts:   0%|          | 0/100 [00:00<?, ?it/s]
Parsing SEvenLLM prompts: 100%|██████████| 100/100 [00:00<00:00, 4421.15it/s]
2025-04-24 13:02:01,100 - INFO - Successfully loaded 100 prompts from sevenllm_bench.
2025-04-24 13:02:01,101 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 13:02:01,102 - INFO - Starting inference on 100 samples with batch size 32...

Inference Batches:   0%|          | 0/4 [00:00<?, ?it/s]
Inference Batches:  25%|██▌       | 1/4 [03:10<09:30, 190.13s/it]
Inference Batches:  50%|█████     | 2/4 [06:11<06:09, 184.93s/it]
Inference Batches:  75%|███████▌  | 3/4 [09:20<03:06, 186.60s/it]
Inference Batches: 100%|██████████| 4/4 [11:35<00:00, 166.24s/it]
Inference Batches: 100%|██████████| 4/4 [11:35<00:00, 173.76s/it]
2025-04-24 13:13:36,146 - INFO - 
--- Inference Summary ---
2025-04-24 13:13:36,146 - INFO - Processed 100 samples.
2025-04-24 13:13:36,146 - INFO - Total 'generate' time (sum): 694.92 sec
2025-04-24 13:13:36,146 - INFO - Overall inference loop duration: 695.04 sec
2025-04-24 13:13:36,146 - INFO - Average 'generate' time per sample: 6.9492 sec
2025-04-24 13:13:36,146 - INFO - Total effective tokens generated: 50986
2025-04-24 13:13:36,146 - INFO - Overall effective tokens per second: 73.37
2025-04-24 13:13:36,146 - INFO - RAM Delta during inference: 87.75 MB
2025-04-24 13:13:36,146 - INFO - PyTorch VRAM Peak Delta during inference: 6681.01 MB
2025-04-24 13:13:36,146 - INFO - System VRAM Peak Approx. Delta during inference: -4797.38 MB
2025-04-24 13:13:36,161 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/sevenllm_bench_Mistral-7B-Instruct-v0.3-GPTQ_20250424-130142_outputs.json
2025-04-24 13:13:36,166 - INFO - Saved metrics to: /workspace/results/mod/48/sevenllm_bench_Mistral-7B-Instruct-v0.3-GPTQ_20250424-130142_metrics.json
2025-04-24 13:13:36,166 - INFO - Cleaning up resources...
2025-04-24 13:13:36,199 - INFO - CUDA cache cleared.
2025-04-24 13:13:36,199 - INFO - NVML shut down.
2025-04-24 13:13:36,199 - INFO - --- Evaluation Complete ---

2025-04-24 13:13:37,091 - INFO - SUCCESS: Run 2/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 13:13:37,092 - INFO - 
Run 3/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 13:13:37,093 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 32 --cti-subset cti-mcq --max-new-tokens 512 --trust-remote-code
2025-04-24 13:17:49,105 - WARNING - Stderr:
2025-04-24 13:14:08,916 - INFO - Initialized NVML for device 0.
2025-04-24 13:14:09,077 - INFO - --- Starting Evaluation ---
2025-04-24 13:14:09,077 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq', 'batch_size': 32, 'trust_remote_code': True}
2025-04-24 13:14:09,077 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 13:14:09,077 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131409_metrics.json
2025-04-24 13:14:09,077 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131409_outputs.json
2025-04-24 13:14:09,077 - INFO - NVML Reporting Active: True
2025-04-24 13:14:09,077 - INFO - Initial RAM usage: 529.18 MB
2025-04-24 13:14:09,077 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 13:14:09,077 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 13:14:09,077 - INFO - Trust Remote Code: True
2025-04-24 13:14:09,086 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 13:14:09,814 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 13:14:26,769 - WARNING - CUDA extension not installed.
2025-04-24 13:14:26,778 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 13:14:58,342 - INFO - CausalLM Model loaded.
2025-04-24 13:14:58,345 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 13:14:58,347 - INFO - Model loaded in 49.27 seconds.
2025-04-24 13:14:58,347 - INFO - Loading prompts from CTIBench subset: cti-mcq
2025-04-24 13:14:59,385 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 13:14:59,391 - INFO - Selected first 100 samples.
2025-04-24 13:14:59,391 - INFO - Using columns for cti-mcq: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-mcq):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-mcq): 100%|██████████| 100/100 [00:00<00:00, 7951.74it/s]
2025-04-24 13:14:59,404 - INFO - Successfully loaded 100 prompts from ctibench subset cti-mcq.
2025-04-24 13:14:59,405 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 13:14:59,405 - INFO - Starting inference on 100 samples with batch size 32...

Inference Batches:   0%|          | 0/4 [00:00<?, ?it/s]
Inference Batches:  25%|██▌       | 1/4 [01:24<04:14, 84.81s/it]
Inference Batches:  50%|█████     | 2/4 [01:50<01:40, 50.03s/it]
Inference Batches:  75%|███████▌  | 3/4 [02:24<00:42, 42.83s/it]
Inference Batches: 100%|██████████| 4/4 [02:46<00:00, 34.54s/it]
Inference Batches: 100%|██████████| 4/4 [02:46<00:00, 41.65s/it]
2025-04-24 13:17:45,999 - INFO - 
--- Inference Summary ---
2025-04-24 13:17:45,999 - INFO - Processed 100 samples.
2025-04-24 13:17:45,999 - INFO - Total 'generate' time (sum): 166.53 sec
2025-04-24 13:17:45,999 - INFO - Overall inference loop duration: 166.59 sec
2025-04-24 13:17:45,999 - INFO - Average 'generate' time per sample: 1.6653 sec
2025-04-24 13:17:45,999 - INFO - Total effective tokens generated: 15616
2025-04-24 13:17:45,999 - INFO - Overall effective tokens per second: 93.77
2025-04-24 13:17:45,999 - INFO - RAM Delta during inference: 71.25 MB
2025-04-24 13:17:45,999 - INFO - PyTorch VRAM Peak Delta during inference: 2151.26 MB
2025-04-24 13:17:45,999 - INFO - System VRAM Peak Approx. Delta during inference: -4797.38 MB
2025-04-24 13:17:46,026 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-mcq_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131409_outputs.json
2025-04-24 13:17:46,034 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-mcq_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131409_metrics.json
2025-04-24 13:17:46,034 - INFO - Cleaning up resources...
2025-04-24 13:17:46,078 - INFO - CUDA cache cleared.
2025-04-24 13:17:46,078 - INFO - NVML shut down.
2025-04-24 13:17:46,078 - INFO - --- Evaluation Complete ---

2025-04-24 13:17:49,113 - INFO - SUCCESS: Run 3/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 13:17:49,114 - INFO - 
Run 4/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 13:17:49,115 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 32 --cti-subset cti-vsp --max-new-tokens 512 --trust-remote-code
2025-04-24 13:29:54,292 - WARNING - Stderr:
2025-04-24 13:18:06,712 - INFO - Initialized NVML for device 0.
2025-04-24 13:18:06,866 - INFO - --- Starting Evaluation ---
2025-04-24 13:18:06,866 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-vsp', 'batch_size': 32, 'trust_remote_code': True}
2025-04-24 13:18:06,866 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 13:18:06,866 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-vsp_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131806_metrics.json
2025-04-24 13:18:06,866 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-vsp_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131806_outputs.json
2025-04-24 13:18:06,866 - INFO - NVML Reporting Active: True
2025-04-24 13:18:06,866 - INFO - Initial RAM usage: 537.27 MB
2025-04-24 13:18:06,866 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 13:18:06,866 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 13:18:06,866 - INFO - Trust Remote Code: True
2025-04-24 13:18:06,872 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 13:18:07,466 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 13:18:19,327 - WARNING - CUDA extension not installed.
2025-04-24 13:18:19,332 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 13:18:24,546 - INFO - CausalLM Model loaded.
2025-04-24 13:18:24,547 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 13:18:24,548 - INFO - Model loaded in 17.68 seconds.
2025-04-24 13:18:24,548 - INFO - Loading prompts from CTIBench subset: cti-vsp
2025-04-24 13:18:25,401 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 13:18:25,405 - INFO - Selected first 100 samples.
2025-04-24 13:18:25,405 - INFO - Using columns for cti-vsp: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-vsp):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-vsp): 100%|██████████| 100/100 [00:00<00:00, 7315.31it/s]
2025-04-24 13:18:25,420 - INFO - Successfully loaded 100 prompts from ctibench subset cti-vsp.
2025-04-24 13:18:25,421 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 13:18:25,422 - INFO - Starting inference on 100 samples with batch size 32...

Inference Batches:   0%|          | 0/4 [00:00<?, ?it/s]
Inference Batches:  25%|██▌       | 1/4 [03:08<09:24, 188.01s/it]
Inference Batches:  50%|█████     | 2/4 [06:21<06:22, 191.38s/it]
Inference Batches:  75%|███████▌  | 3/4 [09:26<03:08, 188.23s/it]
Inference Batches: 100%|██████████| 4/4 [11:27<00:00, 161.97s/it]
Inference Batches: 100%|██████████| 4/4 [11:27<00:00, 171.99s/it]
2025-04-24 13:29:53,374 - INFO - 
--- Inference Summary ---
2025-04-24 13:29:53,374 - INFO - Processed 100 samples.
2025-04-24 13:29:53,374 - INFO - Total 'generate' time (sum): 687.82 sec
2025-04-24 13:29:53,374 - INFO - Overall inference loop duration: 687.95 sec
2025-04-24 13:29:53,374 - INFO - Average 'generate' time per sample: 6.8782 sec
2025-04-24 13:29:53,374 - INFO - Total effective tokens generated: 50835
2025-04-24 13:29:53,374 - INFO - Overall effective tokens per second: 73.91
2025-04-24 13:29:53,374 - INFO - RAM Delta during inference: 86.25 MB
2025-04-24 13:29:53,374 - INFO - PyTorch VRAM Peak Delta during inference: 7341.69 MB
2025-04-24 13:29:53,374 - INFO - System VRAM Peak Approx. Delta during inference: -4797.38 MB
2025-04-24 13:29:53,387 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-vsp_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131806_outputs.json
2025-04-24 13:29:53,391 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-vsp_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131806_metrics.json
2025-04-24 13:29:53,391 - INFO - Cleaning up resources...
2025-04-24 13:29:53,429 - INFO - CUDA cache cleared.
2025-04-24 13:29:53,430 - INFO - NVML shut down.
2025-04-24 13:29:53,430 - INFO - --- Evaluation Complete ---

2025-04-24 13:29:54,295 - INFO - SUCCESS: Run 4/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-vsp'. Check output files in /workspace/results/mod/48/
2025-04-24 13:29:54,296 - INFO - 
Run 5/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 13:29:54,297 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 32 --cti-subset cti-rcm --max-new-tokens 512 --trust-remote-code
2025-04-24 13:36:40,003 - WARNING - Stderr:
2025-04-24 13:30:11,822 - INFO - Initialized NVML for device 0.
2025-04-24 13:30:11,967 - INFO - --- Starting Evaluation ---
2025-04-24 13:30:11,967 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-rcm', 'batch_size': 32, 'trust_remote_code': True}
2025-04-24 13:30:11,967 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 13:30:11,967 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-rcm_Mistral-7B-Instruct-v0.3-GPTQ_20250424-133011_metrics.json
2025-04-24 13:30:11,967 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-rcm_Mistral-7B-Instruct-v0.3-GPTQ_20250424-133011_outputs.json
2025-04-24 13:30:11,967 - INFO - NVML Reporting Active: True
2025-04-24 13:30:11,967 - INFO - Initial RAM usage: 530.03 MB
2025-04-24 13:30:11,967 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 13:30:11,968 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 13:30:11,968 - INFO - Trust Remote Code: True
2025-04-24 13:30:11,973 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 13:30:12,585 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 13:30:23,316 - WARNING - CUDA extension not installed.
2025-04-24 13:30:23,320 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 13:30:28,452 - INFO - CausalLM Model loaded.
2025-04-24 13:30:28,453 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 13:30:28,454 - INFO - Model loaded in 16.49 seconds.
2025-04-24 13:30:28,454 - INFO - Loading prompts from CTIBench subset: cti-rcm
2025-04-24 13:30:29,443 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 13:30:29,447 - INFO - Selected first 100 samples.
2025-04-24 13:30:29,447 - INFO - Using columns for cti-rcm: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-rcm):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-rcm): 100%|██████████| 100/100 [00:00<00:00, 7408.34it/s]
2025-04-24 13:30:29,462 - INFO - Successfully loaded 100 prompts from ctibench subset cti-rcm.
2025-04-24 13:30:29,463 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 13:30:29,464 - INFO - Starting inference on 100 samples with batch size 32...

Inference Batches:   0%|          | 0/4 [00:00<?, ?it/s]
Inference Batches:  25%|██▌       | 1/4 [01:14<03:42, 74.24s/it]
Inference Batches:  50%|█████     | 2/4 [03:38<03:50, 115.45s/it]
Inference Batches:  75%|███████▌  | 3/4 [05:16<01:47, 107.46s/it]
Inference Batches: 100%|██████████| 4/4 [06:09<00:00, 85.99s/it] 
Inference Batches: 100%|██████████| 4/4 [06:09<00:00, 92.39s/it]
2025-04-24 13:36:39,021 - INFO - 
--- Inference Summary ---
2025-04-24 13:36:39,022 - INFO - Processed 100 samples.
2025-04-24 13:36:39,022 - INFO - Total 'generate' time (sum): 369.45 sec
2025-04-24 13:36:39,022 - INFO - Overall inference loop duration: 369.56 sec
2025-04-24 13:36:39,022 - INFO - Average 'generate' time per sample: 3.6945 sec
2025-04-24 13:36:39,022 - INFO - Total effective tokens generated: 30604
2025-04-24 13:36:39,022 - INFO - Overall effective tokens per second: 82.84
2025-04-24 13:36:39,022 - INFO - RAM Delta during inference: 159.00 MB
2025-04-24 13:36:39,022 - INFO - PyTorch VRAM Peak Delta during inference: 5149.85 MB
2025-04-24 13:36:39,022 - INFO - System VRAM Peak Approx. Delta during inference: -4797.38 MB
2025-04-24 13:36:39,032 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-rcm_Mistral-7B-Instruct-v0.3-GPTQ_20250424-133011_outputs.json
2025-04-24 13:36:39,037 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-rcm_Mistral-7B-Instruct-v0.3-GPTQ_20250424-133011_metrics.json
2025-04-24 13:36:39,037 - INFO - Cleaning up resources...
2025-04-24 13:36:39,071 - INFO - CUDA cache cleared.
2025-04-24 13:36:39,071 - INFO - NVML shut down.
2025-04-24 13:36:39,071 - INFO - --- Evaluation Complete ---

2025-04-24 13:36:40,018 - INFO - SUCCESS: Run 5/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-rcm'. Check output files in /workspace/results/mod/48/
2025-04-24 13:36:40,020 - INFO - --- Security Benchmark Automation Finished (QUANTIZED MODELS) ---
