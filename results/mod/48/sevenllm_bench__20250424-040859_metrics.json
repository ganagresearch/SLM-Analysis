{
    "run_args": {
        "model_path": "/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/",
        "model_type": "causal",
        "benchmark_name": "sevenllm_bench",
        "benchmark_path": "/workspace/datasets/SEVENLLM_instruct_HF",
        "results_dir": "/workspace/results/mod/48",
        "device": "cuda",
        "max_new_tokens": 256,
        "num_samples": 10,
        "cti_subset": null
    },
    "load_metrics": {
        "ram_initial_mb": 556.7890625,
        "system_vram_current_initial_mb": null,
        "load_time_sec": 2.290864944458008,
        "ram_after_load_mb": 1225.12109375,
        "pytorch_vram_current_after_load_mb": 759.2275390625,
        "pytorch_vram_peak_load_mb": 759.2275390625,
        "system_vram_current_after_load_mb": null
    },
    "inference_metrics": {
        "total_generate_time_sec": 25.09153175354004,
        "overall_inference_duration_sec": 25.117564916610718,
        "avg_generate_time_per_sample_sec": 2.5091531753540037,
        "total_tokens_generated": 2308,
        "tokens_per_second": 91.98322456636693,
        "ram_before_inference_mb": 1228.1015625,
        "ram_after_inference_mb": 1828.15625,
        "pytorch_vram_before_inference_mb": 759.2275390625,
        "pytorch_vram_after_inference_mb": 791.23388671875,
        "pytorch_vram_peak_inference_mb": 817.59033203125,
        "system_vram_before_inference_mb": null,
        "system_vram_after_inference_mb": null,
        "system_vram_peak_inference_approx_mb": null,
        "num_samples_run": 10
    }
}