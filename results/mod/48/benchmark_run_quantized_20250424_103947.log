2025-04-24 10:39:47,351 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 10:39:47,352 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 10:39:47,353 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 10:39:47,353 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 10:39:47,354 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 10:39:47,358 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_103947.log
2025-04-24 10:39:47,359 - INFO - Models to run: ['TinyLlama-1.1B-Chat-v1.0-GPTQ', 'Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 10:39:47,359 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 10:39:47,360 - INFO - Running 100 samples per benchmark.
2025-04-24 10:39:47,360 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 10:39:47,361 - INFO - 
Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 10:39:47,363 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 10:40:15,672 - WARNING - Stderr:
2025-04-24 10:40:11,030 - INFO - Initialized NVML for device 0.
2025-04-24 10:40:11,178 - INFO - --- Starting Evaluation ---
2025-04-24 10:40:11,178 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 10:40:11,178 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:40:11,178 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104011_metrics.json
2025-04-24 10:40:11,178 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104011_outputs.json
2025-04-24 10:40:11,178 - INFO - NVML Reporting Active: True
2025-04-24 10:40:11,178 - INFO - Initial RAM usage: 600.81 MB
2025-04-24 10:40:11,178 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:40:11,178 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:40:11,184 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:40:11,365 - INFO - Tokenizer loaded.
2025-04-24 10:40:15,078 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:40:15,089 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:40:15,090 - INFO - Cleaning up resources...
2025-04-24 10:40:15,090 - INFO - CUDA cache cleared.
2025-04-24 10:40:15,090 - INFO - NVML shut down.
2025-04-24 10:40:15,090 - INFO - --- Evaluation Complete ---

2025-04-24 10:40:15,673 - INFO - SUCCESS: Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 10:40:15,674 - INFO - 
Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 10:40:15,675 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
2025-04-24 10:40:38,858 - WARNING - Stderr:
2025-04-24 10:40:35,276 - INFO - Initialized NVML for device 0.
2025-04-24 10:40:35,421 - INFO - --- Starting Evaluation ---
2025-04-24 10:40:35,421 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None}
2025-04-24 10:40:35,421 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:40:35,421 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104035_metrics.json
2025-04-24 10:40:35,421 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104035_outputs.json
2025-04-24 10:40:35,421 - INFO - NVML Reporting Active: True
2025-04-24 10:40:35,421 - INFO - Initial RAM usage: 601.20 MB
2025-04-24 10:40:35,421 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:40:35,421 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:40:35,427 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:40:35,559 - INFO - Tokenizer loaded.
2025-04-24 10:40:38,236 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:40:38,255 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:40:38,256 - INFO - Cleaning up resources...
2025-04-24 10:40:38,256 - INFO - CUDA cache cleared.
2025-04-24 10:40:38,256 - INFO - NVML shut down.
2025-04-24 10:40:38,256 - INFO - --- Evaluation Complete ---

2025-04-24 10:40:38,860 - INFO - SUCCESS: Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 10:40:38,861 - INFO - 
Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 10:40:38,862 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 10:41:01,578 - WARNING - Stderr:
2025-04-24 10:40:58,133 - INFO - Initialized NVML for device 0.
2025-04-24 10:40:58,277 - INFO - --- Starting Evaluation ---
2025-04-24 10:40:58,277 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq'}
2025-04-24 10:40:58,277 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:40:58,277 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104058_metrics.json
2025-04-24 10:40:58,277 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104058_outputs.json
2025-04-24 10:40:58,277 - INFO - NVML Reporting Active: True
2025-04-24 10:40:58,277 - INFO - Initial RAM usage: 603.18 MB
2025-04-24 10:40:58,277 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:40:58,277 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:40:58,283 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:40:58,437 - INFO - Tokenizer loaded.
2025-04-24 10:41:00,953 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:41:00,970 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:41:00,972 - INFO - Cleaning up resources...
2025-04-24 10:41:00,972 - INFO - CUDA cache cleared.
2025-04-24 10:41:00,972 - INFO - NVML shut down.
2025-04-24 10:41:00,972 - INFO - --- Evaluation Complete ---

2025-04-24 10:41:01,580 - INFO - SUCCESS: Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 10:41:01,580 - INFO - 
Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 10:41:01,582 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-vsp --max-new-tokens 512
2025-04-24 10:41:23,925 - WARNING - Stderr:
2025-04-24 10:41:20,518 - INFO - Initialized NVML for device 0.
2025-04-24 10:41:20,662 - INFO - --- Starting Evaluation ---
2025-04-24 10:41:20,663 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-vsp'}
2025-04-24 10:41:20,663 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:41:20,663 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104120_metrics.json
2025-04-24 10:41:20,663 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-vsp_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104120_outputs.json
2025-04-24 10:41:20,663 - INFO - NVML Reporting Active: True
2025-04-24 10:41:20,663 - INFO - Initial RAM usage: 604.52 MB
2025-04-24 10:41:20,663 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:41:20,663 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:41:20,669 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:41:20,811 - INFO - Tokenizer loaded.
2025-04-24 10:41:23,308 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:41:23,326 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:41:23,327 - INFO - Cleaning up resources...
2025-04-24 10:41:23,327 - INFO - CUDA cache cleared.
2025-04-24 10:41:23,327 - INFO - NVML shut down.
2025-04-24 10:41:23,327 - INFO - --- Evaluation Complete ---

2025-04-24 10:41:23,928 - INFO - SUCCESS: Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'. Check output files in /workspace/results/mod/48/
2025-04-24 10:41:23,929 - INFO - 
Run 5/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 10:41:23,930 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --cti-subset cti-rcm --max-new-tokens 512
2025-04-24 10:41:46,709 - WARNING - Stderr:
2025-04-24 10:41:43,304 - INFO - Initialized NVML for device 0.
2025-04-24 10:41:43,449 - INFO - --- Starting Evaluation ---
2025-04-24 10:41:43,449 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-rcm'}
2025-04-24 10:41:43,449 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 10:41:43,449 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-rcm_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104143_metrics.json
2025-04-24 10:41:43,449 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-rcm_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-104143_outputs.json
2025-04-24 10:41:43,449 - INFO - NVML Reporting Active: True
2025-04-24 10:41:43,449 - INFO - Initial RAM usage: 600.79 MB
2025-04-24 10:41:43,449 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 10:41:43,449 - INFO - Loading causal model from: /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ...
2025-04-24 10:41:43,455 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 10:41:43,602 - INFO - Tokenizer loaded.
2025-04-24 10:41:46,053 - ERROR - Error loading model: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
Traceback (most recent call last):
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1967, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/lib/python3.11/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1204, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 940, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 31, in <module>
    from ...modeling_flash_attention_utils import FlashAttentionKwargs
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 36, in <module>
    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/__init__.py", line 3, in <module>
    from flash_attn.flash_attn_interface import (
  File "/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 15, in <module>
    import flash_attn_2_cuda as flash_attn_gpu
ImportError: /workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/workspace/code/model_evaluator/model_loader.py", line 33, in load_model_and_tokenizer
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device, **model_args)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 568, in from_pretrained
    model_class = _get_model_class(config, cls._model_mapping)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 388, in _get_model_class
    supported_models = model_mapping[type(config)]
                       ~~~~~~~~~~~~~^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 770, in __getitem__
    return self._load_attr_from_module(model_type, model_name)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 784, in _load_attr_from_module
    return getattribute_from_module(self._modules[module_name], attr)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 700, in getattribute_from_module
    if hasattr(module, attr):
       ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1955, in __getattr__
    module = self._get_module(self._class_to_module[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/utils/import_utils.py", line 1969, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
/workspace/testbedvenv/lib/python3.11/site-packages/flash_attn_2_cuda.cpython-311-x86_64-linux-gnu.so: undefined symbol: _ZN3c105ErrorC2ENS_14SourceLocationESs
2025-04-24 10:41:46,071 - ERROR - An error occurred during evaluation pipeline: Model/Tokenizer loading failed
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    raise RuntimeError("Model/Tokenizer loading failed")
RuntimeError: Model/Tokenizer loading failed
2025-04-24 10:41:46,072 - INFO - Cleaning up resources...
2025-04-24 10:41:46,072 - INFO - CUDA cache cleared.
2025-04-24 10:41:46,073 - INFO - NVML shut down.
2025-04-24 10:41:46,073 - INFO - --- Evaluation Complete ---

2025-04-24 10:41:46,711 - INFO - SUCCESS: Run 5/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-rcm'. Check output files in /workspace/results/mod/48/
2025-04-24 10:41:46,712 - INFO - 
Run 6/15: Model='Phi-3-mini-4k-instruct-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 10:41:46,714 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Phi-3-mini-4k-instruct-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --max-new-tokens 512
