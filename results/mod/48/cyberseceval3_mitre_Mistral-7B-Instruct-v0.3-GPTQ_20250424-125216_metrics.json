{
    "run_args": {
        "model_path": "/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ",
        "model_type": "causal",
        "benchmark_name": "cyberseceval3_mitre",
        "benchmark_path": "/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json",
        "results_dir": "/workspace/results/mod/48",
        "device": "cuda",
        "max_new_tokens": 512,
        "num_samples": 100,
        "cti_subset": null,
        "batch_size": 32,
        "trust_remote_code": true
    },
    "load_metrics": {
        "ram_initial_mb": 520.484375,
        "system_vram_current_initial_mb": 556.0625,
        "load_time_sec": 51.40504050254822,
        "ram_after_load_mb": 3425.37890625,
        "pytorch_vram_current_after_load_mb": 3975.25830078125,
        "pytorch_vram_peak_load_mb": 3975.25830078125,
        "system_vram_current_after_load_mb": null
    },
    "inference_metrics": {
        "total_generate_time_sec": 496.0319924354553,
        "overall_inference_duration_sec": 496.1275990009308,
        "avg_generate_time_per_sample_sec": 6.123851758462411,
        "total_tokens_generated": 41471,
        "tokens_per_second": 83.60549446898082,
        "ram_before_inference_mb": 3438.12890625,
        "ram_after_inference_mb": 4106.37890625,
        "pytorch_vram_before_inference_mb": 3975.25830078125,
        "pytorch_vram_after_inference_mb": 3983.74951171875,
        "pytorch_vram_peak_inference_mb": 8498.654296875,
        "system_vram_before_inference_mb": 4797.375,
        "system_vram_after_inference_mb": 11069.375,
        "system_vram_peak_inference_approx_mb": 0,
        "num_samples_run": 81,
        "num_results_produced": 81
    }
}