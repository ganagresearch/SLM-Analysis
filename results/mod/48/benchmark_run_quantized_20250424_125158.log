2025-04-24 12:51:58,953 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 12:51:58,954 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 12:51:58,954 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 12:51:58,955 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 12:51:58,955 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 12:51:58,956 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_125158.log
2025-04-24 12:51:58,957 - INFO - Models to run: ['Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 12:51:58,957 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 12:51:58,958 - INFO - Running 100 samples per benchmark.
2025-04-24 12:51:58,958 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 12:51:58,959 - INFO - 
Run 1/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 12:51:58,961 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 32 --max-new-tokens 512 --trust-remote-code
2025-04-24 13:01:25,213 - WARNING - Stderr:
2025-04-24 12:52:16,209 - INFO - Initialized NVML for device 0.
2025-04-24 12:52:16,356 - INFO - --- Starting Evaluation ---
2025-04-24 12:52:16,356 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None, 'batch_size': 32, 'trust_remote_code': True}
2025-04-24 12:52:16,356 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 12:52:16,356 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_Mistral-7B-Instruct-v0.3-GPTQ_20250424-125216_metrics.json
2025-04-24 12:52:16,356 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_Mistral-7B-Instruct-v0.3-GPTQ_20250424-125216_outputs.json
2025-04-24 12:52:16,356 - INFO - NVML Reporting Active: True
2025-04-24 12:52:16,356 - INFO - Initial RAM usage: 520.48 MB
2025-04-24 12:52:16,356 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 12:52:16,356 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 12:52:16,356 - INFO - Trust Remote Code: True
2025-04-24 12:52:16,363 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 12:52:17,605 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 12:52:27,431 - WARNING - CUDA extension not installed.
2025-04-24 12:52:27,437 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 12:53:07,759 - INFO - CausalLM Model loaded.
2025-04-24 12:53:07,761 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 12:53:07,762 - INFO - Model loaded in 51.41 seconds.
2025-04-24 12:53:07,763 - INFO - Loading prompts from CyberSecEval3 MITRE file: /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json
2025-04-24 12:53:07,863 - INFO - Processing the first 100 items.

Parsing MITRE prompts:   0%|          | 0/100 [00:00<?, ?it/s]2025-04-24 12:53:07,869 - WARNING - Item 5: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,871 - WARNING - Item 8: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,873 - WARNING - Item 14: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,875 - WARNING - Item 21: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,876 - WARNING - Item 25: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,877 - WARNING - Item 27: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,878 - WARNING - Item 30: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,878 - WARNING - Item 32: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,880 - WARNING - Item 35: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,880 - WARNING - Item 36: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,883 - WARNING - Item 48: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,884 - WARNING - Item 49: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,885 - WARNING - Item 52: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,887 - WARNING - Item 58: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,888 - WARNING - Item 63: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,890 - WARNING - Item 72: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,893 - WARNING - Item 81: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,894 - WARNING - Item 86: Regex failed to find/extract 'prompt' value from mutated_prompt_str.
2025-04-24 12:53:07,894 - WARNING - Item 88: Regex failed to find/extract 'prompt' value from mutated_prompt_str.

Parsing MITRE prompts: 100%|██████████| 100/100 [00:00<00:00, 3350.40it/s]
2025-04-24 12:53:07,896 - INFO - Successfully loaded 81 prompts from cyberseceval3_mitre.
2025-04-24 12:53:07,897 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 12:53:07,898 - INFO - Starting inference on 81 samples with batch size 32...

Inference Batches:   0%|          | 0/3 [00:00<?, ?it/s]
Inference Batches:  33%|███▎      | 1/3 [02:51<05:43, 171.65s/it]
Inference Batches:  67%|██████▋   | 2/3 [05:41<02:50, 170.56s/it]
Inference Batches: 100%|██████████| 3/3 [08:16<00:00, 163.31s/it]
Inference Batches: 100%|██████████| 3/3 [08:16<00:00, 165.38s/it]
2025-04-24 13:01:24,027 - INFO - 
--- Inference Summary ---
2025-04-24 13:01:24,027 - INFO - Processed 81 samples.
2025-04-24 13:01:24,027 - INFO - Total 'generate' time (sum): 496.03 sec
2025-04-24 13:01:24,027 - INFO - Overall inference loop duration: 496.13 sec
2025-04-24 13:01:24,028 - INFO - Average 'generate' time per sample: 6.1239 sec
2025-04-24 13:01:24,028 - INFO - Total effective tokens generated: 41471
2025-04-24 13:01:24,028 - INFO - Overall effective tokens per second: 83.61
2025-04-24 13:01:24,028 - INFO - RAM Delta during inference: 668.25 MB
2025-04-24 13:01:24,028 - INFO - PyTorch VRAM Peak Delta during inference: 4523.40 MB
2025-04-24 13:01:24,028 - INFO - System VRAM Peak Approx. Delta during inference: -4797.38 MB
2025-04-24 13:01:24,045 - INFO - Saved outputs (81 items) to: /workspace/results/mod/48/cyberseceval3_mitre_Mistral-7B-Instruct-v0.3-GPTQ_20250424-125216_outputs.json
2025-04-24 13:01:24,050 - INFO - Saved metrics to: /workspace/results/mod/48/cyberseceval3_mitre_Mistral-7B-Instruct-v0.3-GPTQ_20250424-125216_metrics.json
2025-04-24 13:01:24,050 - INFO - Cleaning up resources...
2025-04-24 13:01:24,088 - INFO - CUDA cache cleared.
2025-04-24 13:01:24,088 - INFO - NVML shut down.
2025-04-24 13:01:24,088 - INFO - --- Evaluation Complete ---

2025-04-24 13:01:25,217 - INFO - SUCCESS: Run 1/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 13:01:25,218 - INFO - 
Run 2/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 13:01:25,219 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 32 --max-new-tokens 512 --trust-remote-code
2025-04-24 13:13:37,087 - WARNING - Stderr:
2025-04-24 13:01:42,703 - INFO - Initialized NVML for device 0.
2025-04-24 13:01:42,859 - INFO - --- Starting Evaluation ---
2025-04-24 13:01:42,859 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None, 'batch_size': 32, 'trust_remote_code': True}
2025-04-24 13:01:42,859 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 13:01:42,859 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_Mistral-7B-Instruct-v0.3-GPTQ_20250424-130142_metrics.json
2025-04-24 13:01:42,859 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_Mistral-7B-Instruct-v0.3-GPTQ_20250424-130142_outputs.json
2025-04-24 13:01:42,859 - INFO - NVML Reporting Active: True
2025-04-24 13:01:42,859 - INFO - Initial RAM usage: 512.08 MB
2025-04-24 13:01:42,859 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 13:01:42,859 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 13:01:42,859 - INFO - Trust Remote Code: True
2025-04-24 13:01:42,866 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 13:01:43,843 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 13:01:55,481 - WARNING - CUDA extension not installed.
2025-04-24 13:01:55,487 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 13:02:00,973 - INFO - CausalLM Model loaded.
2025-04-24 13:02:00,975 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 13:02:00,975 - INFO - Model loaded in 18.12 seconds.
2025-04-24 13:02:00,976 - INFO - Loading prompts from SEvenLLM saved dataset directory: /workspace/calibration_data/sevenllm_instruct_subset_manual/
2025-04-24 13:02:01,073 - INFO - Loaded dataset: Dataset({
    features: ['output', 'category', 'thought', 'input', 'id', 'instruction'],
    num_rows: 200
})
2025-04-24 13:02:01,076 - INFO - Selected first 100 samples for evaluation.

Parsing SEvenLLM prompts:   0%|          | 0/100 [00:00<?, ?it/s]
Parsing SEvenLLM prompts: 100%|██████████| 100/100 [00:00<00:00, 4421.15it/s]
2025-04-24 13:02:01,100 - INFO - Successfully loaded 100 prompts from sevenllm_bench.
2025-04-24 13:02:01,101 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 13:02:01,102 - INFO - Starting inference on 100 samples with batch size 32...

Inference Batches:   0%|          | 0/4 [00:00<?, ?it/s]
Inference Batches:  25%|██▌       | 1/4 [03:10<09:30, 190.13s/it]
Inference Batches:  50%|█████     | 2/4 [06:11<06:09, 184.93s/it]
Inference Batches:  75%|███████▌  | 3/4 [09:20<03:06, 186.60s/it]
Inference Batches: 100%|██████████| 4/4 [11:35<00:00, 166.24s/it]
Inference Batches: 100%|██████████| 4/4 [11:35<00:00, 173.76s/it]
2025-04-24 13:13:36,146 - INFO - 
--- Inference Summary ---
2025-04-24 13:13:36,146 - INFO - Processed 100 samples.
2025-04-24 13:13:36,146 - INFO - Total 'generate' time (sum): 694.92 sec
2025-04-24 13:13:36,146 - INFO - Overall inference loop duration: 695.04 sec
2025-04-24 13:13:36,146 - INFO - Average 'generate' time per sample: 6.9492 sec
2025-04-24 13:13:36,146 - INFO - Total effective tokens generated: 50986
2025-04-24 13:13:36,146 - INFO - Overall effective tokens per second: 73.37
2025-04-24 13:13:36,146 - INFO - RAM Delta during inference: 87.75 MB
2025-04-24 13:13:36,146 - INFO - PyTorch VRAM Peak Delta during inference: 6681.01 MB
2025-04-24 13:13:36,146 - INFO - System VRAM Peak Approx. Delta during inference: -4797.38 MB
2025-04-24 13:13:36,161 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/sevenllm_bench_Mistral-7B-Instruct-v0.3-GPTQ_20250424-130142_outputs.json
2025-04-24 13:13:36,166 - INFO - Saved metrics to: /workspace/results/mod/48/sevenllm_bench_Mistral-7B-Instruct-v0.3-GPTQ_20250424-130142_metrics.json
2025-04-24 13:13:36,166 - INFO - Cleaning up resources...
2025-04-24 13:13:36,199 - INFO - CUDA cache cleared.
2025-04-24 13:13:36,199 - INFO - NVML shut down.
2025-04-24 13:13:36,199 - INFO - --- Evaluation Complete ---

2025-04-24 13:13:37,091 - INFO - SUCCESS: Run 2/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 13:13:37,092 - INFO - 
Run 3/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 13:13:37,093 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 32 --cti-subset cti-mcq --max-new-tokens 512 --trust-remote-code
2025-04-24 13:17:49,105 - WARNING - Stderr:
2025-04-24 13:14:08,916 - INFO - Initialized NVML for device 0.
2025-04-24 13:14:09,077 - INFO - --- Starting Evaluation ---
2025-04-24 13:14:09,077 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq', 'batch_size': 32, 'trust_remote_code': True}
2025-04-24 13:14:09,077 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 13:14:09,077 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131409_metrics.json
2025-04-24 13:14:09,077 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131409_outputs.json
2025-04-24 13:14:09,077 - INFO - NVML Reporting Active: True
2025-04-24 13:14:09,077 - INFO - Initial RAM usage: 529.18 MB
2025-04-24 13:14:09,077 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 13:14:09,077 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 13:14:09,077 - INFO - Trust Remote Code: True
2025-04-24 13:14:09,086 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 13:14:09,814 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 13:14:26,769 - WARNING - CUDA extension not installed.
2025-04-24 13:14:26,778 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 13:14:58,342 - INFO - CausalLM Model loaded.
2025-04-24 13:14:58,345 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 13:14:58,347 - INFO - Model loaded in 49.27 seconds.
2025-04-24 13:14:58,347 - INFO - Loading prompts from CTIBench subset: cti-mcq
2025-04-24 13:14:59,385 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 13:14:59,391 - INFO - Selected first 100 samples.
2025-04-24 13:14:59,391 - INFO - Using columns for cti-mcq: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-mcq):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-mcq): 100%|██████████| 100/100 [00:00<00:00, 7951.74it/s]
2025-04-24 13:14:59,404 - INFO - Successfully loaded 100 prompts from ctibench subset cti-mcq.
2025-04-24 13:14:59,405 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 13:14:59,405 - INFO - Starting inference on 100 samples with batch size 32...

Inference Batches:   0%|          | 0/4 [00:00<?, ?it/s]
Inference Batches:  25%|██▌       | 1/4 [01:24<04:14, 84.81s/it]
Inference Batches:  50%|█████     | 2/4 [01:50<01:40, 50.03s/it]
Inference Batches:  75%|███████▌  | 3/4 [02:24<00:42, 42.83s/it]
Inference Batches: 100%|██████████| 4/4 [02:46<00:00, 34.54s/it]
Inference Batches: 100%|██████████| 4/4 [02:46<00:00, 41.65s/it]
2025-04-24 13:17:45,999 - INFO - 
--- Inference Summary ---
2025-04-24 13:17:45,999 - INFO - Processed 100 samples.
2025-04-24 13:17:45,999 - INFO - Total 'generate' time (sum): 166.53 sec
2025-04-24 13:17:45,999 - INFO - Overall inference loop duration: 166.59 sec
2025-04-24 13:17:45,999 - INFO - Average 'generate' time per sample: 1.6653 sec
2025-04-24 13:17:45,999 - INFO - Total effective tokens generated: 15616
2025-04-24 13:17:45,999 - INFO - Overall effective tokens per second: 93.77
2025-04-24 13:17:45,999 - INFO - RAM Delta during inference: 71.25 MB
2025-04-24 13:17:45,999 - INFO - PyTorch VRAM Peak Delta during inference: 2151.26 MB
2025-04-24 13:17:45,999 - INFO - System VRAM Peak Approx. Delta during inference: -4797.38 MB
2025-04-24 13:17:46,026 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-mcq_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131409_outputs.json
2025-04-24 13:17:46,034 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-mcq_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131409_metrics.json
2025-04-24 13:17:46,034 - INFO - Cleaning up resources...
2025-04-24 13:17:46,078 - INFO - CUDA cache cleared.
2025-04-24 13:17:46,078 - INFO - NVML shut down.
2025-04-24 13:17:46,078 - INFO - --- Evaluation Complete ---

2025-04-24 13:17:49,113 - INFO - SUCCESS: Run 3/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 13:17:49,114 - INFO - 
Run 4/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 13:17:49,115 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 32 --cti-subset cti-vsp --max-new-tokens 512 --trust-remote-code
2025-04-24 13:29:54,292 - WARNING - Stderr:
2025-04-24 13:18:06,712 - INFO - Initialized NVML for device 0.
2025-04-24 13:18:06,866 - INFO - --- Starting Evaluation ---
2025-04-24 13:18:06,866 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-vsp', 'batch_size': 32, 'trust_remote_code': True}
2025-04-24 13:18:06,866 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 13:18:06,866 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-vsp_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131806_metrics.json
2025-04-24 13:18:06,866 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-vsp_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131806_outputs.json
2025-04-24 13:18:06,866 - INFO - NVML Reporting Active: True
2025-04-24 13:18:06,866 - INFO - Initial RAM usage: 537.27 MB
2025-04-24 13:18:06,866 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 13:18:06,866 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 13:18:06,866 - INFO - Trust Remote Code: True
2025-04-24 13:18:06,872 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 13:18:07,466 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 13:18:19,327 - WARNING - CUDA extension not installed.
2025-04-24 13:18:19,332 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 13:18:24,546 - INFO - CausalLM Model loaded.
2025-04-24 13:18:24,547 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 13:18:24,548 - INFO - Model loaded in 17.68 seconds.
2025-04-24 13:18:24,548 - INFO - Loading prompts from CTIBench subset: cti-vsp
2025-04-24 13:18:25,401 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 13:18:25,405 - INFO - Selected first 100 samples.
2025-04-24 13:18:25,405 - INFO - Using columns for cti-vsp: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-vsp):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-vsp): 100%|██████████| 100/100 [00:00<00:00, 7315.31it/s]
2025-04-24 13:18:25,420 - INFO - Successfully loaded 100 prompts from ctibench subset cti-vsp.
2025-04-24 13:18:25,421 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 13:18:25,422 - INFO - Starting inference on 100 samples with batch size 32...

Inference Batches:   0%|          | 0/4 [00:00<?, ?it/s]
Inference Batches:  25%|██▌       | 1/4 [03:08<09:24, 188.01s/it]
Inference Batches:  50%|█████     | 2/4 [06:21<06:22, 191.38s/it]
Inference Batches:  75%|███████▌  | 3/4 [09:26<03:08, 188.23s/it]
Inference Batches: 100%|██████████| 4/4 [11:27<00:00, 161.97s/it]
Inference Batches: 100%|██████████| 4/4 [11:27<00:00, 171.99s/it]
2025-04-24 13:29:53,374 - INFO - 
--- Inference Summary ---
2025-04-24 13:29:53,374 - INFO - Processed 100 samples.
2025-04-24 13:29:53,374 - INFO - Total 'generate' time (sum): 687.82 sec
2025-04-24 13:29:53,374 - INFO - Overall inference loop duration: 687.95 sec
2025-04-24 13:29:53,374 - INFO - Average 'generate' time per sample: 6.8782 sec
2025-04-24 13:29:53,374 - INFO - Total effective tokens generated: 50835
2025-04-24 13:29:53,374 - INFO - Overall effective tokens per second: 73.91
2025-04-24 13:29:53,374 - INFO - RAM Delta during inference: 86.25 MB
2025-04-24 13:29:53,374 - INFO - PyTorch VRAM Peak Delta during inference: 7341.69 MB
2025-04-24 13:29:53,374 - INFO - System VRAM Peak Approx. Delta during inference: -4797.38 MB
2025-04-24 13:29:53,387 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-vsp_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131806_outputs.json
2025-04-24 13:29:53,391 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-vsp_Mistral-7B-Instruct-v0.3-GPTQ_20250424-131806_metrics.json
2025-04-24 13:29:53,391 - INFO - Cleaning up resources...
2025-04-24 13:29:53,429 - INFO - CUDA cache cleared.
2025-04-24 13:29:53,430 - INFO - NVML shut down.
2025-04-24 13:29:53,430 - INFO - --- Evaluation Complete ---

2025-04-24 13:29:54,295 - INFO - SUCCESS: Run 4/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-vsp'. Check output files in /workspace/results/mod/48/
2025-04-24 13:29:54,296 - INFO - 
Run 5/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-rcm'
2025-04-24 13:29:54,297 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 32 --cti-subset cti-rcm --max-new-tokens 512 --trust-remote-code
2025-04-24 13:36:40,003 - WARNING - Stderr:
2025-04-24 13:30:11,822 - INFO - Initialized NVML for device 0.
2025-04-24 13:30:11,967 - INFO - --- Starting Evaluation ---
2025-04-24 13:30:11,967 - INFO - Args: {'model_path': '/workspace/models/Mistral-7B-Instruct-v0.3-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-rcm', 'batch_size': 32, 'trust_remote_code': True}
2025-04-24 13:30:11,967 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 13:30:11,967 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-rcm_Mistral-7B-Instruct-v0.3-GPTQ_20250424-133011_metrics.json
2025-04-24 13:30:11,967 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-rcm_Mistral-7B-Instruct-v0.3-GPTQ_20250424-133011_outputs.json
2025-04-24 13:30:11,967 - INFO - NVML Reporting Active: True
2025-04-24 13:30:11,967 - INFO - Initial RAM usage: 530.03 MB
2025-04-24 13:30:11,967 - INFO - Initial System VRAM usage: 556.06 MB
2025-04-24 13:30:11,968 - INFO - Loading causal model from: /workspace/models/Mistral-7B-Instruct-v0.3-GPTQ...
2025-04-24 13:30:11,968 - INFO - Trust Remote Code: True
2025-04-24 13:30:11,973 - INFO - Reset peak PyTorch VRAM stats.
2025-04-24 13:30:12,585 - INFO - Tokenizer loaded.
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:410: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:418: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/workspace/testbedvenv/lib/python3.11/site-packages/auto_gptq/nn_modules/triton_utils/kernels.py:461: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd(cast_inputs=torch.float16)
2025-04-24 13:30:23,316 - WARNING - CUDA extension not installed.
2025-04-24 13:30:23,320 - WARNING - CUDA extension not installed.
`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.
2025-04-24 13:30:28,452 - INFO - CausalLM Model loaded.
2025-04-24 13:30:28,453 - INFO - Model placed on device: cuda and set to eval mode.
2025-04-24 13:30:28,454 - INFO - Model loaded in 16.49 seconds.
2025-04-24 13:30:28,454 - INFO - Loading prompts from CTIBench subset: cti-rcm
2025-04-24 13:30:29,443 - INFO - Using split 'test' from CTIBench DatasetDict.
2025-04-24 13:30:29,447 - INFO - Selected first 100 samples.
2025-04-24 13:30:29,447 - INFO - Using columns for cti-rcm: prompt='Prompt', id=index fallback

Parsing CTIBench (cti-rcm):   0%|          | 0/100 [00:00<?, ?it/s]
Parsing CTIBench (cti-rcm): 100%|██████████| 100/100 [00:00<00:00, 7408.34it/s]
2025-04-24 13:30:29,462 - INFO - Successfully loaded 100 prompts from ctibench subset cti-rcm.
2025-04-24 13:30:29,463 - INFO - Reset peak PyTorch VRAM stats for inference.
2025-04-24 13:30:29,464 - INFO - Starting inference on 100 samples with batch size 32...

Inference Batches:   0%|          | 0/4 [00:00<?, ?it/s]
Inference Batches:  25%|██▌       | 1/4 [01:14<03:42, 74.24s/it]
Inference Batches:  50%|█████     | 2/4 [03:38<03:50, 115.45s/it]
Inference Batches:  75%|███████▌  | 3/4 [05:16<01:47, 107.46s/it]
Inference Batches: 100%|██████████| 4/4 [06:09<00:00, 85.99s/it] 
Inference Batches: 100%|██████████| 4/4 [06:09<00:00, 92.39s/it]
2025-04-24 13:36:39,021 - INFO - 
--- Inference Summary ---
2025-04-24 13:36:39,022 - INFO - Processed 100 samples.
2025-04-24 13:36:39,022 - INFO - Total 'generate' time (sum): 369.45 sec
2025-04-24 13:36:39,022 - INFO - Overall inference loop duration: 369.56 sec
2025-04-24 13:36:39,022 - INFO - Average 'generate' time per sample: 3.6945 sec
2025-04-24 13:36:39,022 - INFO - Total effective tokens generated: 30604
2025-04-24 13:36:39,022 - INFO - Overall effective tokens per second: 82.84
2025-04-24 13:36:39,022 - INFO - RAM Delta during inference: 159.00 MB
2025-04-24 13:36:39,022 - INFO - PyTorch VRAM Peak Delta during inference: 5149.85 MB
2025-04-24 13:36:39,022 - INFO - System VRAM Peak Approx. Delta during inference: -4797.38 MB
2025-04-24 13:36:39,032 - INFO - Saved outputs (100 items) to: /workspace/results/mod/48/ctibench_cti-rcm_Mistral-7B-Instruct-v0.3-GPTQ_20250424-133011_outputs.json
2025-04-24 13:36:39,037 - INFO - Saved metrics to: /workspace/results/mod/48/ctibench_cti-rcm_Mistral-7B-Instruct-v0.3-GPTQ_20250424-133011_metrics.json
2025-04-24 13:36:39,037 - INFO - Cleaning up resources...
2025-04-24 13:36:39,071 - INFO - CUDA cache cleared.
2025-04-24 13:36:39,071 - INFO - NVML shut down.
2025-04-24 13:36:39,071 - INFO - --- Evaluation Complete ---

2025-04-24 13:36:40,018 - INFO - SUCCESS: Run 5/5: Model='Mistral-7B-Instruct-v0.3-GPTQ', Benchmark='ctibench', Subset='cti-rcm'. Check output files in /workspace/results/mod/48/
2025-04-24 13:36:40,020 - INFO - --- Security Benchmark Automation Finished (QUANTIZED MODELS) ---
