2025-04-24 12:12:54,162 - INFO - --- Starting Security Benchmark Automation (QUANTIZED MODELS) ---
2025-04-24 12:12:54,163 - INFO - Using Python: /workspace/testbedvenv/bin/python
2025-04-24 12:12:54,163 - INFO - Evaluation Script: /workspace/code/model_evaluator/evaluate_cli.py
2025-04-24 12:12:54,164 - INFO - Results Directory: /workspace/results/mod/48
2025-04-24 12:12:54,165 - INFO - Error Log File: /workspace/results/mod/48/automation_quantized_errors.log
2025-04-24 12:12:54,166 - INFO - Run Log File: /workspace/results/mod/48/benchmark_run_quantized_20250424_121254.log
2025-04-24 12:12:54,166 - INFO - Models to run: ['TinyLlama-1.1B-Chat-v1.0-GPTQ', 'Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 12:12:54,167 - INFO - Benchmarks to run: [('cyberseceval3_mitre', [None]), ('sevenllm_bench', [None]), ('ctibench', ['cti-mcq', 'cti-vsp', 'cti-rcm'])]
2025-04-24 12:12:54,168 - INFO - Running 100 samples per benchmark.
2025-04-24 12:12:54,168 - INFO - Max New Tokens for Causal Models: 512
2025-04-24 12:12:54,169 - INFO - 
Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'
2025-04-24 12:12:54,171 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name cyberseceval3_mitre --benchmark-path /workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
2025-04-24 12:13:10,971 - WARNING - Stderr:
2025-04-24 12:13:10,367 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 12:13:10,521 - INFO - --- Starting Evaluation ---
2025-04-24 12:13:10,521 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'cyberseceval3_mitre', 'benchmark_path': '/workspace/code/PurpleLlama/CybersecurityBenchmarks/datasets/mitre/mitre_benchmark_100_per_category_with_augmentation.json', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None, 'batch_size': 8, 'trust_remote_code': False}
2025-04-24 12:13:10,521 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 12:13:10,521 - INFO - Metrics file: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-121310_metrics.json
2025-04-24 12:13:10,521 - INFO - Outputs file: /workspace/results/mod/48/cyberseceval3_mitre_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-121310_outputs.json
2025-04-24 12:13:10,521 - INFO - NVML Reporting Active: False
2025-04-24 12:13:10,521 - INFO - Initial RAM usage: 520.38 MB
2025-04-24 12:13:10,521 - ERROR - An error occurred during evaluation pipeline: load_model_and_tokenizer() got an unexpected keyword argument 'trust_remote_code'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    model, tokenizer, load_metrics_partial = load_model_and_tokenizer(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: load_model_and_tokenizer() got an unexpected keyword argument 'trust_remote_code'
2025-04-24 12:13:10,522 - INFO - Cleaning up resources...
2025-04-24 12:13:10,522 - INFO - CUDA cache cleared.
2025-04-24 12:13:10,522 - INFO - --- Evaluation Complete ---

2025-04-24 12:13:10,972 - INFO - SUCCESS: Run 1/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='cyberseceval3_mitre'. Check output files in /workspace/results/mod/48/
2025-04-24 12:13:10,973 - INFO - 
Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'
2025-04-24 12:13:10,974 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name sevenllm_bench --benchmark-path /workspace/calibration_data/sevenllm_instruct_subset_manual/ --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --max-new-tokens 512
2025-04-24 12:13:28,319 - WARNING - Stderr:
2025-04-24 12:13:27,763 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 12:13:27,915 - INFO - --- Starting Evaluation ---
2025-04-24 12:13:27,915 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'sevenllm_bench', 'benchmark_path': '/workspace/calibration_data/sevenllm_instruct_subset_manual/', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': None, 'batch_size': 8, 'trust_remote_code': False}
2025-04-24 12:13:27,915 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 12:13:27,915 - INFO - Metrics file: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-121327_metrics.json
2025-04-24 12:13:27,915 - INFO - Outputs file: /workspace/results/mod/48/sevenllm_bench_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-121327_outputs.json
2025-04-24 12:13:27,915 - INFO - NVML Reporting Active: False
2025-04-24 12:13:27,915 - INFO - Initial RAM usage: 518.64 MB
2025-04-24 12:13:27,915 - ERROR - An error occurred during evaluation pipeline: load_model_and_tokenizer() got an unexpected keyword argument 'trust_remote_code'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    model, tokenizer, load_metrics_partial = load_model_and_tokenizer(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: load_model_and_tokenizer() got an unexpected keyword argument 'trust_remote_code'
2025-04-24 12:13:27,916 - INFO - Cleaning up resources...
2025-04-24 12:13:27,916 - INFO - CUDA cache cleared.
2025-04-24 12:13:27,916 - INFO - --- Evaluation Complete ---

2025-04-24 12:13:28,322 - INFO - SUCCESS: Run 2/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='sevenllm_bench'. Check output files in /workspace/results/mod/48/
2025-04-24 12:13:28,323 - INFO - 
Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'
2025-04-24 12:13:28,324 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-mcq --max-new-tokens 512
2025-04-24 12:13:45,330 - WARNING - Stderr:
2025-04-24 12:13:44,773 - WARNING - pynvml not found, GPU system memory usage will not be reported. Run 'pip install nvidia-ml-py' to enable.
2025-04-24 12:13:44,921 - INFO - --- Starting Evaluation ---
2025-04-24 12:13:44,921 - INFO - Args: {'model_path': '/workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ', 'model_type': 'causal', 'benchmark_name': 'ctibench', 'benchmark_path': '/workspace/datasets', 'results_dir': '/workspace/results/mod/48', 'device': 'cuda', 'max_new_tokens': 512, 'num_samples': 100, 'cti_subset': 'cti-mcq', 'batch_size': 8, 'trust_remote_code': False}
2025-04-24 12:13:44,921 - INFO - Results Dir: /workspace/results/mod/48
2025-04-24 12:13:44,921 - INFO - Metrics file: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-121344_metrics.json
2025-04-24 12:13:44,921 - INFO - Outputs file: /workspace/results/mod/48/ctibench_cti-mcq_TinyLlama-1.1B-Chat-v1.0-GPTQ_20250424-121344_outputs.json
2025-04-24 12:13:44,921 - INFO - NVML Reporting Active: False
2025-04-24 12:13:44,921 - INFO - Initial RAM usage: 522.47 MB
2025-04-24 12:13:44,921 - ERROR - An error occurred during evaluation pipeline: load_model_and_tokenizer() got an unexpected keyword argument 'trust_remote_code'
Traceback (most recent call last):
  File "/workspace/code/model_evaluator/evaluate_cli.py", line 66, in main
    model, tokenizer, load_metrics_partial = load_model_and_tokenizer(
                                             ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: load_model_and_tokenizer() got an unexpected keyword argument 'trust_remote_code'
2025-04-24 12:13:44,922 - INFO - Cleaning up resources...
2025-04-24 12:13:44,922 - INFO - CUDA cache cleared.
2025-04-24 12:13:44,922 - INFO - --- Evaluation Complete ---

2025-04-24 12:13:45,332 - INFO - SUCCESS: Run 3/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-mcq'. Check output files in /workspace/results/mod/48/
2025-04-24 12:13:45,332 - INFO - 
Run 4/15: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Benchmark='ctibench', Subset='cti-vsp'
2025-04-24 12:13:45,333 - INFO - Executing command: /workspace/testbedvenv/bin/python /workspace/code/model_evaluator/evaluate_cli.py --model-path /workspace/models/TinyLlama-1.1B-Chat-v1.0-GPTQ --model-type causal --benchmark-name ctibench --benchmark-path /workspace/datasets --results-dir /workspace/results/mod/48 --num-samples 100 --batch-size 8 --cti-subset cti-vsp --max-new-tokens 512
