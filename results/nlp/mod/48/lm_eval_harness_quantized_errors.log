2025-04-24 03:47:33,411 - INFO - --- Starting LM Evaluation Harness Automation (QUANTIZED MODELS) ---
2025-04-24 03:47:33,411 - INFO - Using Python executable: /workspace/testbedvenv/bin/python
2025-04-24 03:47:33,411 - INFO - Running lm_eval as module: python -m lm_eval
2025-04-24 03:47:33,411 - INFO - Harness Directory (for cwd): /workspace/code/lm-evaluation-harness
2025-04-24 03:47:33,411 - INFO - Results Base Directory: /workspace/results/nlp/mod/48
2025-04-24 03:47:33,411 - INFO - Error Log File: /workspace/results/nlp/mod/48/lm_eval_harness_quantized_errors.log
2025-04-24 03:47:33,411 - INFO - Models to run: ['TinyLlama-1.1B-Chat-v1.0-GPTQ', 'Phi-3-mini-4k-instruct-GPTQ', 'Mistral-7B-Instruct-v0.3-GPTQ']
2025-04-24 03:47:33,411 - INFO - Tasks to run: ['arc_challenge', 'hellaswag', 'winogrande', 'mmlu_elementary_mathematics', 'truthfulqa_mc2', 'gsm8k']
2025-04-24 03:47:33,411 - INFO - Num Fewshot: 0
2025-04-24 03:47:33,411 - INFO - Batch Size: auto
2025-04-24 03:47:33,411 - INFO - Applying limit: 5
2025-04-24 03:47:33,411 - INFO - Total runs planned: 18
2025-04-24 03:47:33,411 - INFO - Verifying lm-evaluation-harness installation via python -m lm_eval --help...
2025-04-24 03:47:36,846 - INFO - lm_eval module found and appears runnable via specified Python.
2025-04-24 03:47:36,847 - INFO - 
Run 1/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Task='arc_challenge'
2025-04-24 03:47:36,847 - INFO - Executing command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/,disable_exllama=True,device_map='auto' --tasks arc_challenge --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/TinyLlama-1.1B-Chat-v1.0-GPTQ/arc_challenge/results.json --limit 5
2025-04-24 03:47:42,293 - ERROR - ERROR during: Run 1/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Task='arc_challenge'
  Command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/,disable_exllama=True,device_map='auto' --tasks arc_challenge --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/TinyLlama-1.1B-Chat-v1.0-GPTQ/arc_challenge/results.json --limit 5
  Return Code: 1
  Output File Target: /workspace/results/nlp/mod/48/TinyLlama-1.1B-Chat-v1.0-GPTQ/arc_challenge/results.json
  Stderr:

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.5 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "/workspace/code/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 10, in <module>
    import torch
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-04-24:03:47:41 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-04-24:03:47:41 INFO     [__main__:440] Selected Tasks: ['arc_challenge']
2025-04-24:03:47:41 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-04-24:03:47:41 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/', 'disable_exllama': True, 'device_map': "'auto'"}
2025-04-24:03:47:41 INFO     [models.huggingface:137] Using device 'cuda:0'
2025-04-24:03:47:41 INFO     [models.huggingface:388] Model parallel was set to False.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 571, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 205, in merge_quantization_configs
    loading_attr_dict = quantization_config_from_args.get_loading_attributes()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'get_loading_attributes'

  Stdout:

----------------------------------------

2025-04-24 03:47:42,293 - INFO - 
Run 2/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Task='hellaswag'
2025-04-24 03:47:42,294 - INFO - Executing command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/,disable_exllama=True,device_map='auto' --tasks hellaswag --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/TinyLlama-1.1B-Chat-v1.0-GPTQ/hellaswag/results.json --limit 5
2025-04-24 03:47:47,638 - ERROR - ERROR during: Run 2/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Task='hellaswag'
  Command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/,disable_exllama=True,device_map='auto' --tasks hellaswag --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/TinyLlama-1.1B-Chat-v1.0-GPTQ/hellaswag/results.json --limit 5
  Return Code: 1
  Output File Target: /workspace/results/nlp/mod/48/TinyLlama-1.1B-Chat-v1.0-GPTQ/hellaswag/results.json
  Stderr:

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.5 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "/workspace/code/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 10, in <module>
    import torch
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-04-24:03:47:46 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-04-24:03:47:46 INFO     [__main__:440] Selected Tasks: ['hellaswag']
2025-04-24:03:47:46 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-04-24:03:47:46 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/', 'disable_exllama': True, 'device_map': "'auto'"}
2025-04-24:03:47:47 INFO     [models.huggingface:137] Using device 'cuda:0'
2025-04-24:03:47:47 INFO     [models.huggingface:388] Model parallel was set to False.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 571, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 205, in merge_quantization_configs
    loading_attr_dict = quantization_config_from_args.get_loading_attributes()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'get_loading_attributes'

  Stdout:

----------------------------------------

2025-04-24 03:47:47,638 - INFO - 
Run 3/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Task='winogrande'
2025-04-24 03:47:47,638 - INFO - Executing command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/,disable_exllama=True,device_map='auto' --tasks winogrande --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/TinyLlama-1.1B-Chat-v1.0-GPTQ/winogrande/results.json --limit 5
2025-04-24 03:47:53,145 - ERROR - ERROR during: Run 3/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Task='winogrande'
  Command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/,disable_exllama=True,device_map='auto' --tasks winogrande --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/TinyLlama-1.1B-Chat-v1.0-GPTQ/winogrande/results.json --limit 5
  Return Code: 1
  Output File Target: /workspace/results/nlp/mod/48/TinyLlama-1.1B-Chat-v1.0-GPTQ/winogrande/results.json
  Stderr:

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.5 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "/workspace/code/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 10, in <module>
    import torch
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-04-24:03:47:52 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-04-24:03:47:52 INFO     [__main__:440] Selected Tasks: ['winogrande']
2025-04-24:03:47:52 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-04-24:03:47:52 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/', 'disable_exllama': True, 'device_map': "'auto'"}
2025-04-24:03:47:52 INFO     [models.huggingface:137] Using device 'cuda:0'
2025-04-24:03:47:52 INFO     [models.huggingface:388] Model parallel was set to False.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 571, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 205, in merge_quantization_configs
    loading_attr_dict = quantization_config_from_args.get_loading_attributes()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'get_loading_attributes'

  Stdout:

----------------------------------------

2025-04-24 03:47:53,145 - INFO - 
Run 4/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Task='mmlu_elementary_mathematics'
2025-04-24 03:47:53,145 - INFO - Executing command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/,disable_exllama=True,device_map='auto' --tasks mmlu_elementary_mathematics --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/TinyLlama-1.1B-Chat-v1.0-GPTQ/mmlu_elementary_mathematics/results.json --limit 5
2025-04-24 03:47:58,907 - ERROR - ERROR during: Run 4/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Task='mmlu_elementary_mathematics'
  Command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/,disable_exllama=True,device_map='auto' --tasks mmlu_elementary_mathematics --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/TinyLlama-1.1B-Chat-v1.0-GPTQ/mmlu_elementary_mathematics/results.json --limit 5
  Return Code: 1
  Output File Target: /workspace/results/nlp/mod/48/TinyLlama-1.1B-Chat-v1.0-GPTQ/mmlu_elementary_mathematics/results.json
  Stderr:

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.5 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "/workspace/code/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 10, in <module>
    import torch
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-04-24:03:47:57 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-04-24:03:47:57 INFO     [__main__:440] Selected Tasks: ['mmlu_elementary_mathematics']
2025-04-24:03:47:57 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-04-24:03:47:57 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/', 'disable_exllama': True, 'device_map': "'auto'"}
2025-04-24:03:47:58 INFO     [models.huggingface:137] Using device 'cuda:0'
2025-04-24:03:47:58 INFO     [models.huggingface:388] Model parallel was set to False.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 571, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 205, in merge_quantization_configs
    loading_attr_dict = quantization_config_from_args.get_loading_attributes()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'get_loading_attributes'

  Stdout:

----------------------------------------

2025-04-24 03:47:58,907 - INFO - 
Run 5/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Task='truthfulqa_mc2'
2025-04-24 03:47:58,907 - INFO - Executing command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/,disable_exllama=True,device_map='auto' --tasks truthfulqa_mc2 --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/TinyLlama-1.1B-Chat-v1.0-GPTQ/truthfulqa_mc2/results.json --limit 5
2025-04-24 03:48:04,466 - ERROR - ERROR during: Run 5/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Task='truthfulqa_mc2'
  Command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/,disable_exllama=True,device_map='auto' --tasks truthfulqa_mc2 --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/TinyLlama-1.1B-Chat-v1.0-GPTQ/truthfulqa_mc2/results.json --limit 5
  Return Code: 1
  Output File Target: /workspace/results/nlp/mod/48/TinyLlama-1.1B-Chat-v1.0-GPTQ/truthfulqa_mc2/results.json
  Stderr:

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.5 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "/workspace/code/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 10, in <module>
    import torch
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-04-24:03:48:03 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-04-24:03:48:03 INFO     [__main__:440] Selected Tasks: ['truthfulqa_mc2']
2025-04-24:03:48:03 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-04-24:03:48:03 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/', 'disable_exllama': True, 'device_map': "'auto'"}
2025-04-24:03:48:03 INFO     [models.huggingface:137] Using device 'cuda:0'
2025-04-24:03:48:03 INFO     [models.huggingface:388] Model parallel was set to False.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 571, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 205, in merge_quantization_configs
    loading_attr_dict = quantization_config_from_args.get_loading_attributes()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'get_loading_attributes'

  Stdout:

----------------------------------------

2025-04-24 03:48:04,466 - INFO - 
Run 6/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Task='gsm8k'
2025-04-24 03:48:04,466 - INFO - Executing command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/,disable_exllama=True,device_map='auto' --tasks gsm8k --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/TinyLlama-1.1B-Chat-v1.0-GPTQ/gsm8k/results.json --limit 5
2025-04-24 03:48:09,955 - ERROR - ERROR during: Run 6/18: Model='TinyLlama-1.1B-Chat-v1.0-GPTQ', Task='gsm8k'
  Command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/,disable_exllama=True,device_map='auto' --tasks gsm8k --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/TinyLlama-1.1B-Chat-v1.0-GPTQ/gsm8k/results.json --limit 5
  Return Code: 1
  Output File Target: /workspace/results/nlp/mod/48/TinyLlama-1.1B-Chat-v1.0-GPTQ/gsm8k/results.json
  Stderr:

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.5 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "/workspace/code/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 10, in <module>
    import torch
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-04-24:03:48:08 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-04-24:03:48:08 INFO     [__main__:440] Selected Tasks: ['gsm8k']
2025-04-24:03:48:08 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-04-24:03:48:08 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/workspace/models/TinyLlama-1.1B-Chat-v1.0/quantized-gptq-4bit/', 'disable_exllama': True, 'device_map': "'auto'"}
2025-04-24:03:48:09 INFO     [models.huggingface:137] Using device 'cuda:0'
2025-04-24:03:48:09 INFO     [models.huggingface:388] Model parallel was set to False.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 571, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 205, in merge_quantization_configs
    loading_attr_dict = quantization_config_from_args.get_loading_attributes()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'get_loading_attributes'

  Stdout:

----------------------------------------

2025-04-24 03:48:09,955 - INFO - 
Run 7/18: Model='Phi-3-mini-4k-instruct-GPTQ', Task='arc_challenge'
2025-04-24 03:48:09,955 - INFO - Executing command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks arc_challenge --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Phi-3-mini-4k-instruct-GPTQ/arc_challenge/results.json --limit 5
2025-04-24 03:48:15,685 - ERROR - ERROR during: Run 7/18: Model='Phi-3-mini-4k-instruct-GPTQ', Task='arc_challenge'
  Command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks arc_challenge --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Phi-3-mini-4k-instruct-GPTQ/arc_challenge/results.json --limit 5
  Return Code: 1
  Output File Target: /workspace/results/nlp/mod/48/Phi-3-mini-4k-instruct-GPTQ/arc_challenge/results.json
  Stderr:

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.5 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "/workspace/code/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 10, in <module>
    import torch
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-04-24:03:48:14 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-04-24:03:48:14 INFO     [__main__:440] Selected Tasks: ['arc_challenge']
2025-04-24:03:48:14 WARNING  [evaluator:159] Instruct model detected, but chat template not applied. Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).
2025-04-24:03:48:14 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-04-24:03:48:14 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/', 'trust_remote_code': True, 'disable_exllama': True, 'device_map': "'auto'"}
2025-04-24:03:48:14 INFO     [models.huggingface:137] Using device 'cuda:0'
2025-04-24:03:48:14 INFO     [models.huggingface:388] Model parallel was set to False.
2025-04-24:03:48:15 WARNING  [transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:62] `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24:03:48:15 WARNING  [transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:66] Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 205, in merge_quantization_configs
    loading_attr_dict = quantization_config_from_args.get_loading_attributes()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'get_loading_attributes'

  Stdout:

----------------------------------------

2025-04-24 03:48:15,685 - INFO - 
Run 8/18: Model='Phi-3-mini-4k-instruct-GPTQ', Task='hellaswag'
2025-04-24 03:48:15,685 - INFO - Executing command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks hellaswag --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Phi-3-mini-4k-instruct-GPTQ/hellaswag/results.json --limit 5
2025-04-24 03:48:21,238 - ERROR - ERROR during: Run 8/18: Model='Phi-3-mini-4k-instruct-GPTQ', Task='hellaswag'
  Command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks hellaswag --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Phi-3-mini-4k-instruct-GPTQ/hellaswag/results.json --limit 5
  Return Code: 1
  Output File Target: /workspace/results/nlp/mod/48/Phi-3-mini-4k-instruct-GPTQ/hellaswag/results.json
  Stderr:

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.5 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "/workspace/code/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 10, in <module>
    import torch
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-04-24:03:48:20 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-04-24:03:48:20 INFO     [__main__:440] Selected Tasks: ['hellaswag']
2025-04-24:03:48:20 WARNING  [evaluator:159] Instruct model detected, but chat template not applied. Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).
2025-04-24:03:48:20 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-04-24:03:48:20 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/', 'trust_remote_code': True, 'disable_exllama': True, 'device_map': "'auto'"}
2025-04-24:03:48:20 INFO     [models.huggingface:137] Using device 'cuda:0'
2025-04-24:03:48:20 INFO     [models.huggingface:388] Model parallel was set to False.
2025-04-24:03:48:20 WARNING  [transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:62] `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24:03:48:20 WARNING  [transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:66] Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 205, in merge_quantization_configs
    loading_attr_dict = quantization_config_from_args.get_loading_attributes()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'get_loading_attributes'

  Stdout:

----------------------------------------

2025-04-24 03:48:21,238 - INFO - 
Run 9/18: Model='Phi-3-mini-4k-instruct-GPTQ', Task='winogrande'
2025-04-24 03:48:21,239 - INFO - Executing command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks winogrande --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Phi-3-mini-4k-instruct-GPTQ/winogrande/results.json --limit 5
2025-04-24 03:48:26,769 - ERROR - ERROR during: Run 9/18: Model='Phi-3-mini-4k-instruct-GPTQ', Task='winogrande'
  Command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks winogrande --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Phi-3-mini-4k-instruct-GPTQ/winogrande/results.json --limit 5
  Return Code: 1
  Output File Target: /workspace/results/nlp/mod/48/Phi-3-mini-4k-instruct-GPTQ/winogrande/results.json
  Stderr:

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.5 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "/workspace/code/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 10, in <module>
    import torch
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-04-24:03:48:25 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-04-24:03:48:25 INFO     [__main__:440] Selected Tasks: ['winogrande']
2025-04-24:03:48:25 WARNING  [evaluator:159] Instruct model detected, but chat template not applied. Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).
2025-04-24:03:48:25 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-04-24:03:48:25 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/', 'trust_remote_code': True, 'disable_exllama': True, 'device_map': "'auto'"}
2025-04-24:03:48:25 INFO     [models.huggingface:137] Using device 'cuda:0'
2025-04-24:03:48:26 INFO     [models.huggingface:388] Model parallel was set to False.
2025-04-24:03:48:26 WARNING  [transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:62] `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24:03:48:26 WARNING  [transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:66] Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 205, in merge_quantization_configs
    loading_attr_dict = quantization_config_from_args.get_loading_attributes()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'get_loading_attributes'

  Stdout:

----------------------------------------

2025-04-24 03:48:26,769 - INFO - 
Run 10/18: Model='Phi-3-mini-4k-instruct-GPTQ', Task='mmlu_elementary_mathematics'
2025-04-24 03:48:26,769 - INFO - Executing command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks mmlu_elementary_mathematics --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Phi-3-mini-4k-instruct-GPTQ/mmlu_elementary_mathematics/results.json --limit 5
2025-04-24 03:48:32,512 - ERROR - ERROR during: Run 10/18: Model='Phi-3-mini-4k-instruct-GPTQ', Task='mmlu_elementary_mathematics'
  Command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks mmlu_elementary_mathematics --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Phi-3-mini-4k-instruct-GPTQ/mmlu_elementary_mathematics/results.json --limit 5
  Return Code: 1
  Output File Target: /workspace/results/nlp/mod/48/Phi-3-mini-4k-instruct-GPTQ/mmlu_elementary_mathematics/results.json
  Stderr:

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.5 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "/workspace/code/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 10, in <module>
    import torch
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-04-24:03:48:31 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-04-24:03:48:31 INFO     [__main__:440] Selected Tasks: ['mmlu_elementary_mathematics']
2025-04-24:03:48:31 WARNING  [evaluator:159] Instruct model detected, but chat template not applied. Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).
2025-04-24:03:48:31 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-04-24:03:48:31 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/', 'trust_remote_code': True, 'disable_exllama': True, 'device_map': "'auto'"}
2025-04-24:03:48:31 INFO     [models.huggingface:137] Using device 'cuda:0'
2025-04-24:03:48:31 INFO     [models.huggingface:388] Model parallel was set to False.
2025-04-24:03:48:31 WARNING  [transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:62] `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24:03:48:31 WARNING  [transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:66] Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 205, in merge_quantization_configs
    loading_attr_dict = quantization_config_from_args.get_loading_attributes()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'get_loading_attributes'

  Stdout:

----------------------------------------

2025-04-24 03:48:32,512 - INFO - 
Run 11/18: Model='Phi-3-mini-4k-instruct-GPTQ', Task='truthfulqa_mc2'
2025-04-24 03:48:32,512 - INFO - Executing command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks truthfulqa_mc2 --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Phi-3-mini-4k-instruct-GPTQ/truthfulqa_mc2/results.json --limit 5
2025-04-24 03:48:38,081 - ERROR - ERROR during: Run 11/18: Model='Phi-3-mini-4k-instruct-GPTQ', Task='truthfulqa_mc2'
  Command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks truthfulqa_mc2 --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Phi-3-mini-4k-instruct-GPTQ/truthfulqa_mc2/results.json --limit 5
  Return Code: 1
  Output File Target: /workspace/results/nlp/mod/48/Phi-3-mini-4k-instruct-GPTQ/truthfulqa_mc2/results.json
  Stderr:

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.5 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "/workspace/code/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 10, in <module>
    import torch
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-04-24:03:48:36 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-04-24:03:48:36 INFO     [__main__:440] Selected Tasks: ['truthfulqa_mc2']
2025-04-24:03:48:36 WARNING  [evaluator:159] Instruct model detected, but chat template not applied. Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).
2025-04-24:03:48:36 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-04-24:03:48:36 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/', 'trust_remote_code': True, 'disable_exllama': True, 'device_map': "'auto'"}
2025-04-24:03:48:37 INFO     [models.huggingface:137] Using device 'cuda:0'
2025-04-24:03:48:37 INFO     [models.huggingface:388] Model parallel was set to False.
2025-04-24:03:48:37 WARNING  [transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:62] `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24:03:48:37 WARNING  [transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:66] Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 205, in merge_quantization_configs
    loading_attr_dict = quantization_config_from_args.get_loading_attributes()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'get_loading_attributes'

  Stdout:

----------------------------------------

2025-04-24 03:48:38,082 - INFO - 
Run 12/18: Model='Phi-3-mini-4k-instruct-GPTQ', Task='gsm8k'
2025-04-24 03:48:38,082 - INFO - Executing command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks gsm8k --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Phi-3-mini-4k-instruct-GPTQ/gsm8k/results.json --limit 5
2025-04-24 03:48:43,503 - ERROR - ERROR during: Run 12/18: Model='Phi-3-mini-4k-instruct-GPTQ', Task='gsm8k'
  Command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks gsm8k --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Phi-3-mini-4k-instruct-GPTQ/gsm8k/results.json --limit 5
  Return Code: 1
  Output File Target: /workspace/results/nlp/mod/48/Phi-3-mini-4k-instruct-GPTQ/gsm8k/results.json
  Stderr:

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.5 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "/workspace/code/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 10, in <module>
    import torch
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-04-24:03:48:42 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-04-24:03:48:42 INFO     [__main__:440] Selected Tasks: ['gsm8k']
2025-04-24:03:48:42 WARNING  [evaluator:159] Instruct model detected, but chat template not applied. Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).
2025-04-24:03:48:42 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-04-24:03:48:42 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/workspace/models/Phi-3-mini-4k-instruct/quantized-gptq-4bit/', 'trust_remote_code': True, 'disable_exllama': True, 'device_map': "'auto'"}
2025-04-24:03:48:42 INFO     [models.huggingface:137] Using device 'cuda:0'
2025-04-24:03:48:42 INFO     [models.huggingface:388] Model parallel was set to False.
2025-04-24:03:48:43 WARNING  [transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:62] `flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.
2025-04-24:03:48:43 WARNING  [transformers_modules.microsoft.Phi-3-mini-4k-instruct.0a67737cc96d2554230f90338b163bc6380a2a85.modeling_phi3:66] Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 205, in merge_quantization_configs
    loading_attr_dict = quantization_config_from_args.get_loading_attributes()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'get_loading_attributes'

  Stdout:

----------------------------------------

2025-04-24 03:48:43,504 - INFO - 
Run 13/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Task='arc_challenge'
2025-04-24 03:48:43,504 - INFO - Executing command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks arc_challenge --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/arc_challenge/results.json --limit 5
2025-04-24 03:48:49,097 - ERROR - ERROR during: Run 13/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Task='arc_challenge'
  Command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks arc_challenge --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/arc_challenge/results.json --limit 5
  Return Code: 1
  Output File Target: /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/arc_challenge/results.json
  Stderr:

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.5 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "/workspace/code/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 10, in <module>
    import torch
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-04-24:03:48:47 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-04-24:03:48:47 INFO     [__main__:440] Selected Tasks: ['arc_challenge']
2025-04-24:03:48:47 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-04-24:03:48:47 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/', 'trust_remote_code': True, 'disable_exllama': True, 'device_map': "'auto'"}
2025-04-24:03:48:48 INFO     [models.huggingface:137] Using device 'cuda:0'
2025-04-24:03:48:48 INFO     [models.huggingface:388] Model parallel was set to False.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 571, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 205, in merge_quantization_configs
    loading_attr_dict = quantization_config_from_args.get_loading_attributes()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'get_loading_attributes'

  Stdout:

----------------------------------------

2025-04-24 03:48:49,098 - INFO - 
Run 14/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Task='hellaswag'
2025-04-24 03:48:49,098 - INFO - Executing command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks hellaswag --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/hellaswag/results.json --limit 5
2025-04-24 03:48:54,612 - ERROR - ERROR during: Run 14/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Task='hellaswag'
  Command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks hellaswag --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/hellaswag/results.json --limit 5
  Return Code: 1
  Output File Target: /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/hellaswag/results.json
  Stderr:

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.5 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "/workspace/code/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 10, in <module>
    import torch
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-04-24:03:48:53 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-04-24:03:48:53 INFO     [__main__:440] Selected Tasks: ['hellaswag']
2025-04-24:03:48:53 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-04-24:03:48:53 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/', 'trust_remote_code': True, 'disable_exllama': True, 'device_map': "'auto'"}
2025-04-24:03:48:53 INFO     [models.huggingface:137] Using device 'cuda:0'
2025-04-24:03:48:54 INFO     [models.huggingface:388] Model parallel was set to False.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 571, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 205, in merge_quantization_configs
    loading_attr_dict = quantization_config_from_args.get_loading_attributes()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'get_loading_attributes'

  Stdout:

----------------------------------------

2025-04-24 03:48:54,612 - INFO - 
Run 15/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Task='winogrande'
2025-04-24 03:48:54,612 - INFO - Executing command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks winogrande --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/winogrande/results.json --limit 5
2025-04-24 03:49:00,095 - ERROR - ERROR during: Run 15/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Task='winogrande'
  Command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks winogrande --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/winogrande/results.json --limit 5
  Return Code: 1
  Output File Target: /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/winogrande/results.json
  Stderr:

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.5 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "/workspace/code/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 10, in <module>
    import torch
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-04-24:03:48:58 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-04-24:03:48:58 INFO     [__main__:440] Selected Tasks: ['winogrande']
2025-04-24:03:48:58 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-04-24:03:48:58 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/', 'trust_remote_code': True, 'disable_exllama': True, 'device_map': "'auto'"}
2025-04-24:03:48:59 INFO     [models.huggingface:137] Using device 'cuda:0'
2025-04-24:03:48:59 INFO     [models.huggingface:388] Model parallel was set to False.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 571, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 205, in merge_quantization_configs
    loading_attr_dict = quantization_config_from_args.get_loading_attributes()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'get_loading_attributes'

  Stdout:

----------------------------------------

2025-04-24 03:49:00,096 - INFO - 
Run 16/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Task='mmlu_elementary_mathematics'
2025-04-24 03:49:00,096 - INFO - Executing command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks mmlu_elementary_mathematics --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/mmlu_elementary_mathematics/results.json --limit 5
2025-04-24 03:49:05,551 - ERROR - ERROR during: Run 16/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Task='mmlu_elementary_mathematics'
  Command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks mmlu_elementary_mathematics --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/mmlu_elementary_mathematics/results.json --limit 5
  Return Code: 1
  Output File Target: /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/mmlu_elementary_mathematics/results.json
  Stderr:

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.5 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "/workspace/code/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 10, in <module>
    import torch
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-04-24:03:49:04 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-04-24:03:49:04 INFO     [__main__:440] Selected Tasks: ['mmlu_elementary_mathematics']
2025-04-24:03:49:04 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-04-24:03:49:04 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/', 'trust_remote_code': True, 'disable_exllama': True, 'device_map': "'auto'"}
2025-04-24:03:49:04 INFO     [models.huggingface:137] Using device 'cuda:0'
2025-04-24:03:49:05 INFO     [models.huggingface:388] Model parallel was set to False.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 571, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 205, in merge_quantization_configs
    loading_attr_dict = quantization_config_from_args.get_loading_attributes()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'get_loading_attributes'

  Stdout:

----------------------------------------

2025-04-24 03:49:05,552 - INFO - 
Run 17/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Task='truthfulqa_mc2'
2025-04-24 03:49:05,552 - INFO - Executing command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks truthfulqa_mc2 --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/truthfulqa_mc2/results.json --limit 5
2025-04-24 03:49:11,097 - ERROR - ERROR during: Run 17/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Task='truthfulqa_mc2'
  Command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks truthfulqa_mc2 --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/truthfulqa_mc2/results.json --limit 5
  Return Code: 1
  Output File Target: /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/truthfulqa_mc2/results.json
  Stderr:

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.5 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "/workspace/code/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 10, in <module>
    import torch
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-04-24:03:49:09 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-04-24:03:49:09 INFO     [__main__:440] Selected Tasks: ['truthfulqa_mc2']
2025-04-24:03:49:09 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-04-24:03:49:09 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/', 'trust_remote_code': True, 'disable_exllama': True, 'device_map': "'auto'"}
2025-04-24:03:49:10 INFO     [models.huggingface:137] Using device 'cuda:0'
2025-04-24:03:49:10 INFO     [models.huggingface:388] Model parallel was set to False.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 571, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 205, in merge_quantization_configs
    loading_attr_dict = quantization_config_from_args.get_loading_attributes()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'get_loading_attributes'

  Stdout:

----------------------------------------

2025-04-24 03:49:11,097 - INFO - 
Run 18/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Task='gsm8k'
2025-04-24 03:49:11,097 - INFO - Executing command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks gsm8k --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/gsm8k/results.json --limit 5
2025-04-24 03:49:16,569 - ERROR - ERROR during: Run 18/18: Model='Mistral-7B-Instruct-v0.3-GPTQ', Task='gsm8k'
  Command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks gsm8k --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/gsm8k/results.json --limit 5
  Return Code: 1
  Output File Target: /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/gsm8k/results.json
  Stderr:

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.5 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "/workspace/code/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 10, in <module>
    import torch
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-04-24:03:49:15 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-04-24:03:49:15 INFO     [__main__:440] Selected Tasks: ['gsm8k']
2025-04-24:03:49:15 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-04-24:03:49:15 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/', 'trust_remote_code': True, 'disable_exllama': True, 'device_map': "'auto'"}
2025-04-24:03:49:15 INFO     [models.huggingface:137] Using device 'cuda:0'
2025-04-24:03:49:16 INFO     [models.huggingface:388] Model parallel was set to False.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 571, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 205, in merge_quantization_configs
    loading_attr_dict = quantization_config_from_args.get_loading_attributes()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'get_loading_attributes'

  Stdout:

----------------------------------------

2025-04-24 03:49:16,570 - INFO - --- LM Evaluation Harness Automation Finished (QUANTIZED MODELS) ---
8k'
  Command: /workspace/testbedvenv/bin/python -m lm_eval --model hf --model_args pretrained=/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/,trust_remote_code=True,disable_exllama=True,device_map='auto' --tasks gsm8k --num_fewshot 0 --batch_size auto --device cuda:0 --output_path /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/gsm8k/results.json --limit 5
  Return Code: 1
  Output File Target: /workspace/results/nlp/mod/48/Mistral-7B-Instruct-v0.3-GPTQ/gsm8k/results.json
  Stderr:

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.2.5 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "<frozen runpy>", line 189, in _run_module_as_main
  File "<frozen runpy>", line 148, in _get_module_details
  File "<frozen runpy>", line 112, in _get_module_details
  File "/workspace/code/lm-evaluation-harness/lm_eval/__init__.py", line 4, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 10, in <module>
    import torch
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/__init__.py", line 1477, in <module>
    from .functional import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/functional.py", line 9, in <module>
    import torch.nn.functional as F
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/__init__.py", line 1, in <module>
    from .modules import *  # noqa: F403
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/__init__.py", line 35, in <module>
    from .transformer import TransformerEncoder, TransformerDecoder, \
  File "/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py", line 20, in <module>
    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
/workspace/testbedvenv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),
2025-04-24:03:49:15 WARNING  [__main__:368]  --limit SHOULD ONLY BE USED FOR TESTING.REAL METRICS SHOULD NOT BE COMPUTED USING LIMIT.
2025-04-24:03:49:15 INFO     [__main__:440] Selected Tasks: ['gsm8k']
2025-04-24:03:49:15 INFO     [evaluator:185] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234
2025-04-24:03:49:15 INFO     [evaluator:223] Initializing hf model, with arguments: {'pretrained': '/workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit/', 'trust_remote_code': True, 'disable_exllama': True, 'device_map': "'auto'"}
2025-04-24:03:49:15 INFO     [models.huggingface:137] Using device 'cuda:0'
2025-04-24:03:49:16 INFO     [models.huggingface:388] Model parallel was set to False.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 530, in <module>
    cli_evaluate()
  File "/workspace/code/lm-evaluation-harness/lm_eval/__main__.py", line 449, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/utils.py", line 439, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/evaluator.py", line 226, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/api/model.py", line 151, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 193, in __init__
    self._create_model(
  File "/workspace/code/lm-evaluation-harness/lm_eval/models/huggingface.py", line 594, in _create_model
    self._model = self.AUTO_MODEL_CLASS.from_pretrained(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 571, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 279, in _wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/modeling_utils.py", line 4214, in from_pretrained
    config.quantization_config = AutoHfQuantizer.merge_quantization_configs(
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/testbedvenv/lib/python3.11/site-packages/transformers/quantizers/auto.py", line 205, in merge_quantization_configs
    loading_attr_dict = quantization_config_from_args.get_loading_attributes()
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'dict' object has no attribute 'get_loading_attributes'

  Stdout:

----------------------------------------
