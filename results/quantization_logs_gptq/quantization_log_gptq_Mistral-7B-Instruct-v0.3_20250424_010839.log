2025-04-24 01:08:39,358 - INFO - [<module>] - --- System Information ---
2025-04-24 01:08:39,358 - INFO - [<module>] - Script: AutoGPTQ Quantization
2025-04-24 01:08:39,358 - INFO - [<module>] - Python Version: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]
2025-04-24 01:08:39,358 - INFO - [<module>] - Torch Version: 2.7.0+cu126
2025-04-24 01:08:39,359 - INFO - [<module>] - Transformers Version: 4.52.0.dev0
2025-04-24 01:08:39,360 - INFO - [<module>] - Optimum Version: 1.24.0
2025-04-24 01:08:39,360 - INFO - [<module>] - Accelerate Version: 1.6.0
2025-04-24 01:08:39,360 - INFO - [<module>] - Bitsandbytes Version: 0.45.5
2025-04-24 01:08:39,360 - INFO - [<module>] - Datasets Version: 3.5.0
2025-04-24 01:08:39,361 - INFO - [<module>] - CPU Count: 192
2025-04-24 01:08:39,361 - INFO - [<module>] - Total RAM: 2015.55 GB
2025-04-24 01:08:39,380 - INFO - [<module>] - GPU: NVIDIA H200
2025-04-24 01:08:39,381 - INFO - [<module>] - Total VRAM: 139.72 GB
2025-04-24 01:08:39,381 - INFO - [<module>] - CUDA Version: 12.6
2025-04-24 01:08:39,381 - INFO - [<module>] - Using Device: cuda:0
2025-04-24 01:08:39,381 - INFO - [<module>] - --------------------------
2025-04-24 01:08:39,381 - INFO - [run_quantization] - Starting AutoGPTQ quantization for model: mistralai/Mistral-7B-Instruct-v0.3
2025-04-24 01:08:39,381 - INFO - [run_quantization] - Quantizing to 4-bit precision.
2025-04-24 01:08:39,381 - INFO - [run_quantization] - Output directory: /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit
2025-04-24 01:08:39,381 - INFO - [run_quantization] - Loading tokenizer for mistralai/Mistral-7B-Instruct-v0.3...
2025-04-24 01:08:39,831 - INFO - [run_quantization] - Set tokenizer pad_token to eos_token.
2025-04-24 01:08:39,831 - INFO - [run_quantization] - Tokenizer loaded.
2025-04-24 01:08:39,831 - INFO - [run_quantization] - Loading calibration dataset from: /workspace/calibration_data/sevenllm_instruct_subset_manual/
2025-04-24 01:08:39,842 - INFO - [run_quantization] - Loaded and selected 128 calibration samples.
2025-04-24 01:08:39,842 - INFO - [run_quantization] - Prepared calibration data as a list of 128 strings from column 'instruction'.
2025-04-24 01:08:39,842 - INFO - [run_quantization] - Defining GPTQ configuration...
2025-04-24 01:08:39,842 - INFO - [run_quantization] - GPTQ Config: bits=4, group_size=128, damp_percent=0.01, desc_act=False
2025-04-24 01:08:39,842 - INFO - [run_quantization] - Loading model mistralai/Mistral-7B-Instruct-v0.3 and starting quantization (this may take a while)...
2025-04-24 01:08:40,919 - WARNING - [<module>] - CUDA extension not installed.
2025-04-24 01:08:40,920 - WARNING - [<module>] - CUDA extension not installed.
2025-04-24 01:08:41,546 - INFO - [get_balanced_memory] - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
2025-04-24 01:08:44,050 - INFO - [quantize_model] - Start quantizing block model.layers 1/32
2025-04-24 01:08:44,050 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:08:44,422 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 1/32...
2025-04-24 01:08:45,386 - INFO - [fasterquant] - duration: 0.9633142948150635
2025-04-24 01:08:45,393 - INFO - [fasterquant] - avg loss: 0.016444208100438118
2025-04-24 01:08:45,468 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 1/32...
2025-04-24 01:08:46,289 - INFO - [fasterquant] - duration: 0.8212313652038574
2025-04-24 01:08:46,289 - INFO - [fasterquant] - avg loss: 0.006823080591857433
2025-04-24 01:08:46,363 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 1/32...
2025-04-24 01:08:47,179 - INFO - [fasterquant] - duration: 0.8159263134002686
2025-04-24 01:08:47,179 - INFO - [fasterquant] - avg loss: 0.0002744063385762274
2025-04-24 01:08:47,252 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 1/32...
2025-04-24 01:08:48,069 - INFO - [fasterquant] - duration: 0.8177089691162109
2025-04-24 01:08:48,070 - INFO - [fasterquant] - avg loss: 7.603771337016951e-07
2025-04-24 01:08:48,144 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 1/32...
2025-04-24 01:08:48,968 - INFO - [fasterquant] - duration: 0.8238909244537354
2025-04-24 01:08:48,968 - INFO - [fasterquant] - avg loss: 0.0035979244858026505
2025-04-24 01:08:49,043 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 1/32...
2025-04-24 01:08:49,866 - INFO - [fasterquant] - duration: 0.8233814239501953
2025-04-24 01:08:49,867 - INFO - [fasterquant] - avg loss: 0.0031071114353835583
2025-04-24 01:08:50,066 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 1/32...
2025-04-24 01:08:53,156 - INFO - [fasterquant] - duration: 3.0903713703155518
2025-04-24 01:08:53,156 - INFO - [fasterquant] - avg loss: 1.3442340787150897e-05
2025-04-24 01:08:53,229 - INFO - [quantize_model] - Start quantizing block model.layers 2/32
2025-04-24 01:08:53,229 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:08:53,302 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 2/32...
2025-04-24 01:08:54,132 - INFO - [fasterquant] - duration: 0.8296482563018799
2025-04-24 01:08:54,132 - INFO - [fasterquant] - avg loss: 0.029611069709062576
2025-04-24 01:08:54,205 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 2/32...
2025-04-24 01:08:55,027 - INFO - [fasterquant] - duration: 0.8215866088867188
2025-04-24 01:08:55,027 - INFO - [fasterquant] - avg loss: 0.01514369249343872
2025-04-24 01:08:55,100 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 2/32...
2025-04-24 01:08:55,921 - INFO - [fasterquant] - duration: 0.8211228847503662
2025-04-24 01:08:55,921 - INFO - [fasterquant] - avg loss: 0.001885749283246696
2025-04-24 01:08:55,994 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 2/32...
2025-04-24 01:08:56,815 - INFO - [fasterquant] - duration: 0.8203721046447754
2025-04-24 01:08:56,815 - INFO - [fasterquant] - avg loss: 1.6145336303452495e-06
2025-04-24 01:08:56,888 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 2/32...
2025-04-24 01:08:57,714 - INFO - [fasterquant] - duration: 0.82574462890625
2025-04-24 01:08:57,714 - INFO - [fasterquant] - avg loss: 0.011751296930015087
2025-04-24 01:08:57,788 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 2/32...
2025-04-24 01:08:58,614 - INFO - [fasterquant] - duration: 0.8260595798492432
2025-04-24 01:08:58,614 - INFO - [fasterquant] - avg loss: 0.010309010744094849
2025-04-24 01:08:58,812 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 2/32...
2025-04-24 01:09:01,920 - INFO - [fasterquant] - duration: 3.108072280883789
2025-04-24 01:09:01,921 - INFO - [fasterquant] - avg loss: 0.024393636733293533
2025-04-24 01:09:01,988 - INFO - [quantize_model] - Start quantizing block model.layers 3/32
2025-04-24 01:09:01,988 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:09:02,062 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 3/32...
2025-04-24 01:09:02,879 - INFO - [fasterquant] - duration: 0.8173258304595947
2025-04-24 01:09:02,880 - INFO - [fasterquant] - avg loss: 0.12382888793945312
2025-04-24 01:09:02,952 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 3/32...
2025-04-24 01:09:03,774 - INFO - [fasterquant] - duration: 0.8215334415435791
2025-04-24 01:09:03,774 - INFO - [fasterquant] - avg loss: 0.0591854564845562
2025-04-24 01:09:03,847 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 3/32...
2025-04-24 01:09:04,671 - INFO - [fasterquant] - duration: 0.8236451148986816
2025-04-24 01:09:04,671 - INFO - [fasterquant] - avg loss: 0.0077815125696361065
2025-04-24 01:09:04,744 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 3/32...
2025-04-24 01:09:05,566 - INFO - [fasterquant] - duration: 0.8209233283996582
2025-04-24 01:09:05,566 - INFO - [fasterquant] - avg loss: 1.8774766203932813e-06
2025-04-24 01:09:05,639 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 3/32...
2025-04-24 01:09:06,466 - INFO - [fasterquant] - duration: 0.8260464668273926
2025-04-24 01:09:06,466 - INFO - [fasterquant] - avg loss: 0.020055893808603287
2025-04-24 01:09:06,539 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 3/32...
2025-04-24 01:09:07,367 - INFO - [fasterquant] - duration: 0.8277037143707275
2025-04-24 01:09:07,368 - INFO - [fasterquant] - avg loss: 0.017568618059158325
2025-04-24 01:09:07,565 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 3/32...
2025-04-24 01:09:10,663 - INFO - [fasterquant] - duration: 3.0983574390411377
2025-04-24 01:09:10,664 - INFO - [fasterquant] - avg loss: 5.217689704295481e-06
2025-04-24 01:09:10,731 - INFO - [quantize_model] - Start quantizing block model.layers 4/32
2025-04-24 01:09:10,731 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:09:10,804 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 4/32...
2025-04-24 01:09:11,623 - INFO - [fasterquant] - duration: 0.8190710544586182
2025-04-24 01:09:11,623 - INFO - [fasterquant] - avg loss: 0.06108223274350166
2025-04-24 01:09:11,696 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 4/32...
2025-04-24 01:09:12,516 - INFO - [fasterquant] - duration: 0.8198659420013428
2025-04-24 01:09:12,517 - INFO - [fasterquant] - avg loss: 0.02961510419845581
2025-04-24 01:09:12,589 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 4/32...
2025-04-24 01:09:13,409 - INFO - [fasterquant] - duration: 0.8188955783843994
2025-04-24 01:09:13,409 - INFO - [fasterquant] - avg loss: 0.0047855135053396225
2025-04-24 01:09:13,482 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 4/32...
2025-04-24 01:09:14,304 - INFO - [fasterquant] - duration: 0.8214378356933594
2025-04-24 01:09:14,304 - INFO - [fasterquant] - avg loss: 4.567924406728707e-06
2025-04-24 01:09:14,378 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 4/32...
2025-04-24 01:09:15,205 - INFO - [fasterquant] - duration: 0.8270204067230225
2025-04-24 01:09:15,205 - INFO - [fasterquant] - avg loss: 0.03282761573791504
2025-04-24 01:09:15,279 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 4/32...
2025-04-24 01:09:16,105 - INFO - [fasterquant] - duration: 0.826728105545044
2025-04-24 01:09:16,106 - INFO - [fasterquant] - avg loss: 0.028337759897112846
2025-04-24 01:09:16,303 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 4/32...
2025-04-24 01:09:19,399 - INFO - [fasterquant] - duration: 3.095684051513672
2025-04-24 01:09:19,399 - INFO - [fasterquant] - avg loss: 1.0496531103854068e-05
2025-04-24 01:09:19,465 - INFO - [quantize_model] - Start quantizing block model.layers 5/32
2025-04-24 01:09:19,465 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:09:19,538 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 5/32...
2025-04-24 01:09:20,356 - INFO - [fasterquant] - duration: 0.8181159496307373
2025-04-24 01:09:20,357 - INFO - [fasterquant] - avg loss: 0.08568917214870453
2025-04-24 01:09:20,430 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 5/32...
2025-04-24 01:09:21,251 - INFO - [fasterquant] - duration: 0.8212757110595703
2025-04-24 01:09:21,252 - INFO - [fasterquant] - avg loss: 0.03753376379609108
2025-04-24 01:09:21,325 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 5/32...
2025-04-24 01:09:22,148 - INFO - [fasterquant] - duration: 0.8233156204223633
2025-04-24 01:09:22,149 - INFO - [fasterquant] - avg loss: 0.007270763628184795
2025-04-24 01:09:22,222 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 5/32...
2025-04-24 01:09:23,044 - INFO - [fasterquant] - duration: 0.8218934535980225
2025-04-24 01:09:23,045 - INFO - [fasterquant] - avg loss: 4.630168405128643e-06
2025-04-24 01:09:23,119 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 5/32...
2025-04-24 01:09:23,945 - INFO - [fasterquant] - duration: 0.8263659477233887
2025-04-24 01:09:23,946 - INFO - [fasterquant] - avg loss: 0.04707137495279312
2025-04-24 01:09:24,019 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 5/32...
2025-04-24 01:09:24,846 - INFO - [fasterquant] - duration: 0.8268518447875977
2025-04-24 01:09:24,847 - INFO - [fasterquant] - avg loss: 0.038235656917095184
2025-04-24 01:09:25,044 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 5/32...
2025-04-24 01:09:28,144 - INFO - [fasterquant] - duration: 3.100123882293701
2025-04-24 01:09:28,144 - INFO - [fasterquant] - avg loss: 1.6123231034725904e-05
2025-04-24 01:09:28,211 - INFO - [quantize_model] - Start quantizing block model.layers 6/32
2025-04-24 01:09:28,212 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:09:28,284 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 6/32...
2025-04-24 01:09:29,108 - INFO - [fasterquant] - duration: 0.8236000537872314
2025-04-24 01:09:29,108 - INFO - [fasterquant] - avg loss: 0.0960499718785286
2025-04-24 01:09:29,182 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 6/32...
2025-04-24 01:09:30,008 - INFO - [fasterquant] - duration: 0.8262646198272705
2025-04-24 01:09:30,009 - INFO - [fasterquant] - avg loss: 0.04237010329961777
2025-04-24 01:09:30,082 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 6/32...
2025-04-24 01:09:30,902 - INFO - [fasterquant] - duration: 0.8199982643127441
2025-04-24 01:09:30,902 - INFO - [fasterquant] - avg loss: 0.007001410238444805
2025-04-24 01:09:30,975 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 6/32...
2025-04-24 01:09:31,795 - INFO - [fasterquant] - duration: 0.8197348117828369
2025-04-24 01:09:31,796 - INFO - [fasterquant] - avg loss: 1.0969304639729671e-05
2025-04-24 01:09:31,870 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 6/32...
2025-04-24 01:09:32,696 - INFO - [fasterquant] - duration: 0.8266081809997559
2025-04-24 01:09:32,697 - INFO - [fasterquant] - avg loss: 0.06651116162538528
2025-04-24 01:09:32,770 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 6/32...
2025-04-24 01:09:33,598 - INFO - [fasterquant] - duration: 0.827927827835083
2025-04-24 01:09:33,599 - INFO - [fasterquant] - avg loss: 0.05061464011669159
2025-04-24 01:09:33,796 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 6/32...
2025-04-24 01:09:36,905 - INFO - [fasterquant] - duration: 3.109370231628418
2025-04-24 01:09:36,905 - INFO - [fasterquant] - avg loss: 2.6988640456693247e-05
2025-04-24 01:09:36,971 - INFO - [quantize_model] - Start quantizing block model.layers 7/32
2025-04-24 01:09:36,972 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:09:37,044 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 7/32...
2025-04-24 01:09:37,868 - INFO - [fasterquant] - duration: 0.8237442970275879
2025-04-24 01:09:37,869 - INFO - [fasterquant] - avg loss: 0.09357799589633942
2025-04-24 01:09:37,942 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 7/32...
2025-04-24 01:09:38,764 - INFO - [fasterquant] - duration: 0.8217673301696777
2025-04-24 01:09:38,764 - INFO - [fasterquant] - avg loss: 0.043876174837350845
2025-04-24 01:09:38,837 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 7/32...
2025-04-24 01:09:39,663 - INFO - [fasterquant] - duration: 0.8261210918426514
2025-04-24 01:09:39,663 - INFO - [fasterquant] - avg loss: 0.007447943091392517
2025-04-24 01:09:39,736 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 7/32...
2025-04-24 01:09:40,555 - INFO - [fasterquant] - duration: 0.8183367252349854
2025-04-24 01:09:40,555 - INFO - [fasterquant] - avg loss: 2.1614774595946074e-05
2025-04-24 01:09:40,628 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 7/32...
2025-04-24 01:09:41,457 - INFO - [fasterquant] - duration: 0.8290128707885742
2025-04-24 01:09:41,458 - INFO - [fasterquant] - avg loss: 0.0790039598941803
2025-04-24 01:09:41,531 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 7/32...
2025-04-24 01:09:42,362 - INFO - [fasterquant] - duration: 0.830460786819458
2025-04-24 01:09:42,362 - INFO - [fasterquant] - avg loss: 0.061490319669246674
2025-04-24 01:09:42,576 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 7/32...
2025-04-24 01:09:45,679 - INFO - [fasterquant] - duration: 3.1032309532165527
2025-04-24 01:09:45,680 - INFO - [fasterquant] - avg loss: 4.102477032574825e-05
2025-04-24 01:09:45,746 - INFO - [quantize_model] - Start quantizing block model.layers 8/32
2025-04-24 01:09:45,746 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:09:45,818 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 8/32...
2025-04-24 01:09:46,644 - INFO - [fasterquant] - duration: 0.8252077102661133
2025-04-24 01:09:46,644 - INFO - [fasterquant] - avg loss: 0.11455607414245605
2025-04-24 01:09:46,718 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 8/32...
2025-04-24 01:09:47,545 - INFO - [fasterquant] - duration: 0.8270962238311768
2025-04-24 01:09:47,546 - INFO - [fasterquant] - avg loss: 0.05593343451619148
2025-04-24 01:09:47,619 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 8/32...
2025-04-24 01:09:48,445 - INFO - [fasterquant] - duration: 0.8258762359619141
2025-04-24 01:09:48,445 - INFO - [fasterquant] - avg loss: 0.009812603704631329
2025-04-24 01:09:48,518 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 8/32...
2025-04-24 01:09:49,338 - INFO - [fasterquant] - duration: 0.8195970058441162
2025-04-24 01:09:49,339 - INFO - [fasterquant] - avg loss: 2.4311641027452424e-05
2025-04-24 01:09:49,412 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 8/32...
2025-04-24 01:09:50,242 - INFO - [fasterquant] - duration: 0.8302288055419922
2025-04-24 01:09:50,243 - INFO - [fasterquant] - avg loss: 0.09594139456748962
2025-04-24 01:09:50,316 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 8/32...
2025-04-24 01:09:51,142 - INFO - [fasterquant] - duration: 0.8257791996002197
2025-04-24 01:09:51,143 - INFO - [fasterquant] - avg loss: 0.07287948578596115
2025-04-24 01:09:51,340 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 8/32...
2025-04-24 01:09:54,445 - INFO - [fasterquant] - duration: 3.105138063430786
2025-04-24 01:09:54,445 - INFO - [fasterquant] - avg loss: 5.1148068450856954e-05
2025-04-24 01:09:54,512 - INFO - [quantize_model] - Start quantizing block model.layers 9/32
2025-04-24 01:09:54,512 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:09:54,585 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 9/32...
2025-04-24 01:09:55,409 - INFO - [fasterquant] - duration: 0.8242239952087402
2025-04-24 01:09:55,410 - INFO - [fasterquant] - avg loss: 0.09110482037067413
2025-04-24 01:09:55,483 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 9/32...
2025-04-24 01:09:56,307 - INFO - [fasterquant] - duration: 0.8242461681365967
2025-04-24 01:09:56,308 - INFO - [fasterquant] - avg loss: 0.04195432364940643
2025-04-24 01:09:56,380 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 9/32...
2025-04-24 01:09:57,206 - INFO - [fasterquant] - duration: 0.8254227638244629
2025-04-24 01:09:57,207 - INFO - [fasterquant] - avg loss: 0.008830208331346512
2025-04-24 01:09:57,280 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 9/32...
2025-04-24 01:09:58,105 - INFO - [fasterquant] - duration: 0.8241584300994873
2025-04-24 01:09:58,105 - INFO - [fasterquant] - avg loss: 3.94034905184526e-05
2025-04-24 01:09:58,178 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 9/32...
2025-04-24 01:09:59,008 - INFO - [fasterquant] - duration: 0.8292596340179443
2025-04-24 01:09:59,008 - INFO - [fasterquant] - avg loss: 0.10337647795677185
2025-04-24 01:09:59,081 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 9/32...
2025-04-24 01:09:59,912 - INFO - [fasterquant] - duration: 0.8312404155731201
2025-04-24 01:09:59,913 - INFO - [fasterquant] - avg loss: 0.08007258176803589
2025-04-24 01:10:00,110 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 9/32...
2025-04-24 01:10:03,217 - INFO - [fasterquant] - duration: 3.1064565181732178
2025-04-24 01:10:03,217 - INFO - [fasterquant] - avg loss: 6.938219303265214e-05
2025-04-24 01:10:03,283 - INFO - [quantize_model] - Start quantizing block model.layers 10/32
2025-04-24 01:10:03,284 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:10:03,356 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 10/32...
2025-04-24 01:10:04,177 - INFO - [fasterquant] - duration: 0.8206639289855957
2025-04-24 01:10:04,177 - INFO - [fasterquant] - avg loss: 0.11785729229450226
2025-04-24 01:10:04,251 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 10/32...
2025-04-24 01:10:05,077 - INFO - [fasterquant] - duration: 0.8262600898742676
2025-04-24 01:10:05,078 - INFO - [fasterquant] - avg loss: 0.055954016745090485
2025-04-24 01:10:05,151 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 10/32...
2025-04-24 01:10:05,976 - INFO - [fasterquant] - duration: 0.8251392841339111
2025-04-24 01:10:05,977 - INFO - [fasterquant] - avg loss: 0.009911014698445797
2025-04-24 01:10:06,049 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 10/32...
2025-04-24 01:10:06,873 - INFO - [fasterquant] - duration: 0.8230609893798828
2025-04-24 01:10:06,873 - INFO - [fasterquant] - avg loss: 3.864169411826879e-05
2025-04-24 01:10:06,946 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 10/32...
2025-04-24 01:10:07,774 - INFO - [fasterquant] - duration: 0.8273983001708984
2025-04-24 01:10:07,774 - INFO - [fasterquant] - avg loss: 0.10682281851768494
2025-04-24 01:10:07,848 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 10/32...
2025-04-24 01:10:08,677 - INFO - [fasterquant] - duration: 0.8288426399230957
2025-04-24 01:10:08,677 - INFO - [fasterquant] - avg loss: 0.08544240891933441
2025-04-24 01:10:08,874 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 10/32...
2025-04-24 01:10:11,979 - INFO - [fasterquant] - duration: 3.104701280593872
2025-04-24 01:10:11,980 - INFO - [fasterquant] - avg loss: 7.653015927644446e-05
2025-04-24 01:10:12,046 - INFO - [quantize_model] - Start quantizing block model.layers 11/32
2025-04-24 01:10:12,046 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:10:12,119 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 11/32...
2025-04-24 01:10:12,937 - INFO - [fasterquant] - duration: 0.8184599876403809
2025-04-24 01:10:12,938 - INFO - [fasterquant] - avg loss: 0.10805574059486389
2025-04-24 01:10:13,011 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 11/32...
2025-04-24 01:10:13,835 - INFO - [fasterquant] - duration: 0.8245675563812256
2025-04-24 01:10:13,836 - INFO - [fasterquant] - avg loss: 0.05193088576197624
2025-04-24 01:10:13,909 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 11/32...
2025-04-24 01:10:14,732 - INFO - [fasterquant] - duration: 0.8230361938476562
2025-04-24 01:10:14,732 - INFO - [fasterquant] - avg loss: 0.008966075256466866
2025-04-24 01:10:14,805 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 11/32...
2025-04-24 01:10:15,627 - INFO - [fasterquant] - duration: 0.8223841190338135
2025-04-24 01:10:15,628 - INFO - [fasterquant] - avg loss: 6.339180254144594e-05
2025-04-24 01:10:15,701 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 11/32...
2025-04-24 01:10:16,534 - INFO - [fasterquant] - duration: 0.832831859588623
2025-04-24 01:10:16,534 - INFO - [fasterquant] - avg loss: 0.11490878462791443
2025-04-24 01:10:16,609 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 11/32...
2025-04-24 01:10:17,440 - INFO - [fasterquant] - duration: 0.8311882019042969
2025-04-24 01:10:17,441 - INFO - [fasterquant] - avg loss: 0.09442739188671112
2025-04-24 01:10:17,638 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 11/32...
2025-04-24 01:10:20,743 - INFO - [fasterquant] - duration: 3.104957103729248
2025-04-24 01:10:20,743 - INFO - [fasterquant] - avg loss: 8.895478094927967e-05
2025-04-24 01:10:20,809 - INFO - [quantize_model] - Start quantizing block model.layers 12/32
2025-04-24 01:10:20,809 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:10:20,882 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 12/32...
2025-04-24 01:10:21,705 - INFO - [fasterquant] - duration: 0.8231446743011475
2025-04-24 01:10:21,706 - INFO - [fasterquant] - avg loss: 0.1256960928440094
2025-04-24 01:10:21,779 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 12/32...
2025-04-24 01:10:22,600 - INFO - [fasterquant] - duration: 0.8208189010620117
2025-04-24 01:10:22,601 - INFO - [fasterquant] - avg loss: 0.058591436594724655
2025-04-24 01:10:22,674 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 12/32...
2025-04-24 01:10:23,497 - INFO - [fasterquant] - duration: 0.8223183155059814
2025-04-24 01:10:23,497 - INFO - [fasterquant] - avg loss: 0.012902363203465939
2025-04-24 01:10:23,571 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 12/32...
2025-04-24 01:10:24,392 - INFO - [fasterquant] - duration: 0.8210544586181641
2025-04-24 01:10:24,392 - INFO - [fasterquant] - avg loss: 9.310964378528297e-05
2025-04-24 01:10:24,467 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 12/32...
2025-04-24 01:10:25,295 - INFO - [fasterquant] - duration: 0.828101634979248
2025-04-24 01:10:25,296 - INFO - [fasterquant] - avg loss: 0.12557023763656616
2025-04-24 01:10:25,369 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 12/32...
2025-04-24 01:10:26,198 - INFO - [fasterquant] - duration: 0.8286497592926025
2025-04-24 01:10:26,199 - INFO - [fasterquant] - avg loss: 0.1044103130698204
2025-04-24 01:10:26,396 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 12/32...
2025-04-24 01:10:29,512 - INFO - [fasterquant] - duration: 3.115813732147217
2025-04-24 01:10:29,512 - INFO - [fasterquant] - avg loss: 9.957248403225094e-05
2025-04-24 01:10:29,580 - INFO - [quantize_model] - Start quantizing block model.layers 13/32
2025-04-24 01:10:29,580 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:10:29,653 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 13/32...
2025-04-24 01:10:30,477 - INFO - [fasterquant] - duration: 0.8238234519958496
2025-04-24 01:10:30,478 - INFO - [fasterquant] - avg loss: 0.1719183623790741
2025-04-24 01:10:30,551 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 13/32...
2025-04-24 01:10:31,372 - INFO - [fasterquant] - duration: 0.8201329708099365
2025-04-24 01:10:31,372 - INFO - [fasterquant] - avg loss: 0.07757456600666046
2025-04-24 01:10:31,446 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 13/32...
2025-04-24 01:10:32,266 - INFO - [fasterquant] - duration: 0.8200936317443848
2025-04-24 01:10:32,266 - INFO - [fasterquant] - avg loss: 0.014765556901693344
2025-04-24 01:10:32,339 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 13/32...
2025-04-24 01:10:33,162 - INFO - [fasterquant] - duration: 0.8222761154174805
2025-04-24 01:10:33,162 - INFO - [fasterquant] - avg loss: 8.603176684118807e-05
2025-04-24 01:10:33,236 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 13/32...
2025-04-24 01:10:34,063 - INFO - [fasterquant] - duration: 0.8267111778259277
2025-04-24 01:10:34,064 - INFO - [fasterquant] - avg loss: 0.13630300760269165
2025-04-24 01:10:34,137 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 13/32...
2025-04-24 01:10:34,965 - INFO - [fasterquant] - duration: 0.8273370265960693
2025-04-24 01:10:34,965 - INFO - [fasterquant] - avg loss: 0.11620800197124481
2025-04-24 01:10:35,162 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 13/32...
2025-04-24 01:10:38,278 - INFO - [fasterquant] - duration: 3.1160526275634766
2025-04-24 01:10:38,279 - INFO - [fasterquant] - avg loss: 0.00011519859981490299
2025-04-24 01:10:38,346 - INFO - [quantize_model] - Start quantizing block model.layers 14/32
2025-04-24 01:10:38,346 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:10:38,418 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 14/32...
2025-04-24 01:10:39,238 - INFO - [fasterquant] - duration: 0.8198683261871338
2025-04-24 01:10:39,239 - INFO - [fasterquant] - avg loss: 0.12703338265419006
2025-04-24 01:10:39,312 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 14/32...
2025-04-24 01:10:40,133 - INFO - [fasterquant] - duration: 0.8206560611724854
2025-04-24 01:10:40,133 - INFO - [fasterquant] - avg loss: 0.06262190639972687
2025-04-24 01:10:40,206 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 14/32...
2025-04-24 01:10:41,027 - INFO - [fasterquant] - duration: 0.8203661441802979
2025-04-24 01:10:41,027 - INFO - [fasterquant] - avg loss: 0.013361288234591484
2025-04-24 01:10:41,100 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 14/32...
2025-04-24 01:10:41,922 - INFO - [fasterquant] - duration: 0.821854829788208
2025-04-24 01:10:41,923 - INFO - [fasterquant] - avg loss: 0.00011670155072351918
2025-04-24 01:10:41,996 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 14/32...
2025-04-24 01:10:42,827 - INFO - [fasterquant] - duration: 0.830742597579956
2025-04-24 01:10:42,827 - INFO - [fasterquant] - avg loss: 0.15147241950035095
2025-04-24 01:10:42,901 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 14/32...
2025-04-24 01:10:43,730 - INFO - [fasterquant] - duration: 0.82863450050354
2025-04-24 01:10:43,731 - INFO - [fasterquant] - avg loss: 0.13237318396568298
2025-04-24 01:10:43,928 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 14/32...
2025-04-24 01:10:47,039 - INFO - [fasterquant] - duration: 3.1106128692626953
2025-04-24 01:10:47,039 - INFO - [fasterquant] - avg loss: 0.0001418532629031688
2025-04-24 01:10:47,105 - INFO - [quantize_model] - Start quantizing block model.layers 15/32
2025-04-24 01:10:47,106 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:10:47,178 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 15/32...
2025-04-24 01:10:47,997 - INFO - [fasterquant] - duration: 0.819286584854126
2025-04-24 01:10:47,998 - INFO - [fasterquant] - avg loss: 0.14281803369522095
2025-04-24 01:10:48,071 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 15/32...
2025-04-24 01:10:48,893 - INFO - [fasterquant] - duration: 0.8218457698822021
2025-04-24 01:10:48,893 - INFO - [fasterquant] - avg loss: 0.06277543306350708
2025-04-24 01:10:48,966 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 15/32...
2025-04-24 01:10:49,788 - INFO - [fasterquant] - duration: 0.8213326930999756
2025-04-24 01:10:49,788 - INFO - [fasterquant] - avg loss: 0.021311555057764053
2025-04-24 01:10:49,861 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 15/32...
2025-04-24 01:10:50,684 - INFO - [fasterquant] - duration: 0.8224856853485107
2025-04-24 01:10:50,685 - INFO - [fasterquant] - avg loss: 0.00010692779324017465
2025-04-24 01:10:50,758 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 15/32...
2025-04-24 01:10:51,586 - INFO - [fasterquant] - duration: 0.8283395767211914
2025-04-24 01:10:51,587 - INFO - [fasterquant] - avg loss: 0.17133420705795288
2025-04-24 01:10:51,660 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 15/32...
2025-04-24 01:10:52,488 - INFO - [fasterquant] - duration: 0.8285164833068848
2025-04-24 01:10:52,489 - INFO - [fasterquant] - avg loss: 0.1484970599412918
2025-04-24 01:10:52,686 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 15/32...
2025-04-24 01:10:55,799 - INFO - [fasterquant] - duration: 3.1126580238342285
2025-04-24 01:10:55,799 - INFO - [fasterquant] - avg loss: 0.0001927115226862952
2025-04-24 01:10:55,866 - INFO - [quantize_model] - Start quantizing block model.layers 16/32
2025-04-24 01:10:55,866 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:10:55,938 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 16/32...
2025-04-24 01:10:56,760 - INFO - [fasterquant] - duration: 0.8216030597686768
2025-04-24 01:10:56,761 - INFO - [fasterquant] - avg loss: 0.17458707094192505
2025-04-24 01:10:56,834 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 16/32...
2025-04-24 01:10:57,654 - INFO - [fasterquant] - duration: 0.8201870918273926
2025-04-24 01:10:57,655 - INFO - [fasterquant] - avg loss: 0.0792563408613205
2025-04-24 01:10:57,727 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 16/32...
2025-04-24 01:10:58,549 - INFO - [fasterquant] - duration: 0.8213274478912354
2025-04-24 01:10:58,549 - INFO - [fasterquant] - avg loss: 0.023384641855955124
2025-04-24 01:10:58,622 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 16/32...
2025-04-24 01:10:59,442 - INFO - [fasterquant] - duration: 0.8198757171630859
2025-04-24 01:10:59,443 - INFO - [fasterquant] - avg loss: 0.00013542891247197986
2025-04-24 01:10:59,516 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 16/32...
2025-04-24 01:11:00,344 - INFO - [fasterquant] - duration: 0.8282384872436523
2025-04-24 01:11:00,345 - INFO - [fasterquant] - avg loss: 0.19301487505435944
2025-04-24 01:11:00,418 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 16/32...
2025-04-24 01:11:01,248 - INFO - [fasterquant] - duration: 0.8300657272338867
2025-04-24 01:11:01,249 - INFO - [fasterquant] - avg loss: 0.16158565878868103
2025-04-24 01:11:01,446 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 16/32...
2025-04-24 01:11:04,562 - INFO - [fasterquant] - duration: 3.1162850856781006
2025-04-24 01:11:04,563 - INFO - [fasterquant] - avg loss: 0.00023846091062296182
2025-04-24 01:11:04,628 - INFO - [quantize_model] - Start quantizing block model.layers 17/32
2025-04-24 01:11:04,629 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:11:04,701 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 17/32...
2025-04-24 01:11:05,525 - INFO - [fasterquant] - duration: 0.8236267566680908
2025-04-24 01:11:05,526 - INFO - [fasterquant] - avg loss: 0.1526256948709488
2025-04-24 01:11:05,599 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 17/32...
2025-04-24 01:11:06,423 - INFO - [fasterquant] - duration: 0.8238224983215332
2025-04-24 01:11:06,423 - INFO - [fasterquant] - avg loss: 0.07199029624462128
2025-04-24 01:11:06,497 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 17/32...
2025-04-24 01:11:07,318 - INFO - [fasterquant] - duration: 0.8216209411621094
2025-04-24 01:11:07,319 - INFO - [fasterquant] - avg loss: 0.02191470004618168
2025-04-24 01:11:07,392 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 17/32...
2025-04-24 01:11:08,213 - INFO - [fasterquant] - duration: 0.8208053112030029
2025-04-24 01:11:08,213 - INFO - [fasterquant] - avg loss: 0.0001490356371505186
2025-04-24 01:11:08,286 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 17/32...
2025-04-24 01:11:09,115 - INFO - [fasterquant] - duration: 0.8283722400665283
2025-04-24 01:11:09,115 - INFO - [fasterquant] - avg loss: 0.24663330614566803
2025-04-24 01:11:09,188 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 17/32...
2025-04-24 01:11:10,017 - INFO - [fasterquant] - duration: 0.8283469676971436
2025-04-24 01:11:10,018 - INFO - [fasterquant] - avg loss: 0.19446486234664917
2025-04-24 01:11:10,215 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 17/32...
2025-04-24 01:11:13,322 - INFO - [fasterquant] - duration: 3.1073415279388428
2025-04-24 01:11:13,322 - INFO - [fasterquant] - avg loss: 0.0003714623744599521
2025-04-24 01:11:13,389 - INFO - [quantize_model] - Start quantizing block model.layers 18/32
2025-04-24 01:11:13,389 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:11:13,462 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 18/32...
2025-04-24 01:11:14,284 - INFO - [fasterquant] - duration: 0.8217124938964844
2025-04-24 01:11:14,284 - INFO - [fasterquant] - avg loss: 0.1538228988647461
2025-04-24 01:11:14,358 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 18/32...
2025-04-24 01:11:15,176 - INFO - [fasterquant] - duration: 0.8185222148895264
2025-04-24 01:11:15,177 - INFO - [fasterquant] - avg loss: 0.06511540710926056
2025-04-24 01:11:15,251 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 18/32...
2025-04-24 01:11:16,072 - INFO - [fasterquant] - duration: 0.820875883102417
2025-04-24 01:11:16,073 - INFO - [fasterquant] - avg loss: 0.02317310869693756
2025-04-24 01:11:16,147 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 18/32...
2025-04-24 01:11:16,967 - INFO - [fasterquant] - duration: 0.8206274509429932
2025-04-24 01:11:16,968 - INFO - [fasterquant] - avg loss: 0.00018833947251550853
2025-04-24 01:11:17,041 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 18/32...
2025-04-24 01:11:17,868 - INFO - [fasterquant] - duration: 0.8271324634552002
2025-04-24 01:11:17,869 - INFO - [fasterquant] - avg loss: 0.28834855556488037
2025-04-24 01:11:17,943 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 18/32...
2025-04-24 01:11:18,771 - INFO - [fasterquant] - duration: 0.8275933265686035
2025-04-24 01:11:18,772 - INFO - [fasterquant] - avg loss: 0.2279045581817627
2025-04-24 01:11:18,969 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 18/32...
2025-04-24 01:11:22,070 - INFO - [fasterquant] - duration: 3.10158109664917
2025-04-24 01:11:22,071 - INFO - [fasterquant] - avg loss: 0.0004783573094755411
2025-04-24 01:11:22,137 - INFO - [quantize_model] - Start quantizing block model.layers 19/32
2025-04-24 01:11:22,137 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:11:22,210 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 19/32...
2025-04-24 01:11:23,029 - INFO - [fasterquant] - duration: 0.8187997341156006
2025-04-24 01:11:23,029 - INFO - [fasterquant] - avg loss: 0.1855526864528656
2025-04-24 01:11:23,103 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 19/32...
2025-04-24 01:11:23,922 - INFO - [fasterquant] - duration: 0.8198206424713135
2025-04-24 01:11:23,923 - INFO - [fasterquant] - avg loss: 0.07382290065288544
2025-04-24 01:11:23,997 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 19/32...
2025-04-24 01:11:24,817 - INFO - [fasterquant] - duration: 0.8197095394134521
2025-04-24 01:11:24,817 - INFO - [fasterquant] - avg loss: 0.02675982005894184
2025-04-24 01:11:24,891 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 19/32...
2025-04-24 01:11:25,710 - INFO - [fasterquant] - duration: 0.8192548751831055
2025-04-24 01:11:25,711 - INFO - [fasterquant] - avg loss: 0.00020086807489860803
2025-04-24 01:11:25,784 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 19/32...
2025-04-24 01:11:26,611 - INFO - [fasterquant] - duration: 0.8270983695983887
2025-04-24 01:11:26,612 - INFO - [fasterquant] - avg loss: 0.3307194113731384
2025-04-24 01:11:26,685 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 19/32...
2025-04-24 01:11:27,513 - INFO - [fasterquant] - duration: 0.8272216320037842
2025-04-24 01:11:27,513 - INFO - [fasterquant] - avg loss: 0.26328834891319275
2025-04-24 01:11:27,711 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 19/32...
2025-04-24 01:11:30,821 - INFO - [fasterquant] - duration: 3.1101391315460205
2025-04-24 01:11:30,821 - INFO - [fasterquant] - avg loss: 0.0007490526186302304
2025-04-24 01:11:30,888 - INFO - [quantize_model] - Start quantizing block model.layers 20/32
2025-04-24 01:11:30,888 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:11:30,960 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 20/32...
2025-04-24 01:11:31,781 - INFO - [fasterquant] - duration: 0.8207945823669434
2025-04-24 01:11:31,782 - INFO - [fasterquant] - avg loss: 0.16770850121974945
2025-04-24 01:11:31,856 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 20/32...
2025-04-24 01:11:32,679 - INFO - [fasterquant] - duration: 0.8227019309997559
2025-04-24 01:11:32,679 - INFO - [fasterquant] - avg loss: 0.07326426357030869
2025-04-24 01:11:32,757 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 20/32...
2025-04-24 01:11:33,578 - INFO - [fasterquant] - duration: 0.8211855888366699
2025-04-24 01:11:33,578 - INFO - [fasterquant] - avg loss: 0.03120845928788185
2025-04-24 01:11:33,674 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 20/32...
2025-04-24 01:11:34,496 - INFO - [fasterquant] - duration: 0.8215506076812744
2025-04-24 01:11:34,496 - INFO - [fasterquant] - avg loss: 0.00020157647668384016
2025-04-24 01:11:34,570 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 20/32...
2025-04-24 01:11:35,398 - INFO - [fasterquant] - duration: 0.8278841972351074
2025-04-24 01:11:35,398 - INFO - [fasterquant] - avg loss: 0.3733746409416199
2025-04-24 01:11:35,472 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 20/32...
2025-04-24 01:11:36,302 - INFO - [fasterquant] - duration: 0.8292534351348877
2025-04-24 01:11:36,302 - INFO - [fasterquant] - avg loss: 0.2913668155670166
2025-04-24 01:11:36,500 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 20/32...
2025-04-24 01:11:39,603 - INFO - [fasterquant] - duration: 3.103058338165283
2025-04-24 01:11:39,603 - INFO - [fasterquant] - avg loss: 0.001034020446240902
2025-04-24 01:11:39,670 - INFO - [quantize_model] - Start quantizing block model.layers 21/32
2025-04-24 01:11:39,670 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:11:39,742 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 21/32...
2025-04-24 01:11:40,563 - INFO - [fasterquant] - duration: 0.8201160430908203
2025-04-24 01:11:40,563 - INFO - [fasterquant] - avg loss: 0.17712683975696564
2025-04-24 01:11:40,637 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 21/32...
2025-04-24 01:11:41,457 - INFO - [fasterquant] - duration: 0.8197600841522217
2025-04-24 01:11:41,457 - INFO - [fasterquant] - avg loss: 0.0736590176820755
2025-04-24 01:11:41,531 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 21/32...
2025-04-24 01:11:42,351 - INFO - [fasterquant] - duration: 0.8199572563171387
2025-04-24 01:11:42,351 - INFO - [fasterquant] - avg loss: 0.03301237151026726
2025-04-24 01:11:42,424 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 21/32...
2025-04-24 01:11:43,244 - INFO - [fasterquant] - duration: 0.8197247982025146
2025-04-24 01:11:43,245 - INFO - [fasterquant] - avg loss: 0.0002571555960457772
2025-04-24 01:11:43,318 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 21/32...
2025-04-24 01:11:44,146 - INFO - [fasterquant] - duration: 0.8272662162780762
2025-04-24 01:11:44,146 - INFO - [fasterquant] - avg loss: 0.4003344476222992
2025-04-24 01:11:44,220 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 21/32...
2025-04-24 01:11:45,050 - INFO - [fasterquant] - duration: 0.8296289443969727
2025-04-24 01:11:45,051 - INFO - [fasterquant] - avg loss: 0.3045596480369568
2025-04-24 01:11:45,248 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 21/32...
2025-04-24 01:11:48,363 - INFO - [fasterquant] - duration: 3.114962339401245
2025-04-24 01:11:48,364 - INFO - [fasterquant] - avg loss: 0.0009548061643727124
2025-04-24 01:11:48,430 - INFO - [quantize_model] - Start quantizing block model.layers 22/32
2025-04-24 01:11:48,431 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:11:48,503 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 22/32...
2025-04-24 01:11:49,323 - INFO - [fasterquant] - duration: 0.8197517395019531
2025-04-24 01:11:49,324 - INFO - [fasterquant] - avg loss: 0.16915129125118256
2025-04-24 01:11:49,397 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 22/32...
2025-04-24 01:11:50,218 - INFO - [fasterquant] - duration: 0.8206610679626465
2025-04-24 01:11:50,219 - INFO - [fasterquant] - avg loss: 0.06914353370666504
2025-04-24 01:11:50,292 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 22/32...
2025-04-24 01:11:51,114 - INFO - [fasterquant] - duration: 0.8222618103027344
2025-04-24 01:11:51,114 - INFO - [fasterquant] - avg loss: 0.03411690518260002
2025-04-24 01:11:51,188 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 22/32...
2025-04-24 01:11:52,008 - INFO - [fasterquant] - duration: 0.8205687999725342
2025-04-24 01:11:52,009 - INFO - [fasterquant] - avg loss: 0.00020208746718708426
2025-04-24 01:11:52,082 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 22/32...
2025-04-24 01:11:52,913 - INFO - [fasterquant] - duration: 0.8307437896728516
2025-04-24 01:11:52,914 - INFO - [fasterquant] - avg loss: 0.45680293440818787
2025-04-24 01:11:52,988 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 22/32...
2025-04-24 01:11:53,816 - INFO - [fasterquant] - duration: 0.8284854888916016
2025-04-24 01:11:53,817 - INFO - [fasterquant] - avg loss: 0.32470154762268066
2025-04-24 01:11:54,014 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 22/32...
2025-04-24 01:11:57,125 - INFO - [fasterquant] - duration: 3.110788583755493
2025-04-24 01:11:57,125 - INFO - [fasterquant] - avg loss: 0.0009128875099122524
2025-04-24 01:11:57,194 - INFO - [quantize_model] - Start quantizing block model.layers 23/32
2025-04-24 01:11:57,194 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:11:57,268 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 23/32...
2025-04-24 01:11:58,087 - INFO - [fasterquant] - duration: 0.8190732002258301
2025-04-24 01:11:58,087 - INFO - [fasterquant] - avg loss: 0.16565163433551788
2025-04-24 01:11:58,161 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 23/32...
2025-04-24 01:11:58,982 - INFO - [fasterquant] - duration: 0.8207125663757324
2025-04-24 01:11:58,982 - INFO - [fasterquant] - avg loss: 0.06717315316200256
2025-04-24 01:11:59,055 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 23/32...
2025-04-24 01:11:59,879 - INFO - [fasterquant] - duration: 0.8239245414733887
2025-04-24 01:11:59,880 - INFO - [fasterquant] - avg loss: 0.035626284778118134
2025-04-24 01:11:59,953 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 23/32...
2025-04-24 01:12:00,774 - INFO - [fasterquant] - duration: 0.8199498653411865
2025-04-24 01:12:00,774 - INFO - [fasterquant] - avg loss: 0.00012163574865553528
2025-04-24 01:12:00,848 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 23/32...
2025-04-24 01:12:01,679 - INFO - [fasterquant] - duration: 0.8309745788574219
2025-04-24 01:12:01,679 - INFO - [fasterquant] - avg loss: 0.4736340641975403
2025-04-24 01:12:01,753 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 23/32...
2025-04-24 01:12:02,581 - INFO - [fasterquant] - duration: 0.8274068832397461
2025-04-24 01:12:02,582 - INFO - [fasterquant] - avg loss: 0.3402707874774933
2025-04-24 01:12:02,779 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 23/32...
2025-04-24 01:12:05,883 - INFO - [fasterquant] - duration: 3.1042797565460205
2025-04-24 01:12:05,884 - INFO - [fasterquant] - avg loss: 0.0009704548865556717
2025-04-24 01:12:05,950 - INFO - [quantize_model] - Start quantizing block model.layers 24/32
2025-04-24 01:12:05,951 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:12:06,024 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 24/32...
2025-04-24 01:12:06,843 - INFO - [fasterquant] - duration: 0.8197276592254639
2025-04-24 01:12:06,844 - INFO - [fasterquant] - avg loss: 0.16218125820159912
2025-04-24 01:12:06,918 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 24/32...
2025-04-24 01:12:07,739 - INFO - [fasterquant] - duration: 0.820533275604248
2025-04-24 01:12:07,739 - INFO - [fasterquant] - avg loss: 0.06484012305736542
2025-04-24 01:12:07,813 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 24/32...
2025-04-24 01:12:08,634 - INFO - [fasterquant] - duration: 0.8213052749633789
2025-04-24 01:12:08,635 - INFO - [fasterquant] - avg loss: 0.036290254443883896
2025-04-24 01:12:08,709 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 24/32...
2025-04-24 01:12:09,531 - INFO - [fasterquant] - duration: 0.8214352130889893
2025-04-24 01:12:09,531 - INFO - [fasterquant] - avg loss: 0.0002147832710761577
2025-04-24 01:12:09,605 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 24/32...
2025-04-24 01:12:10,434 - INFO - [fasterquant] - duration: 0.8295238018035889
2025-04-24 01:12:10,435 - INFO - [fasterquant] - avg loss: 0.5156468152999878
2025-04-24 01:12:10,509 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 24/32...
2025-04-24 01:12:11,341 - INFO - [fasterquant] - duration: 0.8318917751312256
2025-04-24 01:12:11,342 - INFO - [fasterquant] - avg loss: 0.36859866976737976
2025-04-24 01:12:11,539 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 24/32...
2025-04-24 01:12:14,654 - INFO - [fasterquant] - duration: 3.114677667617798
2025-04-24 01:12:14,654 - INFO - [fasterquant] - avg loss: 0.0011016387725248933
2025-04-24 01:12:14,775 - INFO - [quantize_model] - Start quantizing block model.layers 25/32
2025-04-24 01:12:14,775 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:12:14,856 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 25/32...
2025-04-24 01:12:15,677 - INFO - [fasterquant] - duration: 0.8205604553222656
2025-04-24 01:12:15,677 - INFO - [fasterquant] - avg loss: 0.1777518391609192
2025-04-24 01:12:15,751 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 25/32...
2025-04-24 01:12:16,571 - INFO - [fasterquant] - duration: 0.8203122615814209
2025-04-24 01:12:16,572 - INFO - [fasterquant] - avg loss: 0.07106515765190125
2025-04-24 01:12:16,645 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 25/32...
2025-04-24 01:12:17,465 - INFO - [fasterquant] - duration: 0.8201799392700195
2025-04-24 01:12:17,466 - INFO - [fasterquant] - avg loss: 0.040471259504556656
2025-04-24 01:12:17,539 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 25/32...
2025-04-24 01:12:18,358 - INFO - [fasterquant] - duration: 0.8189213275909424
2025-04-24 01:12:18,359 - INFO - [fasterquant] - avg loss: 0.00013067643158137798
2025-04-24 01:12:18,433 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 25/32...
2025-04-24 01:12:19,263 - INFO - [fasterquant] - duration: 0.8303005695343018
2025-04-24 01:12:19,264 - INFO - [fasterquant] - avg loss: 0.5715172290802002
2025-04-24 01:12:19,338 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 25/32...
2025-04-24 01:12:20,168 - INFO - [fasterquant] - duration: 0.8296301364898682
2025-04-24 01:12:20,168 - INFO - [fasterquant] - avg loss: 0.40089672803878784
2025-04-24 01:12:20,365 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 25/32...
2025-04-24 01:12:23,463 - INFO - [fasterquant] - duration: 3.0972514152526855
2025-04-24 01:12:23,463 - INFO - [fasterquant] - avg loss: 0.0010364046320319176
2025-04-24 01:12:23,530 - INFO - [quantize_model] - Start quantizing block model.layers 26/32
2025-04-24 01:12:23,530 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:12:23,603 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 26/32...
2025-04-24 01:12:24,422 - INFO - [fasterquant] - duration: 0.8187437057495117
2025-04-24 01:12:24,422 - INFO - [fasterquant] - avg loss: 0.17694997787475586
2025-04-24 01:12:24,495 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 26/32...
2025-04-24 01:12:25,314 - INFO - [fasterquant] - duration: 0.8179805278778076
2025-04-24 01:12:25,314 - INFO - [fasterquant] - avg loss: 0.0683308094739914
2025-04-24 01:12:25,387 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 26/32...
2025-04-24 01:12:26,209 - INFO - [fasterquant] - duration: 0.8214592933654785
2025-04-24 01:12:26,210 - INFO - [fasterquant] - avg loss: 0.046595312654972076
2025-04-24 01:12:26,283 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 26/32...
2025-04-24 01:12:27,102 - INFO - [fasterquant] - duration: 0.8193771839141846
2025-04-24 01:12:27,103 - INFO - [fasterquant] - avg loss: 0.00016372260870411992
2025-04-24 01:12:27,176 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 26/32...
2025-04-24 01:12:28,007 - INFO - [fasterquant] - duration: 0.8306961059570312
2025-04-24 01:12:28,008 - INFO - [fasterquant] - avg loss: 0.6101332306861877
2025-04-24 01:12:28,081 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 26/32...
2025-04-24 01:12:28,910 - INFO - [fasterquant] - duration: 0.8285136222839355
2025-04-24 01:12:28,910 - INFO - [fasterquant] - avg loss: 0.4318338930606842
2025-04-24 01:12:29,108 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 26/32...
2025-04-24 01:12:32,221 - INFO - [fasterquant] - duration: 3.1136162281036377
2025-04-24 01:12:32,222 - INFO - [fasterquant] - avg loss: 0.0011074451031163335
2025-04-24 01:12:32,289 - INFO - [quantize_model] - Start quantizing block model.layers 27/32
2025-04-24 01:12:32,289 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:12:32,362 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 27/32...
2025-04-24 01:12:33,180 - INFO - [fasterquant] - duration: 0.8176345825195312
2025-04-24 01:12:33,180 - INFO - [fasterquant] - avg loss: 0.1689607948064804
2025-04-24 01:12:33,255 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 27/32...
2025-04-24 01:12:34,074 - INFO - [fasterquant] - duration: 0.8195230960845947
2025-04-24 01:12:34,075 - INFO - [fasterquant] - avg loss: 0.06439773738384247
2025-04-24 01:12:34,149 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 27/32...
2025-04-24 01:12:34,968 - INFO - [fasterquant] - duration: 0.8190734386444092
2025-04-24 01:12:34,969 - INFO - [fasterquant] - avg loss: 0.049157336354255676
2025-04-24 01:12:35,042 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 27/32...
2025-04-24 01:12:35,862 - INFO - [fasterquant] - duration: 0.8199906349182129
2025-04-24 01:12:35,863 - INFO - [fasterquant] - avg loss: 0.00017825813847593963
2025-04-24 01:12:35,937 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 27/32...
2025-04-24 01:12:36,769 - INFO - [fasterquant] - duration: 0.8318614959716797
2025-04-24 01:12:36,770 - INFO - [fasterquant] - avg loss: 0.6263474225997925
2025-04-24 01:12:36,844 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 27/32...
2025-04-24 01:12:37,673 - INFO - [fasterquant] - duration: 0.8287298679351807
2025-04-24 01:12:37,674 - INFO - [fasterquant] - avg loss: 0.45965704321861267
2025-04-24 01:12:37,871 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 27/32...
2025-04-24 01:12:41,149 - INFO - [fasterquant] - duration: 3.2780098915100098
2025-04-24 01:12:41,150 - INFO - [fasterquant] - avg loss: 0.0012719518272206187
2025-04-24 01:12:41,222 - INFO - [quantize_model] - Start quantizing block model.layers 28/32
2025-04-24 01:12:41,223 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:12:41,298 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 28/32...
2025-04-24 01:12:42,123 - INFO - [fasterquant] - duration: 0.8255834579467773
2025-04-24 01:12:42,124 - INFO - [fasterquant] - avg loss: 0.17114195227622986
2025-04-24 01:12:42,201 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 28/32...
2025-04-24 01:12:43,029 - INFO - [fasterquant] - duration: 0.8274545669555664
2025-04-24 01:12:43,029 - INFO - [fasterquant] - avg loss: 0.06379752606153488
2025-04-24 01:12:43,103 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 28/32...
2025-04-24 01:12:43,934 - INFO - [fasterquant] - duration: 0.8303160667419434
2025-04-24 01:12:43,935 - INFO - [fasterquant] - avg loss: 0.044289492070674896
2025-04-24 01:12:44,010 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 28/32...
2025-04-24 01:12:44,839 - INFO - [fasterquant] - duration: 0.8280701637268066
2025-04-24 01:12:44,839 - INFO - [fasterquant] - avg loss: 0.00023939461971167475
2025-04-24 01:12:44,915 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 28/32...
2025-04-24 01:12:45,754 - INFO - [fasterquant] - duration: 0.8386378288269043
2025-04-24 01:12:45,755 - INFO - [fasterquant] - avg loss: 0.6881312131881714
2025-04-24 01:12:45,830 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 28/32...
2025-04-24 01:12:46,666 - INFO - [fasterquant] - duration: 0.8361527919769287
2025-04-24 01:12:46,667 - INFO - [fasterquant] - avg loss: 0.5087021589279175
2025-04-24 01:12:46,865 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 28/32...
2025-04-24 01:12:50,075 - INFO - [fasterquant] - duration: 3.2095110416412354
2025-04-24 01:12:50,075 - INFO - [fasterquant] - avg loss: 0.001693174010142684
2025-04-24 01:12:50,144 - INFO - [quantize_model] - Start quantizing block model.layers 29/32
2025-04-24 01:12:50,144 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:12:50,217 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 29/32...
2025-04-24 01:12:51,038 - INFO - [fasterquant] - duration: 0.8206295967102051
2025-04-24 01:12:51,039 - INFO - [fasterquant] - avg loss: 0.17221131920814514
2025-04-24 01:12:51,112 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 29/32...
2025-04-24 01:12:51,937 - INFO - [fasterquant] - duration: 0.8251380920410156
2025-04-24 01:12:51,938 - INFO - [fasterquant] - avg loss: 0.06652294099330902
2025-04-24 01:12:52,012 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 29/32...
2025-04-24 01:12:52,837 - INFO - [fasterquant] - duration: 0.8255534172058105
2025-04-24 01:12:52,838 - INFO - [fasterquant] - avg loss: 0.06646367907524109
2025-04-24 01:12:52,912 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 29/32...
2025-04-24 01:12:53,735 - INFO - [fasterquant] - duration: 0.8226926326751709
2025-04-24 01:12:53,736 - INFO - [fasterquant] - avg loss: 0.000427009304985404
2025-04-24 01:12:53,809 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 29/32...
2025-04-24 01:12:54,646 - INFO - [fasterquant] - duration: 0.8366918563842773
2025-04-24 01:12:54,646 - INFO - [fasterquant] - avg loss: 0.7574484348297119
2025-04-24 01:12:54,721 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 29/32...
2025-04-24 01:12:55,557 - INFO - [fasterquant] - duration: 0.8362298011779785
2025-04-24 01:12:55,558 - INFO - [fasterquant] - avg loss: 0.5803806781768799
2025-04-24 01:12:55,756 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 29/32...
2025-04-24 01:12:58,874 - INFO - [fasterquant] - duration: 3.118464231491089
2025-04-24 01:12:58,875 - INFO - [fasterquant] - avg loss: 0.0019541364163160324
2025-04-24 01:12:58,943 - INFO - [quantize_model] - Start quantizing block model.layers 30/32
2025-04-24 01:12:58,943 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:12:59,016 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 30/32...
2025-04-24 01:12:59,839 - INFO - [fasterquant] - duration: 0.8227691650390625
2025-04-24 01:12:59,840 - INFO - [fasterquant] - avg loss: 0.19063225388526917
2025-04-24 01:12:59,914 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 30/32...
2025-04-24 01:13:00,747 - INFO - [fasterquant] - duration: 0.8329036235809326
2025-04-24 01:13:00,748 - INFO - [fasterquant] - avg loss: 0.06455223262310028
2025-04-24 01:13:00,824 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 30/32...
2025-04-24 01:13:01,661 - INFO - [fasterquant] - duration: 0.837059736251831
2025-04-24 01:13:01,662 - INFO - [fasterquant] - avg loss: 0.09169565141201019
2025-04-24 01:13:01,736 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 30/32...
2025-04-24 01:13:02,559 - INFO - [fasterquant] - duration: 0.8233351707458496
2025-04-24 01:13:02,560 - INFO - [fasterquant] - avg loss: 0.0008718708995729685
2025-04-24 01:13:02,634 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 30/32...
2025-04-24 01:13:03,472 - INFO - [fasterquant] - duration: 0.8379740715026855
2025-04-24 01:13:03,473 - INFO - [fasterquant] - avg loss: 0.765069842338562
2025-04-24 01:13:03,547 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 30/32...
2025-04-24 01:13:04,381 - INFO - [fasterquant] - duration: 0.8334908485412598
2025-04-24 01:13:04,381 - INFO - [fasterquant] - avg loss: 0.6109758615493774
2025-04-24 01:13:04,579 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 30/32...
2025-04-24 01:13:07,695 - INFO - [fasterquant] - duration: 3.115826368331909
2025-04-24 01:13:07,695 - INFO - [fasterquant] - avg loss: 0.002990799257531762
2025-04-24 01:13:07,762 - INFO - [quantize_model] - Start quantizing block model.layers 31/32
2025-04-24 01:13:07,762 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:13:07,835 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 31/32...
2025-04-24 01:13:08,658 - INFO - [fasterquant] - duration: 0.8225753307342529
2025-04-24 01:13:08,659 - INFO - [fasterquant] - avg loss: 0.1844150722026825
2025-04-24 01:13:08,732 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 31/32...
2025-04-24 01:13:09,561 - INFO - [fasterquant] - duration: 0.8283874988555908
2025-04-24 01:13:09,561 - INFO - [fasterquant] - avg loss: 0.0625353455543518
2025-04-24 01:13:09,700 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 31/32...
2025-04-24 01:13:10,526 - INFO - [fasterquant] - duration: 0.8256006240844727
2025-04-24 01:13:10,527 - INFO - [fasterquant] - avg loss: 0.10608762502670288
2025-04-24 01:13:10,600 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 31/32...
2025-04-24 01:13:11,422 - INFO - [fasterquant] - duration: 0.8218936920166016
2025-04-24 01:13:11,424 - INFO - [fasterquant] - avg loss: 0.000715480768121779
2025-04-24 01:13:11,498 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 31/32...
2025-04-24 01:13:12,335 - INFO - [fasterquant] - duration: 0.8363909721374512
2025-04-24 01:13:12,336 - INFO - [fasterquant] - avg loss: 0.7817801833152771
2025-04-24 01:13:12,411 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 31/32...
2025-04-24 01:13:13,247 - INFO - [fasterquant] - duration: 0.8355159759521484
2025-04-24 01:13:13,248 - INFO - [fasterquant] - avg loss: 0.634660542011261
2025-04-24 01:13:13,446 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 31/32...
2025-04-24 01:13:16,578 - INFO - [fasterquant] - duration: 3.132551670074463
2025-04-24 01:13:16,579 - INFO - [fasterquant] - avg loss: 0.004695412702858448
2025-04-24 01:13:16,649 - INFO - [quantize_model] - Start quantizing block model.layers 32/32
2025-04-24 01:13:16,649 - INFO - [quantize_model] - Module to quantize [['self_attn.q_proj'], ['self_attn.k_proj'], ['self_attn.v_proj'], ['self_attn.o_proj'], ['mlp.gate_proj'], ['mlp.up_proj'], ['mlp.down_proj']]
2025-04-24 01:13:16,723 - INFO - [quantize_model] - Quantizing self_attn.q_proj in block 32/32...
2025-04-24 01:13:17,548 - INFO - [fasterquant] - duration: 0.824171781539917
2025-04-24 01:13:17,549 - INFO - [fasterquant] - avg loss: 0.16836537420749664
2025-04-24 01:13:17,623 - INFO - [quantize_model] - Quantizing self_attn.k_proj in block 32/32...
2025-04-24 01:13:18,447 - INFO - [fasterquant] - duration: 0.8238058090209961
2025-04-24 01:13:18,448 - INFO - [fasterquant] - avg loss: 0.05835343152284622
2025-04-24 01:13:18,522 - INFO - [quantize_model] - Quantizing self_attn.v_proj in block 32/32...
2025-04-24 01:13:19,348 - INFO - [fasterquant] - duration: 0.8256773948669434
2025-04-24 01:13:19,348 - INFO - [fasterquant] - avg loss: 0.10583534836769104
2025-04-24 01:13:19,423 - INFO - [quantize_model] - Quantizing self_attn.o_proj in block 32/32...
2025-04-24 01:13:20,248 - INFO - [fasterquant] - duration: 0.8241593837738037
2025-04-24 01:13:20,248 - INFO - [fasterquant] - avg loss: 0.001217396929860115
2025-04-24 01:13:20,322 - INFO - [quantize_model] - Quantizing mlp.gate_proj in block 32/32...
2025-04-24 01:13:21,158 - INFO - [fasterquant] - duration: 0.8350248336791992
2025-04-24 01:13:21,158 - INFO - [fasterquant] - avg loss: 0.6640058755874634
2025-04-24 01:13:21,232 - INFO - [quantize_model] - Quantizing mlp.up_proj in block 32/32...
2025-04-24 01:13:22,067 - INFO - [fasterquant] - duration: 0.8341517448425293
2025-04-24 01:13:22,067 - INFO - [fasterquant] - avg loss: 0.5257601737976074
2025-04-24 01:13:22,265 - INFO - [quantize_model] - Quantizing mlp.down_proj in block 32/32...
2025-04-24 01:13:25,382 - INFO - [fasterquant] - duration: 3.116802215576172
2025-04-24 01:13:25,382 - INFO - [fasterquant] - avg loss: 0.026912111788988113
2025-04-24 01:13:25,451 - INFO - [pack_model] - Packing model...
2025-04-24 01:13:28,578 - INFO - [pack_model] - model.layers.0.self_attn.k_proj
2025-04-24 01:13:28,866 - INFO - [pack_model] - model.layers.0.self_attn.o_proj
2025-04-24 01:13:30,636 - INFO - [pack_model] - model.layers.0.self_attn.q_proj
2025-04-24 01:13:32,420 - INFO - [pack_model] - model.layers.0.self_attn.v_proj
2025-04-24 01:13:32,579 - INFO - [pack_model] - model.layers.0.mlp.down_proj
2025-04-24 01:13:40,029 - INFO - [pack_model] - model.layers.0.mlp.gate_proj
2025-04-24 01:13:44,058 - INFO - [pack_model] - model.layers.0.mlp.up_proj
2025-04-24 01:13:48,066 - INFO - [pack_model] - model.layers.1.self_attn.k_proj
2025-04-24 01:13:48,226 - INFO - [pack_model] - model.layers.1.self_attn.o_proj
2025-04-24 01:13:50,017 - INFO - [pack_model] - model.layers.1.self_attn.q_proj
2025-04-24 01:13:51,829 - INFO - [pack_model] - model.layers.1.self_attn.v_proj
2025-04-24 01:13:51,987 - INFO - [pack_model] - model.layers.1.mlp.down_proj
2025-04-24 01:13:59,505 - INFO - [pack_model] - model.layers.1.mlp.gate_proj
2025-04-24 01:14:03,553 - INFO - [pack_model] - model.layers.1.mlp.up_proj
2025-04-24 01:14:07,387 - INFO - [pack_model] - model.layers.2.self_attn.k_proj
2025-04-24 01:14:07,548 - INFO - [pack_model] - model.layers.2.self_attn.o_proj
2025-04-24 01:14:09,415 - INFO - [pack_model] - model.layers.2.self_attn.q_proj
2025-04-24 01:14:11,314 - INFO - [pack_model] - model.layers.2.self_attn.v_proj
2025-04-24 01:14:11,473 - INFO - [pack_model] - model.layers.2.mlp.down_proj
2025-04-24 01:14:19,338 - INFO - [pack_model] - model.layers.2.mlp.gate_proj
2025-04-24 01:14:23,459 - INFO - [pack_model] - model.layers.2.mlp.up_proj
2025-04-24 01:14:27,489 - INFO - [pack_model] - model.layers.3.self_attn.k_proj
2025-04-24 01:14:27,650 - INFO - [pack_model] - model.layers.3.self_attn.o_proj
2025-04-24 01:14:29,524 - INFO - [pack_model] - model.layers.3.self_attn.q_proj
2025-04-24 01:14:31,522 - INFO - [pack_model] - model.layers.3.self_attn.v_proj
2025-04-24 01:14:31,684 - INFO - [pack_model] - model.layers.3.mlp.down_proj
2025-04-24 01:14:38,423 - INFO - [pack_model] - model.layers.3.mlp.gate_proj
2025-04-24 01:14:42,569 - INFO - [pack_model] - model.layers.3.mlp.up_proj
2025-04-24 01:14:46,761 - INFO - [pack_model] - model.layers.4.self_attn.k_proj
2025-04-24 01:14:46,921 - INFO - [pack_model] - model.layers.4.self_attn.o_proj
2025-04-24 01:14:48,723 - INFO - [pack_model] - model.layers.4.self_attn.q_proj
2025-04-24 01:14:50,496 - INFO - [pack_model] - model.layers.4.self_attn.v_proj
2025-04-24 01:14:50,665 - INFO - [pack_model] - model.layers.4.mlp.down_proj
2025-04-24 01:14:57,918 - INFO - [pack_model] - model.layers.4.mlp.gate_proj
2025-04-24 01:15:02,064 - INFO - [pack_model] - model.layers.4.mlp.up_proj
2025-04-24 01:15:06,074 - INFO - [pack_model] - model.layers.5.self_attn.k_proj
2025-04-24 01:15:06,237 - INFO - [pack_model] - model.layers.5.self_attn.o_proj
2025-04-24 01:15:07,823 - INFO - [pack_model] - model.layers.5.self_attn.q_proj
2025-04-24 01:15:09,431 - INFO - [pack_model] - model.layers.5.self_attn.v_proj
2025-04-24 01:15:09,591 - INFO - [pack_model] - model.layers.5.mlp.down_proj
2025-04-24 01:15:16,209 - INFO - [pack_model] - model.layers.5.mlp.gate_proj
2025-04-24 01:15:20,121 - INFO - [pack_model] - model.layers.5.mlp.up_proj
2025-04-24 01:15:24,493 - INFO - [pack_model] - model.layers.6.self_attn.k_proj
2025-04-24 01:15:24,654 - INFO - [pack_model] - model.layers.6.self_attn.o_proj
2025-04-24 01:15:26,515 - INFO - [pack_model] - model.layers.6.self_attn.q_proj
2025-04-24 01:15:28,418 - INFO - [pack_model] - model.layers.6.self_attn.v_proj
2025-04-24 01:15:28,579 - INFO - [pack_model] - model.layers.6.mlp.down_proj
2025-04-24 01:15:35,938 - INFO - [pack_model] - model.layers.6.mlp.gate_proj
2025-04-24 01:15:40,164 - INFO - [pack_model] - model.layers.6.mlp.up_proj
2025-04-24 01:15:44,278 - INFO - [pack_model] - model.layers.7.self_attn.k_proj
2025-04-24 01:15:44,437 - INFO - [pack_model] - model.layers.7.self_attn.o_proj
2025-04-24 01:15:46,217 - INFO - [pack_model] - model.layers.7.self_attn.q_proj
2025-04-24 01:15:48,113 - INFO - [pack_model] - model.layers.7.self_attn.v_proj
2025-04-24 01:15:48,273 - INFO - [pack_model] - model.layers.7.mlp.down_proj
2025-04-24 01:15:56,017 - INFO - [pack_model] - model.layers.7.mlp.gate_proj
2025-04-24 01:16:00,058 - INFO - [pack_model] - model.layers.7.mlp.up_proj
2025-04-24 01:16:04,065 - INFO - [pack_model] - model.layers.8.self_attn.k_proj
2025-04-24 01:16:04,227 - INFO - [pack_model] - model.layers.8.self_attn.o_proj
2025-04-24 01:16:06,020 - INFO - [pack_model] - model.layers.8.self_attn.q_proj
2025-04-24 01:16:07,869 - INFO - [pack_model] - model.layers.8.self_attn.v_proj
2025-04-24 01:16:08,026 - INFO - [pack_model] - model.layers.8.mlp.down_proj
2025-04-24 01:16:15,587 - INFO - [pack_model] - model.layers.8.mlp.gate_proj
2025-04-24 01:16:19,667 - INFO - [pack_model] - model.layers.8.mlp.up_proj
2025-04-24 01:16:23,759 - INFO - [pack_model] - model.layers.9.self_attn.k_proj
2025-04-24 01:16:23,919 - INFO - [pack_model] - model.layers.9.self_attn.o_proj
2025-04-24 01:16:25,717 - INFO - [pack_model] - model.layers.9.self_attn.q_proj
2025-04-24 01:16:27,292 - INFO - [pack_model] - model.layers.9.self_attn.v_proj
2025-04-24 01:16:27,449 - INFO - [pack_model] - model.layers.9.mlp.down_proj
2025-04-24 01:16:34,409 - INFO - [pack_model] - model.layers.9.mlp.gate_proj
2025-04-24 01:16:38,447 - INFO - [pack_model] - model.layers.9.mlp.up_proj
2025-04-24 01:16:42,643 - INFO - [pack_model] - model.layers.10.self_attn.k_proj
2025-04-24 01:16:42,804 - INFO - [pack_model] - model.layers.10.self_attn.o_proj
2025-04-24 01:16:44,395 - INFO - [pack_model] - model.layers.10.self_attn.q_proj
2025-04-24 01:16:45,994 - INFO - [pack_model] - model.layers.10.self_attn.v_proj
2025-04-24 01:16:46,152 - INFO - [pack_model] - model.layers.10.mlp.down_proj
2025-04-24 01:16:52,626 - INFO - [pack_model] - model.layers.10.mlp.gate_proj
2025-04-24 01:16:56,437 - INFO - [pack_model] - model.layers.10.mlp.up_proj
2025-04-24 01:17:00,666 - INFO - [pack_model] - model.layers.11.self_attn.k_proj
2025-04-24 01:17:00,844 - INFO - [pack_model] - model.layers.11.self_attn.o_proj
2025-04-24 01:17:02,919 - INFO - [pack_model] - model.layers.11.self_attn.q_proj
2025-04-24 01:17:04,807 - INFO - [pack_model] - model.layers.11.self_attn.v_proj
2025-04-24 01:17:04,975 - INFO - [pack_model] - model.layers.11.mlp.down_proj
2025-04-24 01:17:13,090 - INFO - [pack_model] - model.layers.11.mlp.gate_proj
2025-04-24 01:17:17,250 - INFO - [pack_model] - model.layers.11.mlp.up_proj
2025-04-24 01:17:21,363 - INFO - [pack_model] - model.layers.12.self_attn.k_proj
2025-04-24 01:17:21,525 - INFO - [pack_model] - model.layers.12.self_attn.o_proj
2025-04-24 01:17:23,416 - INFO - [pack_model] - model.layers.12.self_attn.q_proj
2025-04-24 01:17:25,299 - INFO - [pack_model] - model.layers.12.self_attn.v_proj
2025-04-24 01:17:25,462 - INFO - [pack_model] - model.layers.12.mlp.down_proj
2025-04-24 01:17:33,230 - INFO - [pack_model] - model.layers.12.mlp.gate_proj
2025-04-24 01:17:37,363 - INFO - [pack_model] - model.layers.12.mlp.up_proj
2025-04-24 01:17:41,485 - INFO - [pack_model] - model.layers.13.self_attn.k_proj
2025-04-24 01:17:41,650 - INFO - [pack_model] - model.layers.13.self_attn.o_proj
2025-04-24 01:17:43,516 - INFO - [pack_model] - model.layers.13.self_attn.q_proj
2025-04-24 01:17:45,403 - INFO - [pack_model] - model.layers.13.self_attn.v_proj
2025-04-24 01:17:45,568 - INFO - [pack_model] - model.layers.13.mlp.down_proj
2025-04-24 01:17:53,129 - INFO - [pack_model] - model.layers.13.mlp.gate_proj
2025-04-24 01:17:57,350 - INFO - [pack_model] - model.layers.13.mlp.up_proj
2025-04-24 01:18:01,686 - INFO - [pack_model] - model.layers.14.self_attn.k_proj
2025-04-24 01:18:01,956 - INFO - [pack_model] - model.layers.14.self_attn.o_proj
2025-04-24 01:18:03,988 - INFO - [pack_model] - model.layers.14.self_attn.q_proj
2025-04-24 01:18:05,893 - INFO - [pack_model] - model.layers.14.self_attn.v_proj
2025-04-24 01:18:06,056 - INFO - [pack_model] - model.layers.14.mlp.down_proj
2025-04-24 01:18:13,818 - INFO - [pack_model] - model.layers.14.mlp.gate_proj
2025-04-24 01:18:17,965 - INFO - [pack_model] - model.layers.14.mlp.up_proj
2025-04-24 01:18:22,251 - INFO - [pack_model] - model.layers.15.self_attn.k_proj
2025-04-24 01:18:22,420 - INFO - [pack_model] - model.layers.15.self_attn.o_proj
2025-04-24 01:18:23,994 - INFO - [pack_model] - model.layers.15.self_attn.q_proj
2025-04-24 01:18:25,604 - INFO - [pack_model] - model.layers.15.self_attn.v_proj
2025-04-24 01:18:25,766 - INFO - [pack_model] - model.layers.15.mlp.down_proj
2025-04-24 01:18:32,537 - INFO - [pack_model] - model.layers.15.mlp.gate_proj
2025-04-24 01:18:36,677 - INFO - [pack_model] - model.layers.15.mlp.up_proj
2025-04-24 01:18:40,561 - INFO - [pack_model] - model.layers.16.self_attn.k_proj
2025-04-24 01:18:40,726 - INFO - [pack_model] - model.layers.16.self_attn.o_proj
2025-04-24 01:18:42,421 - INFO - [pack_model] - model.layers.16.self_attn.q_proj
2025-04-24 01:18:44,126 - INFO - [pack_model] - model.layers.16.self_attn.v_proj
2025-04-24 01:18:44,303 - INFO - [pack_model] - model.layers.16.mlp.down_proj
2025-04-24 01:18:52,512 - INFO - [pack_model] - model.layers.16.mlp.gate_proj
2025-04-24 01:18:56,647 - INFO - [pack_model] - model.layers.16.mlp.up_proj
2025-04-24 01:19:00,653 - INFO - [pack_model] - model.layers.17.self_attn.k_proj
2025-04-24 01:19:00,819 - INFO - [pack_model] - model.layers.17.self_attn.o_proj
2025-04-24 01:19:02,413 - INFO - [pack_model] - model.layers.17.self_attn.q_proj
2025-04-24 01:19:03,995 - INFO - [pack_model] - model.layers.17.self_attn.v_proj
2025-04-24 01:19:04,160 - INFO - [pack_model] - model.layers.17.mlp.down_proj
2025-04-24 01:19:10,536 - INFO - [pack_model] - model.layers.17.mlp.gate_proj
2025-04-24 01:19:14,372 - INFO - [pack_model] - model.layers.17.mlp.up_proj
2025-04-24 01:19:18,371 - INFO - [pack_model] - model.layers.18.self_attn.k_proj
2025-04-24 01:19:18,538 - INFO - [pack_model] - model.layers.18.self_attn.o_proj
2025-04-24 01:19:20,128 - INFO - [pack_model] - model.layers.18.self_attn.q_proj
2025-04-24 01:19:21,720 - INFO - [pack_model] - model.layers.18.self_attn.v_proj
2025-04-24 01:19:21,880 - INFO - [pack_model] - model.layers.18.mlp.down_proj
2025-04-24 01:19:28,885 - INFO - [pack_model] - model.layers.18.mlp.gate_proj
2025-04-24 01:19:33,105 - INFO - [pack_model] - model.layers.18.mlp.up_proj
2025-04-24 01:19:37,345 - INFO - [pack_model] - model.layers.19.self_attn.k_proj
2025-04-24 01:19:37,507 - INFO - [pack_model] - model.layers.19.self_attn.o_proj
2025-04-24 01:19:39,410 - INFO - [pack_model] - model.layers.19.self_attn.q_proj
2025-04-24 01:19:41,308 - INFO - [pack_model] - model.layers.19.self_attn.v_proj
2025-04-24 01:19:41,467 - INFO - [pack_model] - model.layers.19.mlp.down_proj
2025-04-24 01:19:49,201 - INFO - [pack_model] - model.layers.19.mlp.gate_proj
2025-04-24 01:19:53,396 - INFO - [pack_model] - model.layers.19.mlp.up_proj
2025-04-24 01:19:57,579 - INFO - [pack_model] - model.layers.20.self_attn.k_proj
2025-04-24 01:19:57,745 - INFO - [pack_model] - model.layers.20.self_attn.o_proj
2025-04-24 01:19:59,553 - INFO - [pack_model] - model.layers.20.self_attn.q_proj
2025-04-24 01:20:01,522 - INFO - [pack_model] - model.layers.20.self_attn.v_proj
2025-04-24 01:20:01,695 - INFO - [pack_model] - model.layers.20.mlp.down_proj
2025-04-24 01:20:08,642 - INFO - [pack_model] - model.layers.20.mlp.gate_proj
2025-04-24 01:20:12,607 - INFO - [pack_model] - model.layers.20.mlp.up_proj
2025-04-24 01:20:16,778 - INFO - [pack_model] - model.layers.21.self_attn.k_proj
2025-04-24 01:20:16,942 - INFO - [pack_model] - model.layers.21.self_attn.o_proj
2025-04-24 01:20:18,810 - INFO - [pack_model] - model.layers.21.self_attn.q_proj
2025-04-24 01:20:20,708 - INFO - [pack_model] - model.layers.21.self_attn.v_proj
2025-04-24 01:20:20,869 - INFO - [pack_model] - model.layers.21.mlp.down_proj
2025-04-24 01:20:28,842 - INFO - [pack_model] - model.layers.21.mlp.gate_proj
2025-04-24 01:20:33,094 - INFO - [pack_model] - model.layers.21.mlp.up_proj
2025-04-24 01:20:37,683 - INFO - [pack_model] - model.layers.22.self_attn.k_proj
2025-04-24 01:20:37,856 - INFO - [pack_model] - model.layers.22.self_attn.o_proj
2025-04-24 01:20:39,929 - INFO - [pack_model] - model.layers.22.self_attn.q_proj
2025-04-24 01:20:41,942 - INFO - [pack_model] - model.layers.22.self_attn.v_proj
2025-04-24 01:20:42,108 - INFO - [pack_model] - model.layers.22.mlp.down_proj
2025-04-24 01:20:51,042 - INFO - [pack_model] - model.layers.22.mlp.gate_proj
2025-04-24 01:20:55,039 - INFO - [pack_model] - model.layers.22.mlp.up_proj
2025-04-24 01:20:58,959 - INFO - [pack_model] - model.layers.23.self_attn.k_proj
2025-04-24 01:20:59,124 - INFO - [pack_model] - model.layers.23.self_attn.o_proj
2025-04-24 01:21:00,741 - INFO - [pack_model] - model.layers.23.self_attn.q_proj
2025-04-24 01:21:02,392 - INFO - [pack_model] - model.layers.23.self_attn.v_proj
2025-04-24 01:21:02,556 - INFO - [pack_model] - model.layers.23.mlp.down_proj
2025-04-24 01:21:09,619 - INFO - [pack_model] - model.layers.23.mlp.gate_proj
2025-04-24 01:21:13,971 - INFO - [pack_model] - model.layers.23.mlp.up_proj
2025-04-24 01:21:18,421 - INFO - [pack_model] - model.layers.24.self_attn.k_proj
2025-04-24 01:21:18,589 - INFO - [pack_model] - model.layers.24.self_attn.o_proj
2025-04-24 01:21:20,624 - INFO - [pack_model] - model.layers.24.self_attn.q_proj
2025-04-24 01:21:22,508 - INFO - [pack_model] - model.layers.24.self_attn.v_proj
2025-04-24 01:21:22,673 - INFO - [pack_model] - model.layers.24.mlp.down_proj
2025-04-24 01:21:30,341 - INFO - [pack_model] - model.layers.24.mlp.gate_proj
2025-04-24 01:21:34,395 - INFO - [pack_model] - model.layers.24.mlp.up_proj
2025-04-24 01:21:38,564 - INFO - [pack_model] - model.layers.25.self_attn.k_proj
2025-04-24 01:21:38,730 - INFO - [pack_model] - model.layers.25.self_attn.o_proj
2025-04-24 01:21:40,713 - INFO - [pack_model] - model.layers.25.self_attn.q_proj
2025-04-24 01:21:42,606 - INFO - [pack_model] - model.layers.25.self_attn.v_proj
2025-04-24 01:21:42,770 - INFO - [pack_model] - model.layers.25.mlp.down_proj
2025-04-24 01:21:50,131 - INFO - [pack_model] - model.layers.25.mlp.gate_proj
2025-04-24 01:21:54,357 - INFO - [pack_model] - model.layers.25.mlp.up_proj
2025-04-24 01:21:59,503 - INFO - [pack_model] - model.layers.26.self_attn.k_proj
2025-04-24 01:21:59,673 - INFO - [pack_model] - model.layers.26.self_attn.o_proj
2025-04-24 01:22:01,921 - INFO - [pack_model] - model.layers.26.self_attn.q_proj
2025-04-24 01:22:03,913 - INFO - [pack_model] - model.layers.26.self_attn.v_proj
2025-04-24 01:22:04,075 - INFO - [pack_model] - model.layers.26.mlp.down_proj
2025-04-24 01:22:11,946 - INFO - [pack_model] - model.layers.26.mlp.gate_proj
2025-04-24 01:22:16,277 - INFO - [pack_model] - model.layers.26.mlp.up_proj
2025-04-24 01:22:20,225 - INFO - [pack_model] - model.layers.27.self_attn.k_proj
2025-04-24 01:22:20,393 - INFO - [pack_model] - model.layers.27.self_attn.o_proj
2025-04-24 01:22:22,132 - INFO - [pack_model] - model.layers.27.self_attn.q_proj
2025-04-24 01:22:23,909 - INFO - [pack_model] - model.layers.27.self_attn.v_proj
2025-04-24 01:22:24,071 - INFO - [pack_model] - model.layers.27.mlp.down_proj
2025-04-24 01:22:30,743 - INFO - [pack_model] - model.layers.27.mlp.gate_proj
2025-04-24 01:22:34,775 - INFO - [pack_model] - model.layers.27.mlp.up_proj
2025-04-24 01:22:38,761 - INFO - [pack_model] - model.layers.28.self_attn.k_proj
2025-04-24 01:22:38,927 - INFO - [pack_model] - model.layers.28.self_attn.o_proj
2025-04-24 01:22:40,536 - INFO - [pack_model] - model.layers.28.self_attn.q_proj
2025-04-24 01:22:42,107 - INFO - [pack_model] - model.layers.28.self_attn.v_proj
2025-04-24 01:22:42,266 - INFO - [pack_model] - model.layers.28.mlp.down_proj
2025-04-24 01:22:49,025 - INFO - [pack_model] - model.layers.28.mlp.gate_proj
2025-04-24 01:22:52,871 - INFO - [pack_model] - model.layers.28.mlp.up_proj
2025-04-24 01:22:56,666 - INFO - [pack_model] - model.layers.29.self_attn.k_proj
2025-04-24 01:22:56,830 - INFO - [pack_model] - model.layers.29.self_attn.o_proj
2025-04-24 01:22:58,515 - INFO - [pack_model] - model.layers.29.self_attn.q_proj
2025-04-24 01:23:00,188 - INFO - [pack_model] - model.layers.29.self_attn.v_proj
2025-04-24 01:23:00,359 - INFO - [pack_model] - model.layers.29.mlp.down_proj
2025-04-24 01:23:07,115 - INFO - [pack_model] - model.layers.29.mlp.gate_proj
2025-04-24 01:23:11,078 - INFO - [pack_model] - model.layers.29.mlp.up_proj
2025-04-24 01:23:14,977 - INFO - [pack_model] - model.layers.30.self_attn.k_proj
2025-04-24 01:23:15,140 - INFO - [pack_model] - model.layers.30.self_attn.o_proj
2025-04-24 01:23:16,716 - INFO - [pack_model] - model.layers.30.self_attn.q_proj
2025-04-24 01:23:18,296 - INFO - [pack_model] - model.layers.30.self_attn.v_proj
2025-04-24 01:23:18,455 - INFO - [pack_model] - model.layers.30.mlp.down_proj
2025-04-24 01:23:26,038 - INFO - [pack_model] - model.layers.30.mlp.gate_proj
2025-04-24 01:23:30,182 - INFO - [pack_model] - model.layers.30.mlp.up_proj
2025-04-24 01:23:34,283 - INFO - [pack_model] - model.layers.31.self_attn.k_proj
2025-04-24 01:23:34,446 - INFO - [pack_model] - model.layers.31.self_attn.o_proj
2025-04-24 01:23:36,308 - INFO - [pack_model] - model.layers.31.self_attn.q_proj
2025-04-24 01:23:38,203 - INFO - [pack_model] - model.layers.31.self_attn.v_proj
2025-04-24 01:23:38,363 - INFO - [pack_model] - model.layers.31.mlp.down_proj
2025-04-24 01:23:45,936 - INFO - [pack_model] - model.layers.31.mlp.gate_proj
2025-04-24 01:23:50,318 - INFO - [pack_model] - model.layers.31.mlp.up_proj
2025-04-24 01:23:54,482 - INFO - [pack_model] - Model packed.
2025-04-24 01:23:55,169 - INFO - [run_quantization] - Model loading and quantization finished. Duration: 0:15:15.326739
2025-04-24 01:23:55,169 - INFO - [run_quantization] - Saving quantized model and tokenizer to /workspace/models/Mistral-7B-Instruct-v0.3/quantized-gptq-4bit...
2025-04-24 01:23:58,631 - INFO - [run_quantization] - Successfully saved quantized model and tokenizer. Duration: 0:00:03.461808
2025-04-24 01:23:58,632 - WARNING - [run_quantization] - quantize_config.json is missing after saving. Attempting to create it manually.
2025-04-24 01:23:58,632 - INFO - [run_quantization] - Found quantization config attached to model config. Saving it.
2025-04-24 01:23:58,632 - ERROR - [run_quantization] - Error manually creating quantize_config.json: QuantizationConfigMixin.to_json_file() got an unexpected keyword argument 'use_diff'
Traceback (most recent call last):
  File "/workspace/code/quantize_models.py", line 218, in run_quantization
    final_gptq_config.to_json_file(quantize_config_path, use_diff=False) # use_diff=False ensures all keys are saved
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: QuantizationConfigMixin.to_json_file() got an unexpected keyword argument 'use_diff'
2025-04-24 01:23:58,633 - INFO - [run_quantization] - Cleaning up model object from memory...
2025-04-24 01:23:58,633 - INFO - [run_quantization] - Cleared CUDA cache.
2025-04-24 01:23:58,633 - INFO - [<module>] - 
--- AutoGPTQ Quantization Script Finished ---
