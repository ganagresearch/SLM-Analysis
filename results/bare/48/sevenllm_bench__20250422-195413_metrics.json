{
    "run_args": {
        "model_path": "/workspace/models/Mistral-7B-Instruct-v0.3/",
        "model_type": "causal",
        "benchmark_name": "sevenllm_bench",
        "benchmark_path": "/workspace/calibration_data/sevenllm_instruct_subset_manual/",
        "results_dir": "/workspace/results/bare/48",
        "device": "cuda",
        "max_new_tokens": 512,
        "num_samples": 100,
        "cti_subset": null
    },
    "load_metrics": {
        "ram_initial_mb": 660.48046875,
        "system_vram_current_initial_mb": 556.0625,
        "load_time_sec": 68.78797459602356,
        "ram_after_load_mb": 1762.515625,
        "pytorch_vram_current_after_load_mb": 27649.01611328125,
        "pytorch_vram_peak_load_mb": 27649.01611328125,
        "system_vram_current_after_load_mb": null
    },
    "inference_metrics": {
        "total_generate_time_sec": 1479.8668024539948,
        "overall_inference_duration_sec": 1480.1274347305298,
        "avg_generate_time_per_sample_sec": 14.798668024539948,
        "total_tokens_generated": 25494,
        "tokens_per_second": 17.227226097459905,
        "ram_before_inference_mb": 1764.015625,
        "ram_after_inference_mb": 1851.015625,
        "pytorch_vram_before_inference_mb": 27649.01611328125,
        "pytorch_vram_after_inference_mb": 27657.1669921875,
        "pytorch_vram_peak_inference_mb": 28117.02880859375,
        "system_vram_before_inference_mb": 28471.375,
        "system_vram_after_inference_mb": 29365.375,
        "system_vram_peak_inference_approx_mb": 0,
        "num_samples_run": 100
    }
}